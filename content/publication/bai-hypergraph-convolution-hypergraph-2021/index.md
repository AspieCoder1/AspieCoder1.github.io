---
title: Hypergraph convolution and hypergraph attention
authors:
- Song Bai
- Feihu Zhang
- Philip H. S. Torr
date: '2021-02-01'
publishDate: '2024-09-13T09:52:57.719703Z'
publication_types:
- article-journal
abstract: Recently, graph neural networks have attracted great attention and achieved
  prominent performance in various research fields. Most of those algorithms have
  assumed pairwise relationships of objects of interest. However, in many real applications,
  the relationships between objects are in higher-order, beyond a pairwise formulation.
  To efficiently learn deep embeddings on the high-order graph-structured data, we
  introduce two end-to-end trainable operators to the family of graph neural networks,
  i.e., hypergraph convolution and hypergraph attention. Whilst hypergraph convolution
  defines the basic formulation of performing convolution on a hypergraph, hypergraph
  attention further enhances the capacity of representation learning by leveraging
  an attention module. With the two operators, a graph neural network is readily extended
  to a more flexible model and applied to diverse applications where non-pairwise
  relationships are observed. Extensive experimental results with semi-supervised
  node classification demonstrate the effectiveness of hypergraph convolution and
  hypergraph attention.
tags:
- Graph learning
- Graph neural networks
- Hypergraph learning
- Semi-supervised learning
links:
- name: URL
  url: https://www.sciencedirect.com/science/article/pii/S0031320320304404
---
