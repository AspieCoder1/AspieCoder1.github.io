@inproceedings{wangVideosSpaceTimeRegion2018,
 abstract = {How do humans recognize the action “opening a book”? We argue that there are two important cues: modeling temporal shape dynamics and modeling functional relationships between humans and objects. In this paper, we propose to represent videos as space-time region graphs which capture these two important cues. Our graph nodes are defined by the object region proposals from different frames in a long range video. These nodes are connected by two types of relations: (i) similarity relations capturing the long range dependencies between correlated objects and (ii) spatial-temporal relations capturing the interactions between nearby objects. We perform reasoning on this graph representation via Graph Convolutional Networks. We achieve state-of-the-art results on the Charades and Something-Something datasets. Especially for Charades with complex environments, we obtain a huge \$\$4.4\%\$\$4.4%gain when our model is applied in complex environments.},
 author = {Wang, Xiaolong and Gupta, Abhinav},
 booktitle = {Computer Vision – ECCV 2018},
 date = {2018},
 editor = {Ferrari, Vittorio and Hebert, Martial and Sminchisescu, Cristian and Weiss, Yair},
 file = {/Users/luke/Zotero/storage/NAWKAL83/Wang and Gupta - 2018 - Videos as Space-Time Region Graphs.pdf},
 isbn = {978-3-030-01228-1},
 langid = {english},
 location = {Cham},
 pages = {413--431},
 publisher = {Springer International Publishing},
 title = {Videos as Space-Time Region Graphs}
}
