@article{liHyperbandNovelBanditbased2017,
 abstract = {Performance of machine learning algorithms depends critically on identifying a good set of hyperparameters. While recent approaches use Bayesian optimization to adaptively select configurations, we focus on speeding up random search through adaptive resource allocation and early-stopping. We formulate hyperparameter optimization as a pure-exploration nonstochastic infinite-armed bandit problem where a predefined resource like iterations, data samples, or features is allocated to randomly sampled configurations. We introduce a novel algorithm, Hyperband, for this framework and analyze its theoretical properties, providing several desirable guarantees. Furthermore, we compare Hyperband with popular Bayesian optimization methods on a suite of hyperparameter optimization problems. We observe that Hyperband can provide over an order-of-magnitude speedup over our competitor set on a variety of deep-learning and kernel-based learning problems.},
 author = {Li, Lisha and Jamieson, Kevin and DeSalvo, Giulia and Rostamizadeh, Afshin and Talwalkar, Ameet},
 date = {2017-01-01},
 file = {/Users/luke/Zotero/storage/77ESXPXG/Li et al. - 2017 - Hyperband a novel bandit-based approach to hyperp.pdf},
 issn = {1532-4435},
 journaltitle = {The Journal of Machine Learning Research},
 keywords = {deep learning,hyperparameter optimization,infinite-armed bandits,model selection,online optimization},
 number = {1},
 pages = {6765--6816},
 shortjournal = {J. Mach. Learn. Res.},
 shorttitle = {Hyperband},
 title = {Hyperband: a novel bandit-based approach to hyperparameter optimization},
 volume = {18}
}
