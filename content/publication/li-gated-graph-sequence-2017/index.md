---
title: Gated Graph Sequence Neural Networks
authors:
- Yujia Li
- Daniel Tarlow
- Marc Brockschmidt
- Richard Zemel
date: '2017-09-22'
publishDate: '2024-09-13T09:52:58.792544Z'
publication_types:
- manuscript
abstract: Graph-structured data appears frequently in domains including chemistry,
  natural language semantics, social networks, and knowledge bases. In this work,
  we study feature learning techniques for graph-structured inputs. Our starting point
  is previous work on Graph Neural Networks (Scarselli et al., 2009), which we modify
  to use gated recurrent units and modern optimization techniques and then extend
  to output sequences. The result is a flexible and broadly useful class of neural
  network models that has favorable inductive biases relative to purely sequence-based
  models (e.g., LSTMs) when the problem is graph-structured. We demonstrate the capabilities
  on some simple AI (bAbI) and graph algorithm learning tasks. We then show it achieves
  state-of-the-art performance on a problem from program verification, in which subgraphs
  need to be matched to abstract data structures.
tags:
- Computer Science - Artificial Intelligence
- Computer Science - Machine Learning
- Computer Science - Neural and Evolutionary Computing
- Statistics - Machine Learning
links:
- name: URL
  url: http://arxiv.org/abs/1511.05493
---
