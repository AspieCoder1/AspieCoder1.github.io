@misc{lvAreWeReally2021,
 abstract = {Heterogeneous graph neural networks (HGNNs) have been blossoming in recent years, but the unique data processing and evaluation setups used by each work obstruct a full understanding of their advancements. In this work, we present a systematical reproduction of 12 recent HGNNs by using their official codes, datasets, settings, and hyperparameters, revealing surprising findings about the progress of HGNNs. We find that the simple homogeneous GNNs, e.g., GCN and GAT, are largely underestimated due to improper settings. GAT with proper inputs can generally match or outperform all existing HGNNs across various scenarios. To facilitate robust and reproducible HGNN research, we construct the Heterogeneous Graph Benchmark (HGB), consisting of 11 diverse datasets with three tasks. HGB standardizes the process of heterogeneous graph data splits, feature processing, and performance evaluation. Finally, we introduce a simple but very strong baseline Simple-HGN--which significantly outperforms all previous models on HGB--to accelerate the advancement of HGNNs in the future.},
 author = {Lv, Qingsong and Ding, Ming and Liu, Qiang and Chen, Yuxiang and Feng, Wenzheng and He, Siming and Zhou, Chang and Jiang, Jianguo and Dong, Yuxiao and Tang, Jie},
 date = {2021-12-30},
 eprint = {2112.14936},
 eprintclass = {cs.LG},
 eprinttype = {arXiv},
 file = {/Users/luke/Zotero/storage/2WPDMEQK/Lv et al. - 2021 - Are we really making much progress Revisiting, be.pdf;/Users/luke/Zotero/storage/LR9UTKS6/2112.html},
 keywords = {Computer Science - Machine Learning,Computer Science - Social and Information Networks},
 note = {arXiv:2112.14936},
 pubstate = {prepublished},
 shorttitle = {Are we really making much progress?},
 title = {Are we really making much progress? Revisiting, benchmarking, and refining heterogeneous graph neural networks},
 url = {http://arxiv.org/abs/2112.14936},
 urldate = {2023-10-30}
}
