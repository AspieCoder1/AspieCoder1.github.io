@article{georgievHEATHyperedgeAttention2022,
 abstract = {Learning from structured data is a core machine learning task. Commonly, such data is represented as graphs, which normally only consider (typed) binary relationships between pairs of nodes. This is a substantial limitation for many domains with highly-structured data. One important such domain is source code, where hypergraph-based representations can better capture the semantically rich and structured nature of code. In this work, we present HEAT, a neural model capable of representing typed and qualified hypergraphs, where each hyperedge explicitly qualifies how participating nodes contribute. It can be viewed as a generalization of both message passing neural networks and Transformers. We evaluate HEAT on knowledge base completion and on bug detection and repair using a novel hypergraph representation of programs. In both settings, it outperforms strong baselines, indicating its power and generality.},
 author = {Georgiev, Dobrik Georgiev and Brockschmidt, Marc and Allamanis, Miltiadis},
 date = {2022-06-09},
 file = {/Users/luke/Zotero/storage/4FH2WNW8/Georgiev et al. - 2022 - HEAT Hyperedge Attention Networks.pdf},
 issn = {2835-8856},
 journaltitle = {Transactions on Machine Learning Research},
 langid = {english},
 shorttitle = {HEAT},
 title = {HEAT: Hyperedge Attention Networks},
 url = {https://openreview.net/forum?id=gCmQK6McbR},
 urldate = {2023-12-04}
}
