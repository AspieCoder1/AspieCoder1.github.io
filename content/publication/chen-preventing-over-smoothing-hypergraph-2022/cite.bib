@misc{chenPreventingOverSmoothingHypergraph2022,
 abstract = {In recent years, hypergraph learning has attracted great attention due to its capacity in representing complex and high-order relationships. However, current neural network approaches designed for hypergraphs are mostly shallow, thus limiting their ability to extract information from high-order neighbors. In this paper, we show both theoretically and empirically, that the performance of hypergraph neural networks does not improve as the number of layers increases, which is known as the over-smoothing problem. To avoid this issue, we develop a new deep hypergraph convolutional network called Deep-HGCN, which can maintain the heterogeneity of node representation in deep layers. Specifically, we prove that a \$k\$-layer Deep-HGCN simulates a polynomial filter of order \$k\$ with arbitrary coefficients, which can relieve the problem of over-smoothing. Experimental results on various datasets demonstrate the superior performance of the proposed model compared to the state-of-the-art hypergraph learning approaches.},
 author = {Chen, Guanzi and Zhang, Jiying and Xiao, Xi and Li, Yang},
 date = {2022-11-02},
 eprint = {2203.17159},
 eprintclass = {cs},
 eprinttype = {arXiv},
 file = {/Users/luke/Zotero/storage/LLCBNMYR/Chen et al. - 2022 - Preventing Over-Smoothing for Hypergraph Neural Ne.pdf;/Users/luke/Zotero/storage/C3FK6SN7/2203.html},
 keywords = {Computer Science - Machine Learning},
 note = {arXiv:2203.17159},
 pubstate = {prepublished},
 title = {Preventing Over-Smoothing for Hypergraph Neural Networks},
 url = {http://arxiv.org/abs/2203.17159},
 urldate = {2024-03-19}
}
