@article{funkeHardMaskingExplaining2020,
 abstract = {Graph Neural Networks (GNNs) are a flexible and powerful family of models that build nodes' representations on irregular graph-structured data. This paper focuses on explaining or interpreting the rationale underlying a given prediction of already trained graph neural networks for the node classification task. Existing approaches for interpreting GNNs try to find subsets of important features and nodes by learning a continuous mask. Our objective is to find discrete masks that are arguably more interpretable while minimizing the expected deviation from the underlying model's prediction. We empirically show that our explanations are both more predictive and sparse. Additionally, we find that multiple diverse explanations are possible, which sufficiently explain a prediction. Finally, we analyze the explanations to find the effect of network homophily on the decision-making process of GNNs.},
 author = {Funke, Thorben and Khosla, Megha and Anand, Avishek},
 date = {2020-10-02},
 file = {/Users/luke/Zotero/storage/CPHCNW3B/Funke et al. - 2020 - Hard Masking for Explaining Graph Neural Networks.pdf},
 langid = {english},
 title = {Hard Masking for Explaining Graph Neural Networks},
 url = {https://openreview.net/forum?id=uDN8pRAdsoC},
 urldate = {2024-03-08}
}
