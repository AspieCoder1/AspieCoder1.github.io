---
title: "HuggingFace's Transformers: State-of-the-art Natural Language Processing"
authors:
- Thomas Wolf
- Lysandre Debut
- Victor Sanh
- Julien Chaumond
- Clement Delangue
- Anthony Moi
- Pierric Cistac
- Tim Rault
- RÃ©mi Louf
- Morgan Funtowicz
- Joe Davison
- Sam Shleifer
- given=Patrick, prefix=von, useprefix=true family=Platen
- Clara Ma
- Yacine Jernite
- Julien Plu
- Canwen Xu
- Teven Le Scao
- Sylvain Gugger
- Mariama Drame
- Quentin Lhoest
- Alexander M. Rush
date: '2020-07-13'
publishDate: '2024-09-13T09:52:59.540015Z'
publication_types:
- manuscript
abstract: Recent progress in natural language processing has been driven by advances
  in both model architecture and model pretraining. Transformer architectures have
  facilitated building higher-capacity models and pretraining has made it possible
  to effectively utilize this capacity for a wide variety of tasks.  textitTransformers
  is an open-source library with the goal of opening up these advances to the wider
  machine learning community. The library consists of carefully engineered state-of-the
  art Transformer architectures under a unified API. Backing this library is a curated
  collection of pretrained models made by and available for the community.  textitTransformers
  is designed to be extensible by researchers, simple for practitioners, and fast
  and robust in industrial deployments. The library is available at  urlhttps://github.com/huggingface/transformers.
tags:
- Computer Science - Computation and Language
links:
- name: URL
  url: http://arxiv.org/abs/1910.03771
---
