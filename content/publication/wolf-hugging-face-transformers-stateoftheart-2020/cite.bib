@misc{wolfHuggingFaceTransformersStateoftheart2020,
 abstract = {Recent progress in natural language processing has been driven by advances in both model architecture and model pretraining. Transformer architectures have facilitated building higher-capacity models and pretraining has made it possible to effectively utilize this capacity for a wide variety of tasks. \ textit\Transformers\ is an open-source library with the goal of opening up these advances to the wider machine learning community. The library consists of carefully engineered state-of-the art Transformer architectures under a unified API. Backing this library is a curated collection of pretrained models made by and available for the community. \ textit\Transformers\ is designed to be extensible by researchers, simple for practitioners, and fast and robust in industrial deployments. The library is available at \ url\https://github.com/huggingface/transformers\.},
 author = {Wolf, Thomas and Debut, Lysandre and Sanh, Victor and Chaumond, Julien and Delangue, Clement and Moi, Anthony and Cistac, Pierric and Rault, Tim and Louf, RÃ©mi and Funtowicz, Morgan and Davison, Joe and Shleifer, Sam and family=Platen, given=Patrick, prefix=von, useprefix=true and Ma, Clara and Jernite, Yacine and Plu, Julien and Xu, Canwen and Scao, Teven Le and Gugger, Sylvain and Drame, Mariama and Lhoest, Quentin and Rush, Alexander M.},
 date = {2020-07-13},
 eprint = {1910.03771},
 eprintclass = {cs.CL},
 eprinttype = {arXiv},
 file = {/Users/luke/Zotero/storage/V4YUGZ46/Wolf et al. - 2020 - HuggingFace's Transformers State-of-the-art Natur.pdf;/Users/luke/Zotero/storage/PC5EHK7E/1910.html},
 keywords = {Computer Science - Computation and Language},
 note = {arXiv:1910.03771},
 pubstate = {prepublished},
 shorttitle = {HuggingFace's Transformers},
 title = {HuggingFace's Transformers: State-of-the-art Natural Language Processing},
 url = {http://arxiv.org/abs/1910.03771},
 urldate = {2023-12-20}
}
