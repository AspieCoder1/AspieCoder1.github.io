---
title: Neural (Tangent Kernel) Collapse
authors:
- Mariia Seleznova
- Dana Weitzner
- Raja Giryes
- Gitta Kutyniok
- Hung-Hsu Chou
date: '2023-10-26'
publishDate: '2024-09-13T09:52:59.206447Z'
publication_types:
- manuscript
abstract: 'This work bridges two important concepts: the Neural Tangent Kernel (NTK),
  which captures the evolution of deep neural networks (DNNs) during training, and
  the Neural Collapse (NC) phenomenon, which refers to the emergence of symmetry and
  structure in the last-layer features of well-trained classification DNNs. We adopt
  the natural assumption that the empirical NTK develops a block structure aligned
  with the class labels, i.e., samples within the same class have stronger correlations
  than samples from different classes. Under this assumption, we derive the dynamics
  of DNNs trained with mean squared (MSE) loss and break them into interpretable phases.
  Moreover, we identify an invariant that captures the essence of the dynamics, and
  use it to prove the emergence of NC in DNNs with block-structured NTK. We provide
  large-scale numerical experiments on three common DNN architectures and three benchmark
  datasets to support our theory.'
tags:
- Computer Science - Artificial Intelligence
- Computer Science - Machine Learning
links:
- name: URL
  url: http://arxiv.org/abs/2305.16427
---
