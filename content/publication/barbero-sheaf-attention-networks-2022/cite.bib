@inproceedings{barberoSheafAttentionNetworks2022,
 abstract = {Attention has become a central inductive bias for deep learning models irrespective of domain. However, increasing theoretical and empirical evidence suggests that Graph Attention Networks (GATs) suffer from the same pathological issues affecting many other Graph Neural Networks (GNNs). First, GAT's features tend to become progressively smoother as more layers are stacked, and second, the model performs poorly in heterophilic graphs. Sheaf Neural Networks (SNNs), a new class of models inspired by algebraic topology and geometry, have shown much promise in tackling these two issues. Building upon the recent success of SNNs and the wide adoption of attention-based architectures, we propose Sheaf Attention Networks (SheafANs). By making use of a novel and more expressive attention mechanism equipped with geometric inductive biases, we show that this type of construction generalizes popular attention-based GNN models to cellular sheaves. We demonstrate that these models help tackle the oversmoothing and heterophily problems and show that, in practice, SheafANs consistently outperform GAT on synthetic and real-world benchmarks.},
 author = {Barbero, Federico and Bodnar, Cristian and Borde, Haitz Sáez de Ocáriz and Lio, Pietro},
 date = {2022-11-07},
 eventtitle = {NeurIPS 2022 Workshop on Symmetry and Geometry in Neural Representations},
 file = {/Users/luke/Zotero/storage/4VDTC37C/Barbero et al. - 2022 - Sheaf Attention Networks.pdf},
 langid = {english},
 title = {Sheaf Attention Networks},
 url = {https://openreview.net/forum?id=LIDvgVjpkZr},
 urldate = {2023-11-04}
}
