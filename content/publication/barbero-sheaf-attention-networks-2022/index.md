---
title: Sheaf Attention Networks
authors:
- Federico Barbero
- Cristian Bodnar
- Haitz Sáez de Ocáriz Borde
- Pietro Lio
date: '2022-11-07'
publishDate: '2024-09-13T09:52:57.741006Z'
publication_types:
- paper-conference
abstract: Attention has become a central inductive bias for deep learning models irrespective
  of domain. However, increasing theoretical and empirical evidence suggests that
  Graph Attention Networks (GATs) suffer from the same pathological issues affecting
  many other Graph Neural Networks (GNNs). First, GAT's features tend to become progressively
  smoother as more layers are stacked, and second, the model performs poorly in heterophilic
  graphs. Sheaf Neural Networks (SNNs), a new class of models inspired by algebraic
  topology and geometry, have shown much promise in tackling these two issues. Building
  upon the recent success of SNNs and the wide adoption of attention-based architectures,
  we propose Sheaf Attention Networks (SheafANs). By making use of a novel and more
  expressive attention mechanism equipped with geometric inductive biases, we show
  that this type of construction generalizes popular attention-based GNN models to
  cellular sheaves. We demonstrate that these models help tackle the oversmoothing
  and heterophily problems and show that, in practice, SheafANs consistently outperform
  GAT on synthetic and real-world benchmarks.
links:
- name: URL
  url: https://openreview.net/forum?id=LIDvgVjpkZr
---
