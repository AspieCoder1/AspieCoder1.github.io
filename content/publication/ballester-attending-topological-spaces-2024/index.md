---
title: 'Attending to Topological Spaces: The Cellular Transformer'
authors:
- Rubén Ballester
- Pablo Hernández-García
- Mathilde Papillon
- Claudio Battiloro
- Nina Miolane
- Tolga Birdal
- Carles Casacuberta
- Sergio Escalera
- Mustafa Hajij
date: '2024-05-26'
publishDate: '2024-09-13T09:52:57.733613Z'
publication_types:
- manuscript
abstract: Topological Deep Learning seeks to enhance the predictive performance of
  neural network models by harnessing topological structures in input data. Topological
  neural networks operate on spaces such as cell complexes and hypergraphs, that can
  be seen as generalizations of graphs. In this work, we introduce the Cellular Transformer
  (CT), a novel architecture that generalizes graph-based transformers to cell complexes.
  First, we propose a new formulation of the usual self- and cross-attention mechanisms,
  tailored to leverage incidence relations in cell complexes, e.g., edge-face and
  node-edge relations. Additionally, we propose a set of topological positional encodings
  specifically designed for cell complexes. By transforming three graph datasets into
  cell complex datasets, our experiments reveal that CT not only achieves state-of-the-art
  performance, but it does so without the need for more complex enhancements such
  as virtual nodes, in-domain structural encodings, or graph rewiring.
tags:
- Computer Science - Artificial Intelligence
- Computer Science - Computer Vision and Pattern Recognition
- Computer Science - Machine Learning
- Mathematics - Algebraic Topology
- Statistics - Machine Learning
links:
- name: URL
  url: http://arxiv.org/abs/2405.14094
---
