---
title: Attention is All you Need
authors:
- Ashish Vaswani
- Noam Shazeer
- Niki Parmar
- Jakob Uszkoreit
- Llion Jones
- Aidan Gomez
- ≈Åukasz Kaiser
- Illia Polosukhin
date: '2017-01-01'
publishDate: '2024-09-13T09:52:59.420595Z'
publication_types:
- paper-conference
publication: '*Advances in Neural Information Processing Systems*'
abstract: The dominant sequence transduction models are based on complex recurrent
  orconvolutional neural networks in an encoder and decoder configuration. The best
  performing such models also connect the encoder and decoder through an attentionm
  echanisms.  We propose a novel, simple network architecture based solely onan attention
  mechanism, dispensing with recurrence and convolutions entirely.Experiments on two
  machine translation tasks show these models to be superiorin quality while being
  more parallelizable and requiring significantly less timeto train. Our single model
  with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation,
  improving over the existing best ensemble result by over 1 BLEU. On English-to-French
  translation, we outperform the previoussingle state-of-the-art with model by 0.7
  BLEU, achieving a BLEU score of 41.1.
links:
- name: URL
  url: 
    https://proceedings.neurips.cc/paper_files/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html
---
