@misc{abubakerSelfSupervisedPretrainingHeterogeneous2023,
  title = {Self-{{Supervised Pretraining}} for {{Heterogeneous Hypergraph Neural Networks}}},
  author = {Abubaker, Abdalgader and Maehara, Takanori and Nimishakavi, Madhav and Plachouras, Vassilis},
  date = {2023-11-19},
  eprint = {2311.11368},
  eprinttype = {arXiv},
  eprintclass = {cs.LG},
  url = {http://arxiv.org/abs/2311.11368},
  urldate = {2023-11-23},
  abstract = {Recently, pretraining methods for the Graph Neural Networks (GNNs) have been successful at learning effective representations from unlabeled graph data. However, most of these methods rely on pairwise relations in the graph and do not capture the underling higher-order relations between entities. Hypergraphs are versatile and expressive structures that can effectively model higher-order relationships among entities in the data. Despite the efforts to adapt GNNs to hypergraphs (HyperGNN), there are currently no fully self-supervised pretraining methods for HyperGNN on heterogeneous hypergraphs. In this paper, we present SPHH, a novel self-supervised pretraining framework for heterogeneous HyperGNNs. Our method is able to effectively capture higher-order relations among entities in the data in a self-supervised manner. SPHH is consist of two self-supervised pretraining tasks that aim to simultaneously learn both local and global representations of the entities in the hypergraph by using informative representations derived from the hypergraph structure. Overall, our work presents a significant advancement in the field of self-supervised pretraining of HyperGNNs, and has the potential to improve the performance of various graph-based downstream tasks such as node classification and link prediction tasks which are mapped to hypergraph configuration. Our experiments on two real-world benchmarks using four different HyperGNN models show that our proposed SPHH framework consistently outperforms state-of-the-art baselines in various downstream tasks. The results demonstrate that SPHH is able to improve the performance of various HyperGNN models in various downstream tasks, regardless of their architecture or complexity, which highlights the robustness of our framework.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Computer Science - Social and Information Networks,Statistics - Machine Learning},
  file = {/Users/luke/Zotero/storage/V4X8KB78/Abubaker et al. - 2023 - Self-Supervised Pretraining for Heterogeneous Hype.pdf;/Users/luke/Zotero/storage/7SYZPTAN/2311.html},
  note = {arXiv:2311.11368}
}

@inproceedings{ahmedCompilationErrorRepair2018,
  title = {Compilation {{Error Repair}}: {{For}} the {{Student Programs}}, {{From}} the {{Student Programs}}},
  shorttitle = {Compilation {{Error Repair}}},
  booktitle = {2018 {{IEEE}}/{{ACM}} 40th {{International Conference}} on {{Software Engineering}}: {{Software Engineering Education}} and {{Training}} ({{ICSE-SEET}})},
  author = {Ahmed, Umair Z. and Kumar, Pawan and Karkare, Amey and Kar, Purushottam and Gulwani, Sumit},
  date = {2018-05},
  pages = {78--87},
  url = {https://ieeexplore.ieee.org/document/8445185},
  urldate = {2023-12-19},
  abstract = {Compile-time errors pose a major learning hurdle for students of introductory programming courses. Compiler error messages, while accurate, are targeted at seasoned programmers, and seem cryptic to beginners. In this work, we address this problem of pedagogically-inspired program repair and report TRACER (Targeted RepAir of Compilation ERrors), a system for performing repairs on compilation errors, aimed at introductory programmers. TRACER invokes a novel combination of tools from programming language theory and deep learning and offers repairs that not only enable successful compilation, but repairs that are very close to those actually performed by students on similar errors. The ability to offer such targeted corrections, rather than just code that compiles, makes TRACER more relevant in offering real-time feedback to students in lab or tutorial sessions, as compared to existing works that merely offer a certain compilation success rate. In an evaluation on 4500 erroneous C programs written by students of a freshman year programming course, TRACER recommends a repair exactly matching the one expected by the student for 68\% of the cases, and in 79.27\% of the cases, produces a compilable repair. On a further set of 6971 programs that require errors to be fixed on multiple lines, TRACER enjoyed a success rate of 44\% compared to the 27\% success rate offered by the state-of-the-art technique DeepFix.},
  eventtitle = {2018 {{IEEE}}/{{ACM}} 40th {{International Conference}} on {{Software Engineering}}: {{Software Engineering Education}} and {{Training}} ({{ICSE-SEET}})},
  file = {/Users/luke/Zotero/storage/SRVSGRMJ/8445185.html}
}

@inproceedings{ahnDescentStepsRelationAware2022a,
  title = {Descent {{Steps}} of a {{Relation-Aware Energy Produce Heterogeneous Graph Neural Networks}}},
  author = {Ahn, Hongjoon and Yang, Yongyi and Gan, Quan and Moon, Taesup and Wipf, David},
  date = {2022-10-31},
  url = {https://openreview.net/forum?id=hgNxCMKARgt},
  urldate = {2024-03-19},
  abstract = {Heterogeneous graph neural networks (GNNs) achieve strong performance on node classification tasks in a semi-supervised learning setting. However, as in the simpler homogeneous GNN case, message-passing-based heterogeneous GNNs may struggle to balance between resisting the oversmoothing that may occur in deep models, and capturing long-range dependencies of graph structured data. Moreover, the complexity of this trade-off is compounded in the heterogeneous graph case due to the disparate heterophily relationships between nodes of different types. To address these issues, we propose a novel heterogeneous GNN architecture in which layers are derived from optimization steps that descend a novel relation-aware energy function. The corresponding minimizer is fully differentiable with respect to the energy function parameters, such that bilevel optimization can be applied to effectively learn a functional form whose minimum provides optimal node representations for subsequent classification tasks. In particular, this methodology allows us to model diverse heterophily relationships between different node types while avoiding oversmoothing effects. Experimental results on 8 heterogeneous graph benchmarks demonstrates that our proposed method can achieve competitive node classification accuracy.},
  eventtitle = {Advances in {{Neural Information Processing Systems}}},
  langid = {english},
  file = {/Users/luke/Zotero/storage/RCI5N8C7/Ahn et al. - 2022 - Descent Steps of a Relation-Aware Energy Produce H.pdf}
}

@book{ahoCompilersPrinciplesTechniques2013,
  title = {Compilers: principles, techniques, and tools},
  shorttitle = {Compilers},
  author = {Aho, A. V. and Lam, Monica S. and Sethi, R. and Ullman, Jeffrey D.},
  date = {2013},
  series = {Pearson custom library},
  edition = {Pearson new international edition.},
  publisher = {Pearson Education},
  abstract = {Compilers: Principles, Techniques and Tools, known to professors, students, and developers worldwide as the "Dragon Book," is available in a new edition. Every chapter has been completely revised to reflect developments in software engineering, programming languages, and computer architecture that have occurred since 1986, when the last edition published. The authors, recognizing that few readers will ever go on to construct a compiler, retain their focus on the broader set of problems faced in software design and software development. New chapters include: Chapter 10 Instruction-Level ParallelismChapter 11 Optimizing for Parallelism and Locality},
  isbn = {978-1-292-02434-9},
  langid = {english},
  keywords = {Compilers (Computer programs)}
}

@inproceedings{allamanisLearningRepresentPrograms2018,
  title = {Learning to {{Represent Programs}} with {{Graphs}}},
  author = {Allamanis, Miltiadis and Brockschmidt, Marc and Khademi, Mahmoud},
  date = {2018-02-15},
  url = {https://openreview.net/forum?id=BJOFETxR-},
  urldate = {2023-12-19},
  abstract = {Learning tasks on source code (i.e., formal languages) have been considered recently, but most work has tried to transfer natural language methods and does not capitalize on the unique opportunities offered by code's known syntax. For example, long-range dependencies induced by using the same variable or function in distant locations are often not considered. We propose to use graphs to represent both the syntactic and semantic structure of code and use graph-based deep learning methods to learn to reason over program structures. In this work, we present how to construct graphs from source code and how to scale Gated Graph Neural Networks training to such large graphs. We evaluate our method on two tasks: VarNaming, in which a network attempts to predict the name of a variable given its usage, and VarMisuse, in which the network learns to reason about selecting the correct variable that should be used at a given program location. Our comparison to methods that use less structured program representations shows the advantages of modeling known structure, and suggests that our models learn to infer meaningful names and to solve the VarMisuse task in many cases. Additionally, our testing showed that VarMisuse identifies a number of bugs in mature open-source projects.},
  eventtitle = {International {{Conference}} on {{Learning Representations}}},
  langid = {english},
  file = {/Users/luke/Zotero/storage/G3JGK9VY/Allamanis et al. - 2018 - Learning to Represent Programs with Graphs.pdf}
}

@inproceedings{allamanisSelfSupervisedBugDetection2021,
  title = {Self-{{Supervised Bug Detection}} and {{Repair}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Allamanis, Miltiadis and Jackson-Flux, Henry and Brockschmidt, Marc},
  date = {2021},
  volume = {34},
  pages = {27865--27876},
  publisher = {Curran Associates, Inc.},
  url = {https://proceedings.neurips.cc/paper/2021/hash/ea96efc03b9a050d895110db8c4af057-Abstract.html},
  urldate = {2023-12-04},
  abstract = {Machine learning-based program analyses have recently shown the promise of integrating formal and probabilistic reasoning towards aiding software development. However, in the absence of large annotated corpora, training these analyses is challenging. Towards addressing this, we present BugLab, an approach for self-supervised learning of bug detection and repair.  BugLab co-trains two models: (1) a detector model that learns to detect and repair bugs in code, (2) a selector model that learns to create buggy code for the detector to use as training data. A Python implementation of BugLab improves by 30\% upon baseline methods on a test dataset of 2374 real-life bugs and finds 19 previously unknown bugs in open-source software.},
  eventtitle = {{{NeurIPS}} 2021},
  file = {/Users/luke/Zotero/storage/ZKFRGAHA/Allamanis et al. - 2021 - Self-Supervised Bug Detection and Repair.pdf}
}

@inproceedings{allamanisTypilusNeuralType2020,
  title = {Typilus: neural type hints},
  shorttitle = {Typilus},
  booktitle = {Proceedings of the 41st {{ACM SIGPLAN Conference}} on {{Programming Language Design}} and {{Implementation}}},
  author = {Allamanis, Miltiadis and Barr, Earl T. and Ducousso, Soline and Gao, Zheng},
  date = {2020-06-11},
  series = {{{PLDI}} 2020},
  pages = {91--105},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  url = {https://doi.org/10.1145/3385412.3385997},
  urldate = {2023-12-19},
  abstract = {Type inference over partial contexts in dynamically typed languages is challenging. In this work, we present a graph neural network model that predicts types by probabilistically reasoning over a program’s structure, names, and patterns. The network uses deep similarity learning to learn a TypeSpace — a continuous relaxation of the discrete space of types — and how to embed the type properties of a symbol (i.e. identifier) into it. Importantly, our model can employ one-shot learning to predict an open vocabulary of types, including rare and user-defined ones. We realise our approach in Typilus for Python that combines the TypeSpace with an optional type checker. We show that Typilus accurately predicts types. Typilus confidently predicts types for 70\% of all annotatable symbols; when it predicts a type, that type optionally type checks 95\% of the time. Typilus can also find incorrect type annotations; two important and popular open source libraries, fairseq and allennlp, accepted our pull requests that fixed the annotation errors Typilus discovered.},
  isbn = {978-1-4503-7613-6},
  keywords = {deep learning,graph neural networks,meta-learning,structured learning,type inference},
  file = {/Users/luke/Zotero/storage/4YI6YXDA/Allamanis et al. - 2020 - Typilus neural type hints.pdf}
}

@inproceedings{alonBottleneckGraphNeural2020,
  title = {On the {{Bottleneck}} of {{Graph Neural Networks}} and its {{Practical Implications}}},
  author = {Alon, Uri and Yahav, Eran},
  date = {2020-10-02},
  url = {https://openreview.net/forum?id=i80OPhOCVH2},
  urldate = {2024-09-11},
  abstract = {Since the proposal of the graph neural network (GNN) by Gori et al. (2005) and Scarselli et al. (2008), one of the major problems in training GNNs was their struggle to propagate information between distant nodes in the graph. We propose a new explanation for this problem: GNNs are susceptible to a bottleneck when aggregating messages across a long path. This bottleneck causes the over-squashing of exponentially growing information into fixed-size vectors. As a result, GNNs fail to propagate messages originating from distant nodes and perform poorly when the prediction task depends on long-range interaction. In this paper, we highlight the inherent problem of over-squashing in GNNs: we demonstrate that the bottleneck hinders popular GNNs from fitting long-range signals in the training data; we further show that GNNs that absorb incoming edges equally, such as GCN and GIN, are more susceptible to over-squashing than GAT and GGNN; finally, we show that prior work, which extensively tuned GNN models of long-range problems, suffers from over-squashing, and that breaking the bottleneck improves their state-of-the-art results without any tuning or additional weights. Our code is available at https://github.com/tech-srl/bottleneck/ .},
  eventtitle = {International {{Conference}} on {{Learning Representations}}},
  langid = {english},
  file = {/Users/luke/Zotero/storage/5ISW3QXX/Alon and Yahav - 2020 - On the Bottleneck of Graph Neural Networks and its.pdf}
}

@article{alvarez-rodriguezEvolutionaryDynamicsHigherorder2021,
  title = {Evolutionary dynamics of higher-order interactions in social networks},
  author = {Alvarez-Rodriguez, Unai and Battiston, Federico and family=Arruda, given=Guilherme Ferraz, prefix=de, useprefix=true and Moreno, Yamir and Perc, Matjaž and Latora, Vito},
  date = {2021-05},
  journaltitle = {Nature Human Behaviour},
  shortjournal = {Nat Hum Behav},
  volume = {5},
  number = {5},
  pages = {586--595},
  publisher = {Nature Publishing Group},
  issn = {2397-3374},
  url = {https://www.nature.com/articles/s41562-020-01024-1},
  urldate = {2024-05-20},
  abstract = {We live and cooperate in networks. However, links in networks only allow for pairwise interactions, thus making the framework suitable for dyadic games, but not for games that are played in larger groups. Here, we study the evolutionary dynamics of a public goods game in social systems with higher-order interactions. First, we show that the game on uniform hypergraphs corresponds to the replicator dynamics in the well-mixed limit, providing a formal theoretical foundation to study cooperation in networked groups. Second, we unveil how the presence of hubs and the coexistence of interactions in groups of different sizes affects the evolution of cooperation. Finally, we apply the proposed framework to extract the actual dependence of the synergy factor on the size of a group from real-world collaboration data in science and technology. Our work provides a way to implement informed actions to boost cooperation in social groups.},
  langid = {english},
  keywords = {Complex networks},
  file = {/Users/luke/Zotero/storage/9QJRZPFH/Alvarez-Rodriguez et al. - 2021 - Evolutionary dynamics of higher-order interactions.pdf}
}

@inproceedings{andriushchenkoUnderstandingSharpnessAwareMinimization2022,
  title = {Towards {{Understanding Sharpness-Aware Minimization}}},
  booktitle = {Proceedings of the 39th {{International Conference}} on {{Machine Learning}}},
  author = {Andriushchenko, Maksym and Flammarion, Nicolas},
  date = {2022-06-28},
  pages = {639--668},
  publisher = {PMLR},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v162/andriushchenko22a.html},
  urldate = {2024-02-13},
  abstract = {Sharpness-Aware Minimization (SAM) is a recent training method that relies on worst-case weight perturbations which significantly improves generalization in various settings. We argue that the existing justifications for the success of SAM which are based on a PAC-Bayes generalization bound and the idea of convergence to flat minima are incomplete. Moreover, there are no explanations for the success of using m-sharpness in SAM which has been shown as essential for generalization. To better understand this aspect of SAM, we theoretically analyze its implicit bias for diagonal linear networks. We prove that SAM always chooses a solution that enjoys better generalization properties than standard gradient descent for a certain class of problems, and this effect is amplified by using m-sharpness. We further study the properties of the implicit bias on non-linear networks empirically, where we show that fine-tuning a standard model with SAM can lead to significant generalization improvements. Finally, we provide convergence results of SAM for non-convex objectives when used with stochastic gradients. We illustrate these results empirically for deep networks and discuss their relation to the generalization behavior of SAM. The code of our experiments is available at https://github.com/tml-epfl/understanding-sam.},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english},
  file = {/Users/luke/Zotero/storage/W49UXMEB/Andriushchenko and Flammarion - 2022 - Towards Understanding Sharpness-Aware Minimization.pdf}
}

@misc{aviles-riveroBilevelHypergraphNetworks2024,
  title = {Bilevel {{Hypergraph Networks}} for {{Multi-Modal Alzheimer}}'s {{Diagnosis}}},
  author = {Aviles-Rivero, Angelica I. and Cheng, Chun-Wun and Deng, Zhongying and Kourtzi, Zoe and Schönlieb, Carola-Bibiane},
  date = {2024-03-19},
  eprint = {2403.12719},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2403.12719},
  urldate = {2024-03-24},
  abstract = {Early detection of Alzheimer's disease's precursor stages is imperative for significantly enhancing patient outcomes and quality of life. This challenge is tackled through a semi-supervised multi-modal diagnosis framework. In particular, we introduce a new hypergraph framework that enables higher-order relations between multi-modal data, while utilising minimal labels. We first introduce a bilevel hypergraph optimisation framework that jointly learns a graph augmentation policy and a semi-supervised classifier. This dual learning strategy is hypothesised to enhance the robustness and generalisation capabilities of the model by fostering new pathways for information propagation. Secondly, we introduce a novel strategy for generating pseudo-labels more effectively via a gradient-driven flow. Our experimental results demonstrate the superior performance of our framework over current techniques in diagnosing Alzheimer's disease.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/luke/Zotero/storage/2DJEJ6JB/Aviles-Rivero et al. - 2024 - Bilevel Hypergraph Networks for Multi-Modal Alzhei.pdf;/Users/luke/Zotero/storage/TGJTYWNH/2403.html},
  note = {arXiv:2403.12719}
}

@software{awesome-tnnsAwesometnnsAwesometnns2024,
  title = {awesome-tnns/awesome-tnns},
  author = {family=tnns, prefix=awesome-, useprefix=true},
  date = {2024-05-14T13:31:33Z},
  origdate = {2023-04-15T01:13:50Z},
  url = {https://github.com/awesome-tnns/awesome-tnns},
  urldate = {2024-05-22}
}

@misc{azzolinGlobalExplainabilityGNNs2023,
  title = {Global {{Explainability}} of {{GNNs}} via {{Logic Combination}} of {{Learned Concepts}}},
  author = {Azzolin, Steve and Longa, Antonio and Barbiero, Pietro and Liò, Pietro and Passerini, Andrea},
  date = {2023-04-11},
  eprint = {2210.07147},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2210.07147},
  urldate = {2024-02-27},
  abstract = {While instance-level explanation of GNN is a well-studied problem with plenty of approaches being developed, providing a global explanation for the behaviour of a GNN is much less explored, despite its potential in interpretability and debugging. Existing solutions either simply list local explanations for a given class, or generate a synthetic prototypical graph with maximal score for a given class, completely missing any combinatorial aspect that the GNN could have learned. In this work, we propose GLGExplainer (Global Logic-based GNN Explainer), the first Global Explainer capable of generating explanations as arbitrary Boolean combinations of learned graphical concepts. GLGExplainer is a fully differentiable architecture that takes local explanations as inputs and combines them into a logic formula over graphical concepts, represented as clusters of local explanations. Contrary to existing solutions, GLGExplainer provides accurate and human-interpretable global explanations that are perfectly aligned with ground-truth explanations (on synthetic data) or match existing domain knowledge (on real-world data). Extracted formulas are faithful to the model predictions, to the point of providing insights into some occasionally incorrect rules learned by the model, making GLGExplainer a promising diagnostic tool for learned GNNs.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Logic in Computer Science,Computer Science - Machine Learning},
  file = {/Users/luke/Zotero/storage/GDRU7YXT/Azzolin et al. - 2023 - Global Explainability of GNNs via Logic Combinatio.pdf;/Users/luke/Zotero/storage/QUWBFBJD/2210.html},
  note = {arXiv:2210.07147}
}

@article{baiHypergraphConvolutionHypergraph2021,
  title = {Hypergraph convolution and hypergraph attention},
  author = {Bai, Song and Zhang, Feihu and Torr, Philip H. S.},
  date = {2021-02-01},
  journaltitle = {Pattern Recognition},
  shortjournal = {Pattern Recognition},
  volume = {110},
  pages = {107637},
  issn = {0031-3203},
  url = {https://www.sciencedirect.com/science/article/pii/S0031320320304404},
  urldate = {2024-02-26},
  abstract = {Recently, graph neural networks have attracted great attention and achieved prominent performance in various research fields. Most of those algorithms have assumed pairwise relationships of objects of interest. However, in many real applications, the relationships between objects are in higher-order, beyond a pairwise formulation. To efficiently learn deep embeddings on the high-order graph-structured data, we introduce two end-to-end trainable operators to the family of graph neural networks, i.e., hypergraph convolution and hypergraph attention. Whilst hypergraph convolution defines the basic formulation of performing convolution on a hypergraph, hypergraph attention further enhances the capacity of representation learning by leveraging an attention module. With the two operators, a graph neural network is readily extended to a more flexible model and applied to diverse applications where non-pairwise relationships are observed. Extensive experimental results with semi-supervised node classification demonstrate the effectiveness of hypergraph convolution and hypergraph attention.},
  keywords = {Graph learning,Graph neural networks,Hypergraph learning,Semi-supervised learning},
  file = {/Users/luke/Zotero/storage/MD3SM9YG/Bai et al. - 2021 - Hypergraph convolution and hypergraph attention.pdf}
}

@misc{baldassarreExplainabilityTechniquesGraph2019,
  title = {Explainability {{Techniques}} for {{Graph Convolutional Networks}}},
  author = {Baldassarre, Federico and Azizpour, Hossein},
  date = {2019-05-31},
  eprint = {1905.13686},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1905.13686},
  urldate = {2024-03-08},
  abstract = {Graph Networks are used to make decisions in potentially complex scenarios but it is usually not obvious how or why they made them. In this work, we study the explainability of Graph Network decisions using two main classes of techniques, gradient-based and decomposition-based, on a toy dataset and a chemistry task. Our study sets the ground for future development as well as application to real-world problems.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/luke/Zotero/storage/62CSF68G/Baldassarre and Azizpour - 2019 - Explainability Techniques for Graph Convolutional .pdf;/Users/luke/Zotero/storage/2SMMJ4YA/1905.html},
  note = {arXiv:1905.13686}
}

@misc{ballesterAttendingTopologicalSpaces2024,
  title = {Attending to {{Topological Spaces}}: {{The Cellular Transformer}}},
  shorttitle = {Attending to {{Topological Spaces}}},
  author = {Ballester, Rubén and Hernández-García, Pablo and Papillon, Mathilde and Battiloro, Claudio and Miolane, Nina and Birdal, Tolga and Casacuberta, Carles and Escalera, Sergio and Hajij, Mustafa},
  date = {2024-05-26},
  eprint = {2405.14094},
  eprinttype = {arXiv},
  eprintclass = {cs, math, stat},
  url = {http://arxiv.org/abs/2405.14094},
  urldate = {2024-05-28},
  abstract = {Topological Deep Learning seeks to enhance the predictive performance of neural network models by harnessing topological structures in input data. Topological neural networks operate on spaces such as cell complexes and hypergraphs, that can be seen as generalizations of graphs. In this work, we introduce the Cellular Transformer (CT), a novel architecture that generalizes graph-based transformers to cell complexes. First, we propose a new formulation of the usual self- and cross-attention mechanisms, tailored to leverage incidence relations in cell complexes, e.g., edge-face and node-edge relations. Additionally, we propose a set of topological positional encodings specifically designed for cell complexes. By transforming three graph datasets into cell complex datasets, our experiments reveal that CT not only achieves state-of-the-art performance, but it does so without the need for more complex enhancements such as virtual nodes, in-domain structural encodings, or graph rewiring.},
  pubstate = {prepublished},
  version = {2},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Mathematics - Algebraic Topology,Statistics - Machine Learning},
  file = {/Users/luke/Zotero/storage/FJSYD5SJ/Ballester et al. - 2024 - Attending to Topological Spaces The Cellular Tran.pdf;/Users/luke/Zotero/storage/8VBHA48T/2405.html},
  note = {arXiv:2405.14094}
}

@inproceedings{barberoSheafAttentionNetworks2022,
  title = {Sheaf {{Attention Networks}}},
  author = {Barbero, Federico and Bodnar, Cristian and Borde, Haitz Sáez de Ocáriz and Lio, Pietro},
  date = {2022-11-07},
  url = {https://openreview.net/forum?id=LIDvgVjpkZr},
  urldate = {2023-11-04},
  abstract = {Attention has become a central inductive bias for deep learning models irrespective of domain. However, increasing theoretical and empirical evidence suggests that Graph Attention Networks (GATs) suffer from the same pathological issues affecting many other Graph Neural Networks (GNNs). First, GAT's features tend to become progressively smoother as more layers are stacked, and second, the model performs poorly in heterophilic graphs. Sheaf Neural Networks (SNNs), a new class of models inspired by algebraic topology and geometry, have shown much promise in tackling these two issues. Building upon the recent success of SNNs and the wide adoption of attention-based architectures, we propose Sheaf Attention Networks (SheafANs). By making use of a novel and more expressive attention mechanism equipped with geometric inductive biases, we show that this type of construction generalizes popular attention-based GNN models to cellular sheaves. We demonstrate that these models help tackle the oversmoothing and heterophily problems and show that, in practice, SheafANs consistently outperform GAT on synthetic and real-world benchmarks.},
  eventtitle = {{{NeurIPS}} 2022 {{Workshop}} on {{Symmetry}} and {{Geometry}} in {{Neural Representations}}},
  langid = {english},
  file = {/Users/luke/Zotero/storage/4VDTC37C/Barbero et al. - 2022 - Sheaf Attention Networks.pdf}
}

@misc{barberoSheafNeuralNetworks2022,
  title = {Sheaf {{Neural Networks}} with {{Connection Laplacians}}},
  author = {Barbero, Federico and Bodnar, Cristian and Borde, Haitz Sáez de Ocáriz and Bronstein, Michael and Veličković, Petar and Liò, Pietro},
  date = {2022-06-17},
  eprint = {2206.08702},
  eprinttype = {arXiv},
  eprintclass = {cs, math},
  url = {http://arxiv.org/abs/2206.08702},
  urldate = {2024-03-31},
  abstract = {A Sheaf Neural Network (SNN) is a type of Graph Neural Network (GNN) that operates on a sheaf, an object that equips a graph with vector spaces over its nodes and edges and linear maps between these spaces. SNNs have been shown to have useful theoretical properties that help tackle issues arising from heterophily and over-smoothing. One complication intrinsic to these models is finding a good sheaf for the task to be solved. Previous works proposed two diametrically opposed approaches: manually constructing the sheaf based on domain knowledge and learning the sheaf end-to-end using gradient-based methods. However, domain knowledge is often insufficient, while learning a sheaf could lead to overfitting and significant computational overhead. In this work, we propose a novel way of computing sheaves drawing inspiration from Riemannian geometry: we leverage the manifold assumption to compute manifold-and-graph-aware orthogonal maps, which optimally align the tangent spaces of neighbouring data points. We show that this approach achieves promising results with less computational overhead when compared to previous SNN models. Overall, this work provides an interesting connection between algebraic topology and differential geometry, and we hope that it will spark future research in this direction.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Mathematics - Algebraic Topology,Mathematics - Differential Geometry},
  file = {/Users/luke/Zotero/storage/ICRV48QB/Barbero et al. - 2022 - Sheaf Neural Networks with Connection Laplacians.pdf;/Users/luke/Zotero/storage/3VYIAC2Y/2206.html},
  note = {arXiv:2206.08702}
}

@inproceedings{barberoSheafNeuralNetworks2022a,
  title = {Sheaf {{Neural Networks}} with {{Connection Laplacians}}},
  booktitle = {Proceedings of {{Topological}}, {{Algebraic}}, and {{Geometric Learning Workshops}} 2022},
  author = {Barbero, Federico and Bodnar, Cristian and Borde, Haitz Sáez de Ocáriz and Bronstein, Michael and Veličković, Petar and Liò, Pietro},
  date = {2022-11-09},
  pages = {28--36},
  publisher = {PMLR},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v196/barbero22a.html},
  urldate = {2024-04-01},
  abstract = {A Sheaf Neural Network (SNN) is a type of Graph Neural Network (GNN) that operates on a sheaf, an object that equips a graph with vector spaces over its nodes and edges and linear maps between these spaces. SNNs have been shown to have useful theoretical properties that help tackle issues arising from heterophily and over-smoothing. One complication intrinsic to these models is finding a good sheaf for the task to be solved. Previous works proposed two diametrically opposed approaches: manually constructing the sheaf based on domain knowledge and learning the sheaf end-to-end using gradient-based methods. However, domain knowledge is often insufficient, while learning a sheaf could lead to overfitting and significant computational overhead. In this work, we propose a novel way of computing sheaves drawing inspiration from Riemannian geometry: we leverage the manifold assumption to compute manifold-and-graph-aware orthogonal maps, which optimally align the tangent spaces of neighbouring data points. We show that this approach achieves promising results with less computational overhead when compared to previous SNN models. Overall, this work provides an interesting connection between algebraic topology and differential geometry, and we hope that it will spark future research in this direction.},
  eventtitle = {Topological, {{Algebraic}} and {{Geometric Learning Workshops}} 2022},
  langid = {english},
  file = {/Users/luke/Zotero/storage/Y7JXC3XK/Barbero et al. - 2022 - Sheaf Neural Networks with Connection Laplacians.pdf}
}

@misc{barbieroEntropybasedLogicExplanations2022,
  title = {Entropy-based {{Logic Explanations}} of {{Neural Networks}}},
  author = {Barbiero, Pietro and Ciravegna, Gabriele and Giannini, Francesco and Lió, Pietro and Gori, Marco and Melacci, Stefano},
  date = {2022-06-28},
  journaltitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
  shortjournal = {AAAI},
  volume = {36},
  number = {6},
  eprint = {2106.06804},
  eprinttype = {arXiv},
  eprintclass = {cs},
  pages = {6046--6054},
  issn = {2374-3468, 2159-5399},
  url = {http://arxiv.org/abs/2106.06804},
  urldate = {2024-03-08},
  abstract = {Explainable artificial intelligence has rapidly emerged since lawmakers have started requiring interpretable models for safety-critical domains. Concept-based neural networks have arisen as explainable-by-design methods as they leverage human-understandable symbols (i.e. concepts) to predict class memberships. However, most of these approaches focus on the identification of the most relevant concepts but do not provide concise, formal explanations of how such concepts are leveraged by the classifier to make predictions. In this paper, we propose a novel end-to-end differentiable approach enabling the extraction of logic explanations from neural networks using the formalism of First-Order Logic. The method relies on an entropy-based criterion which automatically identifies the most relevant concepts. We consider four different case studies to demonstrate that: (i) this entropy-based criterion enables the distillation of concise logic explanations in safety-critical domains from clinical data to computer vision; (ii) the proposed approach outperforms state-of-the-art white-box models in terms of classification accuracy and matches black box performances.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Logic in Computer Science,Computer Science - Machine Learning},
  file = {/Users/luke/Zotero/storage/IGYUCAMK/Barbiero et al. - 2022 - Entropy-based Logic Explanations of Neural Network.pdf;/Users/luke/Zotero/storage/JFY3WTMI/2106.html},
  note = {arXiv:2106.06804}
}

@inproceedings{barronMipNeRFMultiscaleRepresentation2021,
  title = {Mip-{{NeRF}}: {{A Multiscale Representation}} for {{Anti-Aliasing Neural Radiance Fields}}},
  shorttitle = {Mip-{{NeRF}}},
  author = {Barron, Jonathan T. and Mildenhall, Ben and Tancik, Matthew and Hedman, Peter and Martin-Brualla, Ricardo and Srinivasan, Pratul P.},
  date = {2021},
  url = {https://openaccess.thecvf.com/content/ICCV2021/html/Barron_Mip-NeRF_A_Multiscale_Representation_for_Anti-Aliasing_Neural_Radiance_Fields_ICCV_2021_paper.html},
  urldate = {2023-10-22},
  eventtitle = {{{ICCV}}},
  langid = {english},
  file = {/Users/luke/Zotero/storage/6SM8VSJL/Barron et al. - 2021 - Mip-NeRF A Multiscale Representation for Anti-Ali.pdf}
}

@article{basuIterativeRandomForests2018,
  title = {Iterative random forests to discover predictive and stable high-order interactions},
  author = {Basu, Sumanta and Kumbier, Karl and Brown, James B. and Yu, Bin},
  date = {2018-02-20},
  journaltitle = {Proceedings of the National Academy of Sciences},
  volume = {115},
  number = {8},
  pages = {1943--1948},
  publisher = {Proceedings of the National Academy of Sciences},
  url = {https://www.pnas.org/doi/10.1073/pnas.1711236115},
  urldate = {2023-11-26},
  abstract = {Genomics has revolutionized biology, enabling the interrogation of whole transcriptomes, genome-wide binding sites for proteins, and many other molecular processes. However, individual genomic assays measure elements that interact in vivo as components of larger molecular machines. Understanding how these high-order interactions drive gene expression presents a substantial statistical challenge. Building on random forests (RFs) and random intersection trees (RITs) and through extensive, biologically inspired simulations, we developed the iterative random forest algorithm (iRF). iRF trains a feature-weighted ensemble of decision trees to detect stable, high-order interactions with the same order of computational cost as the RF. We demonstrate the utility of iRF for high-order interaction discovery in two prediction problems: enhancer activity in the early Drosophila embryo and alternative splicing of primary transcripts in human-derived cell lines. In Drosophila, among the 20 pairwise transcription factor interactions iRF identifies as stable (returned in more than half of bootstrap replicates), 80\% have been previously reported as physical interactions. Moreover, third-order interactions, e.g., between Zelda (Zld), Giant (Gt), and Twist (Twi), suggest high-order relationships that are candidates for follow-up experiments. In human-derived cells, iRF rediscovered a central role of H3K36me3 in chromatin-mediated splicing regulation and identified interesting fifth- and sixth-order interactions, indicative of multivalent nucleosomes with specific roles in splicing regulation. By decoupling the order of interactions from the computational cost of identification, iRF opens additional avenues of inquiry into the molecular mechanisms underlying genome biology.},
  file = {/Users/luke/Zotero/storage/CMR8P5Z3/Basu et al. - 2018 - Iterative random forests to discover predictive an.pdf}
}

@inproceedings{battagliaInteractionNetworksLearning2016,
  title = {Interaction networks for learning about objects, relations and physics},
  booktitle = {Proceedings of the 30th {{International Conference}} on {{Neural Information Processing Systems}}},
  author = {Battaglia, Peter and Pascanu, Razvan and Lai, Matthew and Rezende, Danilo Jimenez and {kavukcuoglu}, Koray},
  date = {2016-12-05},
  series = {{{NIPS}}'16},
  pages = {4509--4517},
  publisher = {Curran Associates Inc.},
  location = {Red Hook, NY, USA},
  abstract = {Reasoning about objects, relations, and physics is central to human intelligence, and a key goal of artificial intelligence. Here we introduce the interaction network, a model which can reason about how objects in complex systems interact, supporting dynamical predictions, as well as inferences about the abstract properties of the system. Our model takes graphs as input, performs object- and relation-centric reasoning in a way that is analogous to a simulation, and is implemented using deep neural networks. We evaluate its ability to reason about several challenging physical domains: n-body problems, rigid-body collision, and non-rigid dynamics. Our results show it can be trained to accurately simulate the physical trajectories of dozens of objects over thousands of time steps, estimate abstract quantities such as energy, and generalize automatically to systems with different numbers and configurations of objects and relations. Our interaction network implementation is the first general-purpose, learnable physics engine, and a powerful general framework for reasoning about object and relations in a wide variety of complex real-world domains.},
  isbn = {978-1-5108-3881-9},
  file = {/Users/luke/Zotero/storage/ZUQBW9B8/Battaglia et al. - 2016 - Interaction networks for learning about objects, r.pdf}
}

@misc{battagliaRelationalInductiveBiases2018,
  title = {Relational inductive biases, deep learning, and graph networks},
  author = {Battaglia, Peter W. and Hamrick, Jessica B. and Bapst, Victor and Sanchez-Gonzalez, Alvaro and Zambaldi, Vinicius and Malinowski, Mateusz and Tacchetti, Andrea and Raposo, David and Santoro, Adam and Faulkner, Ryan and Gulcehre, Caglar and Song, Francis and Ballard, Andrew and Gilmer, Justin and Dahl, George and Vaswani, Ashish and Allen, Kelsey and Nash, Charles and Langston, Victoria and Dyer, Chris and Heess, Nicolas and Wierstra, Daan and Kohli, Pushmeet and Botvinick, Matt and Vinyals, Oriol and Li, Yujia and Pascanu, Razvan},
  date = {2018-10-17},
  eprint = {1806.01261},
  eprinttype = {arXiv},
  eprintclass = {cs.LG},
  url = {http://arxiv.org/abs/1806.01261},
  urldate = {2024-05-05},
  abstract = {Artificial intelligence (AI) has undergone a renaissance recently, making major progress in key domains such as vision, language, control, and decision-making. This has been due, in part, to cheap data and cheap compute resources, which have fit the natural strengths of deep learning. However, many defining characteristics of human intelligence, which developed under much different pressures, remain out of reach for current approaches. In particular, generalizing beyond one's experiences--a hallmark of human intelligence from infancy--remains a formidable challenge for modern AI. The following is part position paper, part review, and part unification. We argue that combinatorial generalization must be a top priority for AI to achieve human-like abilities, and that structured representations and computations are key to realizing this objective. Just as biology uses nature and nurture cooperatively, we reject the false choice between "hand-engineering" and "end-to-end" learning, and instead advocate for an approach which benefits from their complementary strengths. We explore how using relational inductive biases within deep learning architectures can facilitate learning about entities, relations, and rules for composing them. We present a new building block for the AI toolkit with a strong relational inductive bias--the graph network--which generalizes and extends various approaches for neural networks that operate on graphs, and provides a straightforward interface for manipulating structured knowledge and producing structured behaviors. We discuss how graph networks can support relational reasoning and combinatorial generalization, laying the foundation for more sophisticated, interpretable, and flexible patterns of reasoning. As a companion to this paper, we have released an open-source software library for building graph networks, with demonstrations of how to use them in practice.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/luke/Zotero/storage/96C4HPTC/Battaglia et al. - 2018 - Relational inductive biases, deep learning, and gr.pdf;/Users/luke/Zotero/storage/PBCW7X4J/1806.html},
  note = {arXiv:1806.01261}
}

@inproceedings{bhatEvaluatingEfficacyTestdriven2006,
  title = {Evaluating the efficacy of test-driven development: industrial case studies},
  shorttitle = {Evaluating the efficacy of test-driven development},
  booktitle = {Proceedings of the 2006 {{ACM}}/{{IEEE}} international symposium on {{Empirical}} software engineering},
  author = {Bhat, Thirumalesh and Nagappan, Nachiappan},
  date = {2006-09-21},
  series = {{{ISESE}} '06},
  pages = {356--363},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  url = {https://doi.org/10.1145/1159733.1159787},
  urldate = {2023-12-18},
  abstract = {This paper discusses software development using the Test Driven Development (TDD) methodology in two different environments (Windows and MSN divisions) at Microsoft. In both these case studies we measure the various context, product and outcome measures to compare and evaluate the efficacy of TDD. We observed a significant increase in quality of the code (greater than two times) for projects developed using TDD compared to similar projects developed in the same organization in a non-TDD fashion. The projects also took at least 15\% extra upfront time for writing the tests. Additionally, the unit tests have served as auto documentation for the code when libraries/APIs had to be used as well as for code maintenance.},
  isbn = {978-1-59593-218-1},
  keywords = {software quality,test-driven development}
}

@inproceedings{bianchiniFaceSpottingColor2003,
  title = {Face {{Spotting}} in {{Color Images}} using {{Recursive Neural Networks}}},
  author = {Bianchini, M. and Mazzoni, P. and Sarti, L. and Scarselli, F.},
  date = {2003},
  url = {https://www.semanticscholar.org/paper/Face-Spotting-in-Color-Images-using-Recursive-Bianchini-Mazzoni/12182b550685b5097a1539c8dfa18c8aa49bba36},
  urldate = {2024-02-23},
  abstract = {Images containing faces are fundamental for vision– based human computer interaction. Several problems concerning the presence of faces in images, including face recognition, pose estimation, and expression recognition have focused the research efforts in the last few years. Moreover, those problems are usually solved having assumed that the face was previously localized, often via heuristics based on prototypes of the whole face or significant details. Therefore, each fully automated system, which is able to analyze the information contained in images of faces, requires an efficient face localization method. In this paper, we propose a novel approach to the solution of the face localization problem using recursive neural networks. In particular, the proposed approach assumes a graph–based representation of images that combines structural and sub–symbolic visual features. Each image is represented by a forest of trees. Such trees are then processed by recursive neural networks, in order to establish the eventual presence and the position of faces inside the image. Some preliminary experiments on snapshots from video sequences are reported, showing very promising results.}
}

@inproceedings{bianchiniRecursiveProcessingCyclic2002,
  title = {Recursive processing of cyclic graphs},
  booktitle = {Proceedings of the 2002 {{International Joint Conference}} on {{Neural Networks}}. {{IJCNN}}'02 ({{Cat}}. {{No}}.{{02CH37290}})},
  author = {Bianchini, M. and Gori, M. and Scarselli, F.},
  date = {2002-05},
  volume = {1},
  pages = {154-159 vol.1},
  issn = {1098-7576},
  url = {https://ieeexplore.ieee.org/document/1005461},
  urldate = {2024-02-23},
  abstract = {Recursive neural networks are a powerful tool for processing structured data. According to the recursive learning paradigm, the information to be processed consists of directed positional acyclic graphs (DPAGs). In fact, recursive networks are fed following the partial order defined by the links of the graph. Unfortunately, the hypothesis of processing DPAGs is sometimes too restrictive, being the nature of some real-world problems intrinsically disordered and cyclic. In the paper, a methodology is proposed which allows us to map any cyclic directed graph into a "recursive-equivalent" tree. Therefore, the computational power of recursive networks is definitely established, also clarifying the underlying limitations of the model.},
  eventtitle = {Proceedings of the 2002 {{International Joint Conference}} on {{Neural Networks}}. {{IJCNN}}'02 ({{Cat}}. {{No}}.{{02CH37290}})},
  keywords = {Chemical compounds,Chemistry,HTML,Image databases,Image retrieval,Information retrieval,Marine vehicles,Multimedia databases,Neural networks,Tree graphs},
  file = {/Users/luke/Zotero/storage/9XT9QBB9/Bianchini et al. - 2002 - Recursive processing of cyclic graphs.pdf;/Users/luke/Zotero/storage/BLQSW4FS/1005461.html}
}

@article{birkhoffStructureAbstractAlgebras1935,
  title = {On the {{Structure}} of {{Abstract Algebras}}},
  author = {Birkhoff, Garrett},
  date = {1935-10},
  journaltitle = {Mathematical Proceedings of the Cambridge Philosophical Society},
  volume = {31},
  number = {4},
  pages = {433--454},
  issn = {1469-8064, 0305-0041},
  url = {https://www.cambridge.org/core/journals/mathematical-proceedings-of-the-cambridge-philosophical-society/article/on-the-structure-of-abstract-algebras/D142B3886A3B7A218D8DF8E6DDA2B5B1},
  urldate = {2024-03-07},
  abstract = {The following paper is a study of abstract algebras qua abstract algebras. As no vocabulary suitable for this purpose is current, I have been forced to use a number of new terms, and extend the meaning of some accepted ones.},
  langid = {english},
  file = {/Users/luke/Zotero/storage/H7IHQLGN/Birkhoff - 1935 - On the Structure of Abstract Algebras.pdf}
}

@inproceedings{bodnarNeuralSheafDiffusion2022,
  title = {Neural {{Sheaf Diffusion}}: {{A Topological Perspective}} on {{Heterophily}} and {{Oversmoothing}} in {{GNNs}}},
  shorttitle = {Neural {{Sheaf Diffusion}}},
  author = {Bodnar, Cristian and Giovanni, Francesco Di and Chamberlain, Benjamin Paul and Lio, Pietro and Bronstein, Michael M.},
  date = {2022-05-16},
  url = {https://openreview.net/forum?id=vbPsD-BhOZ},
  urldate = {2023-11-01},
  abstract = {Cellular sheaves equip graphs with a ``geometrical'' structure by assigning vector spaces and linear maps to nodes and edges. Graph Neural Networks (GNNs) implicitly assume a graph with a trivial underlying sheaf. This choice is reflected in the structure of the graph Laplacian operator, the properties of the associated diffusion equation, and the characteristics of the convolutional models that discretise this equation. In this paper, we use cellular sheaf theory to show that the underlying geometry of the graph is deeply linked with the performance of GNNs in heterophilic settings and their oversmoothing behaviour. By considering a hierarchy of increasingly general sheaves, we study how the ability of the sheaf diffusion process to achieve linear separation of the classes in the infinite time limit expands. At the same time, we prove that when the sheaf is non-trivial, discretised parametric diffusion processes have greater control than GNNs over their asymptotic behaviour. On the practical side, we study how sheaves can be learned from data. The resulting sheaf diffusion models have many desirable properties that address the limitations of classical graph diffusion equations (and corresponding GNN models) and obtain competitive results in heterophilic settings. Overall, our work provides new connections between GNNs and algebraic topology and would be of interest to both fields.},
  eventtitle = {{{NeurIPS}} 2022},
  langid = {english},
  file = {/Users/luke/Zotero/storage/T8CRBTA6/Bodnar et al. - 2022 - Neural Sheaf Diffusion A Topological Perspective .pdf}
}

@thesis{bodnarTopologicalDeepLearning2023,
  type = {phdthesis},
  title = {Topological {{Deep Learning}}: {{Graphs}}, {{Complexes}}, {{Sheaves}}},
  shorttitle = {Topological {{Deep Learning}}},
  author = {Bodnar, Cristian},
  date = {2023-06-20T12:04:30Z},
  institution = {University of Cambridge},
  url = {https://www.repository.cam.ac.uk/handle/1810/350982},
  urldate = {2023-10-13},
  abstract = {The types of spaces where data resides - graphs, meshes, grids, manifolds - are becoming increasingly varied and heterogeneous. Therefore, translating ideas, models, and theoretical results between different domains is becoming more and more challenging. Nonetheless, two fundamental principles unite all these settings. The first states that data is localised, meaning that data is associated with some regions of the underlying space. The second says that data is relational, and this relational structure reflects how the various regions of the space overlap. It is natural to formalise these axioms using algebraic topology. The "space'' in question is a topological space - a set with a neighbourhood structure - and the data attached to its neighbourhoods are algebraic objects like vector spaces. Since graphs, manifolds and everything in between is a topological space, we adopt this mathematical viewpoint to smoothly transition between domains, improve our theoretical understanding of existent models and design new ones, including for spaces that are yet to be explored in Machine Learning. Guided by this perspective, this work introduces Topological Deep Learning, a research programme studying (deep) models performing inference on data glued to a topological space. This thesis includes four research works expanding upon the directions outlined above. The first work proposes Message Passing Simplicial Networks (MPSNs), a family of models operating on simplicial complexes, a higher-dimensional generalisation of graphs coming from algebraic topology. We study the symmetries these models must satisfy, the topological invariants that describe their behaviour, and how they can learn representations based on discrete differential forms. The second work takes this generalisation further to cell complexes, a class of spaces that also subsume simplicial complexes. We show their additional flexibility benefits molecular applications, where the resulting models outperform prior art on molecular property prediction tasks. The third work proposes a general topological framework for constructing graph coarsening (aka pooling) operators in a way that naturally generalises existing pooling approaches in computer vision. We show that this framework can be used to construct graph-based hierarchical models and visualise attributed graphs. Finally, the last work introduces a new perspective on graph models based on sheaf theory, a subfield of algebraic topology. Sheaves, which are mathematical data structures that naturally store the data attached to a topological space and its relational structure, faithfully realise the axiomatic principles of Topological Deep Learning. We show that sheaf structures on graphs are intimately connected with the asymptotic behaviour of message passing graph models and exploit these connections to design new sheaf-based convolutional architectures. We demonstrate that these models can cope with the challenges of oversmoothing and heterophilic graphs, which affect many existent graph models. Overall, this thesis introduces a novel topological perspective on deep learning for structured data, whose ramifications establish many new connections with algebraic topology.},
  langid = {english},
  file = {/Users/luke/Zotero/storage/5XS7DN2W/Bodnar - 2023 - Topological Deep Learning Graphs, Complexes, Shea.pdf}
}

@inproceedings{bodnarWeisfeilerLehmanGo2021,
  title = {Weisfeiler and {{Lehman Go Cellular}}: {{CW Networks}}},
  shorttitle = {Weisfeiler and {{Lehman Go Cellular}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Bodnar, Cristian and Frasca, Fabrizio and Otter, Nina and Wang, Yuguang and Liò, Pietro and Montufar, Guido F and Bronstein, Michael},
  date = {2021},
  volume = {34},
  pages = {2625--2640},
  publisher = {Curran Associates, Inc.},
  url = {https://proceedings.neurips.cc/paper/2021/hash/157792e4abb490f99dbd738483e0d2d4-Abstract.html},
  urldate = {2024-05-20},
  abstract = {Graph Neural Networks (GNNs) are limited in their expressive power, struggle with long-range interactions and lack a principled way to model higher-order structures. These problems can be attributed to the strong coupling between the computational graph and the input graph structure. The recently proposed Message Passing Simplicial Networks naturally decouple these elements by performing message passing on the clique complex of the graph. Nevertheless, these models can be severely constrained by the rigid combinatorial structure of Simplicial Complexes (SCs). In this work, we extend recent theoretical results on SCs to regular Cell Complexes, topological objects that flexibly subsume SCs and graphs. We show that this generalisation provides a powerful set of graph "lifting" transformations, each leading to a unique hierarchical message passing procedure. The resulting methods, which we collectively call CW Networks (CWNs), are strictly more powerful than the WL test and not less powerful than the 3-WL test. In particular, we demonstrate the effectiveness of one such scheme, based on rings, when applied to molecular graph problems. The proposed architecture benefits from provably larger expressivity than commonly used GNNs, principled modelling of higher-order signals and from compressing the distances between nodes. We demonstrate that our model achieves state-of-the-art results on a variety of molecular datasets.},
  file = {/Users/luke/Zotero/storage/BBZDPA8D/Bodnar et al. - 2021 - Weisfeiler and Lehman Go Cellular CW Networks.pdf}
}

@inproceedings{bojchevskiDeepGaussianEmbedding2018,
  title = {Deep {{Gaussian Embedding}} of {{Graphs}}: {{Unsupervised Inductive Learning}} via {{Ranking}}},
  shorttitle = {Deep {{Gaussian Embedding}} of {{Graphs}}},
  author = {Bojchevski, Aleksandar and Günnemann, Stephan},
  date = {2018-02-15},
  url = {https://openreview.net/forum?id=r1ZdKJ-0W},
  urldate = {2023-12-01},
  abstract = {Methods that learn representations of nodes in a graph play a critical role in network analysis since they enable many downstream learning tasks. We propose Graph2Gauss - an approach that can efficiently learn versatile node embeddings on large scale (attributed) graphs that show strong performance on tasks such as link prediction and node classification. Unlike most approaches that represent nodes as point vectors in a low-dimensional continuous space, we embed each node as a Gaussian distribution, allowing us to capture uncertainty about the representation. Furthermore, we propose an unsupervised method that handles inductive learning scenarios and is applicable to different types of graphs: plain/attributed, directed/undirected. By leveraging both the network structure and the associated node attributes, we are able to generalize to unseen nodes without additional training. To learn the embeddings we adopt a personalized ranking formulation w.r.t. the node distances that exploits the natural ordering of the nodes imposed by the network structure. Experiments on real world networks demonstrate the high performance of our approach, outperforming state-of-the-art network embedding methods on several different tasks. Additionally, we demonstrate the benefits of modeling uncertainty - by analyzing it we can estimate neighborhood diversity and detect the intrinsic latent dimensionality of a graph.},
  eventtitle = {International {{Conference}} on {{Learning Representations}}},
  langid = {english},
  file = {/Users/luke/Zotero/storage/UWTLTUF6/Bojchevski and Günnemann - 2018 - Deep Gaussian Embedding of Graphs Unsupervised In.pdf}
}

@inproceedings{bollackerFreebaseCollaborativelyCreated2008,
  title = {Freebase: a collaboratively created graph database for structuring human knowledge},
  shorttitle = {Freebase},
  booktitle = {Proceedings of the 2008 {{ACM SIGMOD}} international conference on {{Management}} of data},
  author = {Bollacker, Kurt and Evans, Colin and Paritosh, Praveen and Sturge, Tim and Taylor, Jamie},
  date = {2008-06-09},
  series = {{{SIGMOD}} '08},
  pages = {1247--1250},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  url = {https://doi.org/10.1145/1376616.1376746},
  urldate = {2024-07-05},
  abstract = {Freebase is a practical, scalable tuple database used to structure general human knowledge. The data in Freebase is collaboratively created, structured, and maintained. Freebase currently contains more than 125,000,000 tuples, more than 4000 types, and more than 7000 properties. Public read/write access to Freebase is allowed through an HTTP-based graph-query API using the Metaweb Query Language (MQL) as a data query and manipulation language. MQL provides an easy-to-use object-oriented interface to the tuple data in Freebase and is designed to facilitate the creation of collaborative, Web-based data-oriented applications.},
  isbn = {978-1-60558-102-6}
}

@misc{braithwaiteHeterogeneousSheafNeural2024,
  title = {Heterogeneous {{Sheaf Neural Networks}}},
  author = {Braithwaite, Luke and Duta, Iulia and Liò, Pietro},
  date = {2024-09-12},
  eprint = {2409.08036},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2409.08036},
  urldate = {2024-09-13},
  abstract = {Heterogeneous graphs, with nodes and edges of different types, are commonly used to model relational structures in many real-world applications. Standard Graph Neural Networks (GNNs) struggle to process heterogeneous data due to oversmoothing. Instead, current approaches have focused on accounting for the heterogeneity in the model architecture, leading to increasingly complex models. Inspired by recent work, we propose using cellular sheaves to model the heterogeneity in the graph's underlying topology. Instead of modelling the data as a graph, we represent it as cellular sheaves, which allows us to encode the different data types directly in the data structure, eliminating the need to inject them into the architecture. We introduce HetSheaf, a general framework for heterogeneous sheaf neural networks, and a series of heterogeneous sheaf predictors to better encode the data's heterogeneity into the sheaf structure. Finally, we empirically evaluate HetSheaf on several standard heterogeneous graph benchmarks, achieving competitive results whilst being more parameter-efficient.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/luke/Zotero/storage/JRLBYAGG/Braithwaite et al. - 2024 - Heterogeneous Sheaf Neural Networks.pdf;/Users/luke/Zotero/storage/WZSWDN87/2409.html},
  note = {arXiv:2409.08036}
}

@misc{bronsteinGeometricDeepLearning2017,
  title = {Geometric deep learning: going beyond {{Euclidean}} data},
  shorttitle = {Geometric deep learning},
  author = {Bronstein, Michael M. and Bruna, Joan and LeCun, Yann and Szlam, Arthur and Vandergheynst, Pierre},
  date = {2017-07},
  journaltitle = {IEEE Signal Processing Magazine},
  shortjournal = {IEEE Signal Process. Mag.},
  volume = {34},
  number = {4},
  eprint = {1611.08097},
  eprinttype = {arXiv},
  eprintclass = {cs},
  pages = {18--42},
  issn = {1053-5888, 1558-0792},
  url = {http://arxiv.org/abs/1611.08097},
  urldate = {2024-09-09},
  abstract = {Many scientific fields study data with an underlying structure that is a non-Euclidean space. Some examples include social networks in computational social sciences, sensor networks in communications, functional networks in brain imaging, regulatory networks in genetics, and meshed surfaces in computer graphics. In many applications, such geometric data are large and complex (in the case of social networks, on the scale of billions), and are natural targets for machine learning techniques. In particular, we would like to use deep neural networks, which have recently proven to be powerful tools for a broad range of problems from computer vision, natural language processing, and audio analysis. However, these tools have been most successful on data with an underlying Euclidean or grid-like structure, and in cases where the invariances of these structures are built into networks used to model them. Geometric deep learning is an umbrella term for emerging techniques attempting to generalize (structured) deep neural models to non-Euclidean domains such as graphs and manifolds. The purpose of this paper is to overview different examples of geometric deep learning problems and present available solutions, key difficulties, applications, and future research directions in this nascent field.},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/luke/Zotero/storage/5EJ4DW9T/Bronstein et al. - 2017 - Geometric deep learning going beyond Euclidean da.pdf;/Users/luke/Zotero/storage/3H3BH7C6/1611.html},
  note = {arXiv:1611.08097}
}

@misc{bronsteinGeometricDeepLearning2021,
  title = {Geometric {{Deep Learning}}: {{Grids}}, {{Groups}}, {{Graphs}}, {{Geodesics}}, and {{Gauges}}},
  shorttitle = {Geometric {{Deep Learning}}},
  author = {Bronstein, Michael M. and Bruna, Joan and Cohen, Taco and Veličković, Petar},
  date = {2021-05-02},
  eprint = {2104.13478},
  eprinttype = {arXiv},
  eprintclass = {cs.LG},
  url = {http://arxiv.org/abs/2104.13478},
  urldate = {2023-10-11},
  abstract = {The last decade has witnessed an experimental revolution in data science and machine learning, epitomised by deep learning methods. Indeed, many high-dimensional learning tasks previously thought to be beyond reach -- such as computer vision, playing Go, or protein folding -- are in fact feasible with appropriate computational scale. Remarkably, the essence of deep learning is built from two simple algorithmic principles: first, the notion of representation or feature learning, whereby adapted, often hierarchical, features capture the appropriate notion of regularity for each task, and second, learning by local gradient-descent type methods, typically implemented as backpropagation. While learning generic functions in high dimensions is a cursed estimation problem, most tasks of interest are not generic, and come with essential pre-defined regularities arising from the underlying low-dimensionality and structure of the physical world. This text is concerned with exposing these regularities through unified geometric principles that can be applied throughout a wide spectrum of applications. Such a 'geometric unification' endeavour, in the spirit of Felix Klein's Erlangen Program, serves a dual purpose: on one hand, it provides a common mathematical framework to study the most successful neural network architectures, such as CNNs, RNNs, GNNs, and Transformers. On the other hand, it gives a constructive procedure to incorporate prior physical knowledge into neural architectures and provide principled way to build future architectures yet to be invented.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computational Geometry,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/luke/Zotero/storage/5BQQ9Y62/Bronstein et al. - 2021 - Geometric Deep Learning Grids, Groups, Graphs, Ge.pdf;/Users/luke/Zotero/storage/JLKMGA6X/2104.html},
  note = {arXiv:2104.13478}
}

@book{burrisCourseUniversalAlgebra1981,
  title = {A course in universal algebra},
  author = {Burris, Stanley},
  namea = {{H P Sankappanavar}},
  nameatype = {collaborator},
  date = {1981},
  series = {Graduate texts in mathematics},
  number = {78},
  publisher = {Springer-Verlag, 1981, ©1981},
  location = {New York ; Berlin},
  isbn = {978-0-387-90578-5},
  langid = {english},
  keywords = {Algebra Universal}
}

@misc{busbridgeRelationalGraphAttention2019,
  title = {Relational {{Graph Attention Networks}}},
  author = {Busbridge, Dan and Sherburn, Dane and Cavallo, Pietro and Hammerla, Nils Y.},
  date = {2019-04-11},
  eprint = {1904.05811},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1904.05811},
  urldate = {2023-12-03},
  abstract = {We investigate Relational Graph Attention Networks, a class of models that extends non-relational graph attention mechanisms to incorporate relational information, opening up these methods to a wider variety of problems. A thorough evaluation of these models is performed, and comparisons are made against established benchmarks. To provide a meaningful comparison, we retrain Relational Graph Convolutional Networks, the spectral counterpart of Relational Graph Attention Networks, and evaluate them under the same conditions. We find that Relational Graph Attention Networks perform worse than anticipated, although some configurations are marginally beneficial for modelling molecular properties. We provide insights as to why this may be, and suggest both modifications to evaluation strategies, as well as directions to investigate for future work.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/luke/Zotero/storage/X579TJ6P/Busbridge et al. - 2019 - Relational Graph Attention Networks.pdf;/Users/luke/Zotero/storage/5D9PGPRN/1904.html},
  note = {arXiv:1904.05811}
}

@misc{caiNoteOverSmoothingGraph2020,
  title = {A {{Note}} on {{Over-Smoothing}} for {{Graph Neural Networks}}},
  author = {Cai, Chen and Wang, Yusu},
  date = {2020-06-23},
  eprint = {2006.13318},
  eprinttype = {arXiv},
  eprintclass = {cs.LG},
  url = {http://arxiv.org/abs/2006.13318},
  urldate = {2024-03-19},
  abstract = {Graph Neural Networks (GNNs) have achieved a lot of success on graph-structured data. However, it is observed that the performance of graph neural networks does not improve as the number of layers increases. This effect, known as over-smoothing, has been analyzed mostly in linear cases. In this paper, we build upon previous results \textbackslash cite\{oono2019graph\} to further analyze the over-smoothing effect in the general graph neural network architecture. We show when the weight matrix satisfies the conditions determined by the spectrum of augmented normalized Laplacian, the Dirichlet energy of embeddings will converge to zero, resulting in the loss of discriminative power. Using Dirichlet energy to measure "expressiveness" of embedding is conceptually clean; it leads to simpler proofs than \textbackslash cite\{oono2019graph\} and can handle more non-linearities.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/luke/Zotero/storage/NWW3KFJ7/Cai and Wang - 2020 - A Note on Over-Smoothing for Graph Neural Networks.pdf;/Users/luke/Zotero/storage/VGZIKQ3G/2006.html},
  note = {arXiv:2006.13318}
}

@article{cencettiTemporalPropertiesHigherorder2021,
  title = {Temporal properties of higher-order interactions in social networks},
  author = {Cencetti, Giulia and Battiston, Federico and Lepri, Bruno and Karsai, Márton},
  date = {2021-03-29},
  journaltitle = {Scientific Reports},
  shortjournal = {Sci Rep},
  volume = {11},
  number = {1},
  pages = {7028},
  publisher = {Nature Publishing Group},
  issn = {2045-2322},
  url = {https://www.nature.com/articles/s41598-021-86469-8},
  urldate = {2024-03-19},
  abstract = {Human social interactions in local settings can be experimentally detected by recording the physical proximity and orientation of people. Such interactions, approximating face-to-face communications, can be effectively represented as time varying social networks with links being unceasingly created and destroyed over time. Traditional analyses of temporal networks have addressed mostly pairwise interactions, where links describe dyadic connections among individuals. However, many network dynamics are hardly ascribable to pairwise settings but often comprise larger groups, which are better described by higher-order interactions. Here we investigate the higher-order organizations of temporal social networks by analyzing five publicly available datasets collected in different social settings. We find that higher-order interactions are ubiquitous and, similarly to their pairwise counterparts, characterized by heterogeneous dynamics, with bursty trains of rapidly recurring higher-order events separated by long periods of inactivity. We investigate the evolution and formation of groups by looking at the transition rates between different higher-order structures. We find that in more spontaneous social settings, group are characterized by slower formation and disaggregation, while in work settings these phenomena are more abrupt, possibly reflecting pre-organized social dynamics. Finally, we observe temporal reinforcement suggesting that the longer a group stays together the higher the probability that the same interaction pattern persist in the future. Our findings suggest the importance of considering the higher-order structure of social interactions when investigating human temporal dynamics.},
  langid = {english},
  keywords = {Complex networks,Computational science,Nonlinear phenomena,Scientific data},
  file = {/Users/luke/Zotero/storage/NK6ZPJ8N/Cencetti et al. - 2021 - Temporal properties of higher-order interactions i.pdf}
}

@inproceedings{cenRepresentationLearningAttributed2019,
  title = {Representation {{Learning}} for {{Attributed Multiplex Heterogeneous Network}}},
  booktitle = {Proceedings of the 25th {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} \& {{Data Mining}}},
  author = {Cen, Yukuo and Zou, Xu and Zhang, Jianwei and Yang, Hongxia and Zhou, Jingren and Tang, Jie},
  date = {2019-07-25},
  series = {{{KDD}} '19},
  pages = {1358--1368},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  url = {https://dl.acm.org/doi/10.1145/3292500.3330964},
  urldate = {2024-08-26},
  abstract = {Network embedding (or graph embedding) has been widely used in many real-world applications. However, existing methods mainly focus on networks with single-typed nodes/edges and cannot scale well to handle large networks. Many real-world networks consist of billions of nodes and edges of multiple types, and each node is associated with different attributes. In this paper, we formalize the problem of embedding learning for the Attributed Multiplex Heterogeneous Network and propose a unified framework to address this problem. The framework supports both transductive and inductive learning. We also give the theoretical analysis of the proposed framework, showing its connection with previous works and proving its better expressiveness. We conduct systematical evaluations for the proposed framework on four different genres of challenging datasets: Amazon, YouTube, Twitter, and Alibaba. Experimental results demonstrate that with the learned embeddings from the proposed framework, we can achieve statistically significant improvements (e.g., 5.99-28.23\% lift by F1 scores; p\&lt;\&lt;0.01, t-test) over previous state-of-the-art methods for link prediction. The framework has also been successfully deployed on the recommendation system of a worldwide leading e-commerce company, Alibaba Group. Results of the offline A/B tests on product recommendation further confirm the effectiveness and efficiency of the framework in practice.},
  isbn = {978-1-4503-6201-6},
  file = {/Users/luke/Zotero/storage/ERNUJL6V/Cen et al. - 2019 - Representation Learning for Attributed Multiplex H.pdf}
}

@misc{cesaAlgebraicTopologicalNetworks2023,
  title = {Algebraic {{Topological Networks}} via the {{Persistent Local Homology Sheaf}}},
  author = {Cesa, Gabriele and Behboodi, Arash},
  date = {2023-11-16},
  eprint = {2311.10156},
  eprinttype = {arXiv},
  eprintclass = {cs.LG},
  url = {http://arxiv.org/abs/2311.10156},
  urldate = {2024-01-24},
  abstract = {In this work, we introduce a novel approach based on algebraic topology to enhance graph convolution and attention modules by incorporating local topological properties of the data. To do so, we consider the framework of sheaf neural networks, which has been previously leveraged to incorporate additional structure into graph neural networks' features and construct more expressive, non-isotropic messages. Specifically, given an input simplicial complex (e.g. generated by the cliques of a graph or the neighbors in a point cloud), we construct its local homology sheaf, which assigns to each node the vector space of its local homology. The intermediate features of our networks live in these vector spaces and we leverage the associated sheaf Laplacian to construct more complex linear messages between them. Moreover, we extend this approach by considering the persistent version of local homology associated with a weighted simplicial complex (e.g., built from pairwise distances of nodes embeddings). This i) solves the problem of the lack of a natural choice of basis for the local homology vector spaces and ii) makes the sheaf itself differentiable, which enables our models to directly optimize the topology of their intermediate features.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Mathematics - Algebraic Topology},
  file = {/Users/luke/Zotero/storage/H2UTX6NN/Cesa and Behboodi - 2023 - Algebraic Topological Networks via the Persistent .pdf;/Users/luke/Zotero/storage/RJ2DG3VE/2311.html},
  note = {arXiv:2311.10156}
}

@inproceedings{chamberlainGRANDGraphNeural2021,
  title = {{{GRAND}}: {{Graph Neural Diffusion}}},
  shorttitle = {{{GRAND}}},
  booktitle = {Proceedings of the 38th {{International Conference}} on {{Machine Learning}}},
  author = {Chamberlain, Ben and Rowbottom, James and Gorinova, Maria I. and Bronstein, Michael and Webb, Stefan and Rossi, Emanuele},
  date = {2021-07-01},
  pages = {1407--1418},
  publisher = {PMLR},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v139/chamberlain21a.html},
  urldate = {2023-12-25},
  abstract = {We present Graph Neural Diffusion (GRAND) that approaches deep learning on graphs as a continuous diffusion process and treats Graph Neural Networks (GNNs) as discretisations of an underlying PDE. In our model, the layer structure and topology correspond to the discretisation choices of temporal and spatial operators. Our approach allows a principled development of a broad new class of GNNs that are able to address the common plights of graph learning models such as depth, oversmoothing, and bottlenecks. Key to the success of our models are stability with respect to perturbations in the data and this is addressed for both implicit and explicit discretisation schemes. We develop linear and nonlinear versions of GRAND, which achieve competitive results on many standard graph benchmarks.},
  eventtitle = {{{ICML}} 2021},
  langid = {english},
  file = {/Users/luke/Zotero/storage/EDTTXMMT/Chamberlain et al. - 2021 - GRAND Graph Neural Diffusion.pdf}
}

@article{chanGPUAcceleratedTdistributed2019,
  title = {{{GPU}} accelerated t-distributed stochastic neighbor embedding},
  author = {Chan, David M. and Rao, Roshan and Huang, Forrest and Canny, John F.},
  date = {2019-09-01},
  journaltitle = {Journal of Parallel and Distributed Computing},
  shortjournal = {Journal of Parallel and Distributed Computing},
  volume = {131},
  pages = {1--13},
  issn = {0743-7315},
  url = {https://www.sciencedirect.com/science/article/pii/S074373151830875X},
  urldate = {2024-02-08},
  abstract = {Modern datasets and models are notoriously difficult to explore and analyze due to their inherent high dimensionality and massive numbers of samples. Existing visualization methods which employ dimensionality reduction to two or three dimensions are often inefficient and/or ineffective for these datasets. This paper introduces t-SNE-CUDA, a GPU-accelerated implementation of t-Distributed Symmetric Neighbor Embedding (t-SNE) for visualizing datasets and models. t-SNE-CUDA significantly outperforms current implementations with 15-700x speedups on the CIFAR-10 and MNIST datasets. These speedups enable, for the first time, large scale visualizations of modern computer vision datasets such as ImageNet, as well as larger NLP datasets such as GloVe. From these new visualizations, we can draw a number of interesting conclusions. In addition, the performance on machine learning datasets allows us to compute t-SNE embeddings in close to real time, and we explore the applications of such fast embeddings in the domain of importance sampling for neural network training.},
  keywords = {Applications,CUDA,Embedding,GPU computing,Parallel computing,T-SNE}
}

@article{chappellSaysNorthKorea2017,
  entrysubtype = {newspaper},
  title = {U.{{S}}. {{Says North Korea}} '{{Directly Responsible}}' {{For WannaCry Ransomware Attack}}},
  author = {Chappell, Bill and Neuman, Scott},
  date = {2017-12-19T02:27:00-05:00},
  journaltitle = {NPR},
  url = {https://www.npr.org/sections/thetwo-way/2017/12/19/571854614/u-s-says-north-korea-directly-responsible-for-wannacry-ransomware-attack},
  urldate = {2023-12-23},
  abstract = {Homeland security adviser Tom Bossert says that after careful investigation, the U.S. is sure Pyongyang carried out an attack that caused "havoc and destruction" in May.},
  journalsubtitle = {International},
  langid = {english},
  file = {/Users/luke/Zotero/storage/EMCC4S6T/u-s-says-north-korea-directly-responsible-for-wannacry-ransomware-attack.html}
}

@misc{chenPreventingOverSmoothingHypergraph2022,
  title = {Preventing {{Over-Smoothing}} for {{Hypergraph Neural Networks}}},
  author = {Chen, Guanzi and Zhang, Jiying and Xiao, Xi and Li, Yang},
  date = {2022-11-02},
  eprint = {2203.17159},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2203.17159},
  urldate = {2024-03-19},
  abstract = {In recent years, hypergraph learning has attracted great attention due to its capacity in representing complex and high-order relationships. However, current neural network approaches designed for hypergraphs are mostly shallow, thus limiting their ability to extract information from high-order neighbors. In this paper, we show both theoretically and empirically, that the performance of hypergraph neural networks does not improve as the number of layers increases, which is known as the over-smoothing problem. To avoid this issue, we develop a new deep hypergraph convolutional network called Deep-HGCN, which can maintain the heterogeneity of node representation in deep layers. Specifically, we prove that a \$k\$-layer Deep-HGCN simulates a polynomial filter of order \$k\$ with arbitrary coefficients, which can relieve the problem of over-smoothing. Experimental results on various datasets demonstrate the superior performance of the proposed model compared to the state-of-the-art hypergraph learning approaches.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/luke/Zotero/storage/LLCBNMYR/Chen et al. - 2022 - Preventing Over-Smoothing for Hypergraph Neural Ne.pdf;/Users/luke/Zotero/storage/C3FK6SN7/2203.html},
  note = {arXiv:2203.17159}
}

@article{chenSequenceRSequencetoSequenceLearning2021,
  title = {{{SequenceR}}: {{Sequence-to-Sequence Learning}} for {{End-to-End Program Repair}}},
  shorttitle = {{{SequenceR}}},
  author = {Chen, Zimin and Kommrusch, Steve and Tufano, Michele and Pouchet, Louis-Noël and Poshyvanyk, Denys and Monperrus, Martin},
  date = {2021-09-01},
  journaltitle = {IEEE Transactions on Software Engineering},
  volume = {47},
  number = {09},
  pages = {1943--1959},
  publisher = {IEEE Computer Society},
  issn = {0098-5589},
  url = {https://www.computer.org/csdl/journal/ts/2021/09/08827954/1ddbmnbiydi},
  urldate = {2023-12-19},
  abstract = {This paper presents a novel end-to-end approach to program repair based on sequence-to-sequence learning. We devise, implement, and evaluate a technique, called SequenceR, for fixing bugs based on sequence-to-sequence learning on source code. This approach uses the copy mechanism to overcome the unlimited vocabulary problem that occurs with big code. Our system is data-driven; we train it on 35,578 samples, carefully curated from commits to open-source repositories. We evaluate SequenceR on 4,711 independent real bug fixes, as well on the Defects4J benchmark used in program repair research. SequenceR is able to perfectly predict the fixed line for 950/4,711 testing samples, and find correct patches for 14 bugs in Defects4J benchmark. SequenceR captures a wide range of repair operators without any domain-specific top-down design.},
  langid = {english},
  file = {/Users/luke/Zotero/storage/MVLTH2C2/Chen et al. - 2021 - SequenceR Sequence-to-Sequence Learning for End-t.pdf}
}

@inproceedings{chenTensoRFTensorialRadiance2022,
  title = {{{TensoRF}}: {{Tensorial Radiance Fields}}},
  shorttitle = {{{TensoRF}}},
  author = {Chen, Anpei and Xu, Zexiang and Geiger, Andreas and Yu, Jingyi and Su, Hao},
  editor = {Avidan, Shai and Brostow, Gabriel and Cissé, Moustapha and Farinella, Giovanni Maria and Hassner, Tal},
  date = {2022},
  abstract = {We present TensoRF, a novel approach to model and reconstruct radiance fields. Unlike NeRF that purely uses MLPs, we model the radiance field of a scene as a 4D tensor, which represents a 3D voxel grid with per-voxel multi-channel features.  Our central idea is to factorize the 4D scene tensor into multiple compact low-rank tensor components. We demonstrate that applying traditional CANDECOMP/PARAFAC (CP) decomposition – that factorizes tensors into rank-one components with compact vectors – in our framework leads to improvements over vanilla NeRF. To further boost performance, we introduce a novel vector-matrix (VM) decomposition that relaxes the low-rank constraints for two modes of a tensor and factorizes tensors into compact vector and matrix factors. Beyond superior rendering quality, our models with CP and VM decompositions lead to a significantly lower memory footprint in comparison to previous and concurrent works that directly optimize per-voxel features. Experimentally, we demonstrate that TensoRF with CP decomposition achieves fast reconstruction (\$\${$<$}30\$\${$<$}30min) with better rendering quality and even a smaller model size (\$\${$<$}4\$\${$<$}4MB) compared to NeRF. Moreover, TensoRF with VM decomposition further boosts rendering quality and outperforms previous state-of-the-art methods, while reducing the reconstruction time (\$\${$<$}10\$\${$<$}10min) and retaining a compact model size (\$\${$<$}75\$\${$<$}75MB).},
  eventtitle = {{{ECCV}}},
  isbn = {978-3-031-19824-3},
  langid = {english},
  file = {/Users/luke/Zotero/storage/J78UGX3S/Chen et al. - 2022 - TensoRF Tensorial Radiance Fields.pdf}
}

@inproceedings{chienYouAreAllSet2021,
  title = {You are {{AllSet}}: {{A Multiset Function Framework}} for {{Hypergraph Neural Networks}}},
  shorttitle = {You are {{AllSet}}},
  author = {Chien, Eli and Pan, Chao and Peng, Jianhao and Milenkovic, Olgica},
  date = {2021-10-06},
  url = {https://openreview.net/forum?id=hpBTIv2uy_E},
  urldate = {2023-10-26},
  abstract = {Hypergraphs are used to model higher-order interactions amongst agents and there exist many practically relevant instances of hypergraph datasets. To enable the efficient processing of hypergraph data, several hypergraph neural network platforms have been proposed for learning hypergraph properties and structure, with a special focus on node classification tasks. However, almost all existing methods use heuristic propagation rules and offer suboptimal performance on benchmarking datasets. We propose AllSet, a new hypergraph neural network paradigm that represents a highly general framework for (hyper)graph neural networks and for the first time implements hypergraph neural network layers as compositions of two multiset functions that can be efficiently learned for each task and each dataset. The proposed AllSet framework also for the first time integrates Deep Sets and Set Transformers with hypergraph neural networks for the purpose of learning multiset functions and therefore allows for significant modeling flexibility and high expressive power. To evaluate the performance of AllSet, we conduct the most extensive experiments to date involving ten known benchmarking datasets and three newly curated datasets that represent significant challenges for hypergraph node classification. The results demonstrate that our method has the unique ability to either match or outperform all other hypergraph neural networks across the tested datasets: As an example, the performance improvements over existing methods and a new method based on heterogeneous graph neural networks are close to \$4\textbackslash\%\$ on the Yelp and Zoo datasets, and \$3\textbackslash\%\$ on the Walmart dataset.},
  eventtitle = {International {{Conference}} on {{Learning Representations}}},
  langid = {english},
  file = {/Users/luke/Zotero/storage/4QTYIIBR/Chien et al. - 2021 - You are AllSet A Multiset Function Framework for .pdf}
}

@inproceedings{choeClassificationEdgedependentLabels2023,
  title = {Classification of {{Edge-dependent Labels}} of {{Nodes}} in {{Hypergraphs}}},
  booktitle = {Proceedings of the 29th {{ACM SIGKDD Conference}} on {{Knowledge Discovery}} and {{Data Mining}}},
  author = {Choe, Minyoung and Kim, Sunwoo and Yoo, Jaemin and Shin, Kijung},
  date = {2023-08-04},
  series = {{{KDD}} '23},
  pages = {298--309},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  url = {https://dl.acm.org/doi/10.1145/3580305.3599274},
  urldate = {2024-03-24},
  abstract = {A hypergraph is a data structure composed of nodes and hyperedges, where each hyperedge is an any-sized subset of nodes. Due to the flexibility in hyperedge size, hypergraphs represent group interactions (e.g., co-authorship by more than two authors) more naturally and accurately than ordinary graphs. Interestingly, many real-world systems modeled as hypergraphs contain edge-dependent node labels, i.e., node labels that vary depending on hyperedges. For example, on co-authorship datasets, the same author (i.e., a node) can be the primary author in a paper (i.e., a hyperedge) but the corresponding author in another paper (i.e., another hyperedge). In this work, we introduce a classification of edge-dependent node labels as a new problem. This problem can be used as a benchmark task for hypergraph neural networks, which recently have attracted great attention, and also the usefulness of edge-dependent node labels has been verified in various applications. To tackle this problem, we propose WHATsNet, a novel hypergraph neural network that represents the same node differently depending on the hyperedges it participates in by reflecting its varying importance in the hyperedges. To this end, WHATsNet models the relations between nodes within each hyperedge, using their relative centrality as positional encodings. In our experiments, we demonstrate that WHATsNet significantly and consistently outperforms ten competitors on six real-world hypergraphs, and we also show successful applications of WHATsNet to (a) ranking aggregation, (b) node clustering, and (c) product return prediction.},
  isbn = {9798400701030},
  keywords = {edge-dependent node label,graph neural network,hypergraph},
  file = {/Users/luke/Zotero/storage/UCNW6J4C/Choe et al. - 2023 - Classification of Edge-dependent Labels of Nodes i.pdf}
}

@article{ciravegnaLogicExplainedNetworks2023,
  title = {Logic {{Explained Networks}}},
  author = {Ciravegna, Gabriele and Barbiero, Pietro and Giannini, Francesco and Gori, Marco and Liò, Pietro and Maggini, Marco and Melacci, Stefano},
  date = {2023-01-01},
  journaltitle = {Artificial Intelligence},
  shortjournal = {Artificial Intelligence},
  volume = {314},
  pages = {103822},
  issn = {0004-3702},
  url = {https://www.sciencedirect.com/science/article/pii/S000437022200162X},
  urldate = {2024-03-08},
  abstract = {The large and still increasing popularity of deep learning clashes with a major limit of neural network architectures, that consists in their lack of capability in providing human-understandable motivations of their decisions. In situations in which the machine is expected to support the decision of human experts, providing a comprehensible explanation is a feature of crucial importance. The language used to communicate the explanations must be formal enough to be implementable in a machine and friendly enough to be understandable by a wide audience. In this paper, we propose a general approach to Explainable Artificial Intelligence in the case of neural architectures, showing how a mindful design of the networks leads to a family of interpretable deep learning models called Logic Explained Networks (LENs). LENs only require their inputs to be human-understandable predicates, and they provide explanations in terms of simple First-Order Logic (FOL) formulas involving such predicates. LENs are general enough to cover a large number of scenarios. Amongst them, we consider the case in which LENs are directly used as special classifiers with the capability of being explainable, or when they act as additional networks with the role of creating the conditions for making a black-box classifier explainable by FOL formulas. Despite supervised learning problems are mostly emphasized, we also show that LENs can learn and provide explanations in unsupervised learning settings. Experimental results on several datasets and tasks show that LENs may yield better classifications than established white-box models, such as decision trees and Bayesian rule lists, while providing more compact and meaningful explanations.},
  keywords = {Explainable AI,Logic Explained Networks,Neural networks},
  file = {/Users/luke/Zotero/storage/XTDXE7T9/Ciravegna et al. - 2023 - Logic Explained Networks.pdf}
}

@article{contiscianiInferenceHyperedgesOverlapping2022,
  title = {Inference of hyperedges and overlapping communities in hypergraphs},
  author = {Contisciani, Martina and Battiston, Federico and De Bacco, Caterina},
  date = {2022-11-24},
  journaltitle = {Nature Communications},
  shortjournal = {Nat Commun},
  volume = {13},
  number = {1},
  pages = {1--10},
  publisher = {Nature Publishing Group},
  issn = {2041-1723},
  url = {https://www.nature.com/articles/s41467-022-34714-7},
  urldate = {2024-03-07},
  abstract = {Hypergraphs, encoding structured interactions among any number of system units, have recently proven a successful tool to describe many real-world biological and social networks. Here we propose a framework based on statistical inference to characterize the structural organization of hypergraphs. The method allows to infer missing hyperedges of any size in a principled way, and to jointly detect overlapping communities in presence of higher-order interactions. Furthermore, our model has an efficient numerical implementation, and it runs faster than dyadic algorithms on pairwise records projected from higher-order data. We apply our method to a variety of real-world systems, showing strong performance in hyperedge prediction tasks, detecting communities well aligned with the information carried by interactions, and robustness against addition of noisy hyperedges. Our approach illustrates the fundamental advantages of a hypergraph probabilistic model when modeling relational systems with higher-order interactions. Networks with higher-order interactions are known to provide better representation of real networked systems. Here the authors introduce a framework based on statistical inference to detect overlapping communities and predict hyperedges of any size in hypergraphs.},
  langid = {english},
  keywords = {Complex networks,Hypergraph,Hypergraph dataset,Statistical physics},
  file = {/Users/luke/Zotero/storage/7ZN6RMKN/Contisciani et al. - 2022 - Inference of hyperedges and overlapping communitie.pdf}
}

@inproceedings{corsoPrincipalNeighbourhoodAggregation2020,
  title = {Principal {{Neighbourhood Aggregation}} for {{Graph Nets}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Corso, Gabriele and Cavalleri, Luca and Beaini, Dominique and Liò, Pietro and Veličković, Petar},
  date = {2020},
  volume = {33},
  pages = {13260--13271},
  publisher = {Curran Associates, Inc.},
  url = {https://proceedings.neurips.cc/paper/2020/hash/99cad265a1768cc2dd013f0e740300ae-Abstract.html},
  urldate = {2024-09-11},
  abstract = {Graph Neural Networks (GNNs) have been shown to be effective models for different predictive tasks on graph-structured data. Recent work on their expressive power has focused on isomorphism tasks and countable feature spaces. We extend this theoretical framework to include continuous features---which occur regularly in real-world input domains and within the hidden layers of GNNs---and we demonstrate the requirement for multiple aggregation functions in this context. Accordingly, we propose Principal Neighbourhood Aggregation (PNA), a novel architecture combining multiple aggregators with degree-scalers (which generalize the sum aggregator). Finally, we compare the capacity of different models to capture and exploit the graph structure via a novel benchmark containing multiple tasks taken from classical graph theory, alongside existing benchmarks from real-world domains, all of which demonstrate the strength of our model. With this work we hope to steer some of the GNN research towards new aggregation methods which we believe are essential in the search for powerful and robust models.},
  file = {/Users/luke/Zotero/storage/2G5964Y4/Corso et al. - 2020 - Principal Neighbourhood Aggregation for Graph Nets.pdf}
}

@article{crossleyHubsHumanConnectome2014,
  title = {The hubs of the human connectome are generally implicated in the anatomy of brain disorders},
  author = {Crossley, Nicolas A. and Mechelli, Andrea and Scott, Jessica and Carletti, Francesco and Fox, Peter T. and McGuire, Philip and Bullmore, Edward T.},
  date = {2014-08},
  journaltitle = {Brain: A Journal of Neurology},
  shortjournal = {Brain},
  volume = {137},
  eprint = {25057133},
  eprinttype = {pmid},
  pages = {2382--2395},
  issn = {1460-2156},
  abstract = {Brain networks or 'connectomes' include a minority of highly connected hub nodes that are functionally valuable, because their topological centrality supports integrative processing and adaptive behaviours. Recent studies also suggest that hubs have higher metabolic demands and longer-distance connections than other brain regions, and therefore could be considered biologically costly. Assuming that hubs thus normally combine both high topological value and high biological cost, we predicted that pathological brain lesions would be concentrated in hub regions. To test this general hypothesis, we first identified the hubs of brain anatomical networks estimated from diffusion tensor imaging data on healthy volunteers (n = 56), and showed that computational attacks targeted on hubs disproportionally degraded the efficiency of brain networks compared to random attacks. We then prepared grey matter lesion maps, based on meta-analyses of published magnetic resonance imaging data on more than 20 000 subjects and 26 different brain disorders. Magnetic resonance imaging lesions that were common across all brain disorders were more likely to be located in hubs of the normal brain connectome (P {$<$} 10(-4), permutation test). Specifically, nine brain disorders had lesions that were significantly more likely to be located in hubs (P {$<$} 0.05, permutation test), including schizophrenia and Alzheimer's disease. Both these disorders had significantly hub-concentrated lesion distributions, although (almost completely) distinct subsets of cortical hubs were lesioned in each disorder: temporal lobe hubs specifically were associated with higher lesion probability in Alzheimer's disease, whereas in schizophrenia lesions were concentrated in both frontal and temporal cortical hubs. These results linking pathological lesions to the topological centrality of nodes in the normal diffusion tensor imaging connectome were generally replicated when hubs were defined instead by the meta-analysis of more than 1500 task-related functional neuroimaging studies of healthy volunteers to create a normative functional co-activation network. We conclude that the high cost/high value hubs of human brain networks are more likely to be anatomically abnormal than non-hubs in many (if not all) brain disorders.},
  issue = {Pt 8},
  langid = {english},
  pmcid = {PMC4107735},
  keywords = {Adult,Brain,Computer Simulation,Connectome,Diffusion Tensor Imaging,Female,graph analysis,Humans,Male,Nerve Net,rich club,topology,tractography,VBM},
  file = {/Users/luke/Zotero/storage/HIT4K8D8/Crossley et al. - 2014 - The hubs of the human connectome are generally imp.pdf}
}

@misc{cuiMultimodalFusionEHR2024,
  title = {Multimodal {{Fusion}} of {{EHR}} in {{Structures}} and {{Semantics}}: {{Integrating Clinical Records}} and {{Notes}} with {{Hypergraph}} and {{LLM}}},
  shorttitle = {Multimodal {{Fusion}} of {{EHR}} in {{Structures}} and {{Semantics}}},
  author = {Cui, Hejie and Fang, Xinyu and Xu, Ran and Kan, Xuan and Ho, Joyce C. and Yang, Carl},
  date = {2024-02-19},
  eprint = {2403.08818},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2403.08818},
  urldate = {2024-03-24},
  abstract = {Electronic Health Records (EHRs) have become increasingly popular to support clinical decision-making and healthcare in recent decades. EHRs usually contain heterogeneous information, such as structural data in tabular form and unstructured data in textual notes. Different types of information in EHRs can complement each other and provide a more complete picture of the health status of a patient. While there has been a lot of research on representation learning of structured EHR data, the fusion of different types of EHR data (multimodal fusion) is not well studied. This is mostly because of the complex medical coding systems used and the noise and redundancy present in the written notes. In this work, we propose a new framework called MINGLE, which integrates both structures and semantics in EHR effectively. Our framework uses a two-level infusion strategy to combine medical concept semantics and clinical note semantics into hypergraph neural networks, which learn the complex interactions between different types of data to generate visit representations for downstream prediction. Experiment results on two EHR datasets, the public MIMIC-III and private CRADLE, show that MINGLE can effectively improve predictive performance by 11.83\% relatively, enhancing semantic integration as well as multimodal fusion for structural and textual EHR data.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/luke/Zotero/storage/7E9ANSPZ/Cui et al. - 2024 - Multimodal Fusion of EHR in Structures and Semanti.pdf;/Users/luke/Zotero/storage/FRUU73NH/2403.html},
  note = {arXiv:2403.08818}
}

@misc{currySheavesCosheavesApplications2014,
  title = {Sheaves, {{Cosheaves}} and {{Applications}}},
  author = {Curry, Justin},
  date = {2014-12-17},
  eprint = {1303.3255},
  eprinttype = {arXiv},
  eprintclass = {math.AT},
  url = {http://arxiv.org/abs/1303.3255},
  urldate = {2023-10-26},
  abstract = {This thesis develops the theory of sheaves and cosheaves with an eye towards applications in science and engineering. To provide a theory that is computable, we focus on a combinatorial version of sheaves and cosheaves called cellular sheaves and cosheaves, which are finite families of vector spaces and maps parametrized by a cell complex. We develop cellular (co)sheaves as a new tool for topological data analysis, network coding and sensor networks. A foundation for multi-dimensional level-set persistent homology is laid via constructible cosheaves, which are equivalent to representations of MacPherson's entrance path category. By proving a van Kampen theorem, we give a direct proof of this equivalence. A cosheaf version of the i'th derived pushforward of the constant sheaf along a definable map is constructed directly as a representation of this category. We go on to clarify the relationship of cellular sheaves to cosheaves by providing a formula that defines a derived equivalence, which in turn recovers Verdier duality. Compactly-supported sheaf cohomology is expressed as the coend with the image of the constant sheaf through this equivalence. The equivalence is further used to establish relations between sheaf cohomology and a herein newly introduced theory of cellular sheaf homology. Inspired to provide fast algorithms for persistence, we prove that the derived category of cellular sheaves over a 1D cell complex is equivalent to a category of graded sheaves. Finally, we introduce the interleaving distance as an extended pseudo-metric on the category of sheaves. We prove that global sections partition the space of sheaves into connected components. We conclude with an investigation into the geometry of the space of constructible sheaves over the real line, which we relate to the bottleneck distance in persistence.},
  pubstate = {prepublished},
  file = {/Users/luke/Zotero/storage/M6DGLB75/Curry - 2014 - Sheaves, Cosheaves and Applications.pdf;/Users/luke/Zotero/storage/SPNILHVY/1303.html},
  note = {arXiv:1303.3255}
}

@misc{cvitkovicOpenVocabularyLearning2019,
  title = {Open {{Vocabulary Learning}} on {{Source Code}} with a {{Graph-Structured Cache}}},
  author = {Cvitkovic, Milan and Singh, Badal and Anandkumar, Anima},
  date = {2019-05-19},
  eprint = {1810.08305},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1810.08305},
  urldate = {2023-12-19},
  abstract = {Machine learning models that take computer program source code as input typically use Natural Language Processing (NLP) techniques. However, a major challenge is that code is written using an open, rapidly changing vocabulary due to, e.g., the coinage of new variable and method names. Reasoning over such a vocabulary is not something for which most NLP methods are designed. We introduce a Graph-Structured Cache to address this problem; this cache contains a node for each new word the model encounters with edges connecting each word to its occurrences in the code. We find that combining this graph-structured cache strategy with recent Graph-Neural-Network-based models for supervised learning on code improves the models' performance on a code completion task and a variable naming task --- with over \$100\textbackslash\%\$ relative improvement on the latter --- at the cost of a moderate increase in computation time.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/luke/Zotero/storage/2EZV7VWJ/Cvitkovic et al. - 2019 - Open Vocabulary Learning on Source Code with a Gra.pdf;/Users/luke/Zotero/storage/KPPBWKI6/1810.html},
  note = {arXiv:1810.08305}
}

@inproceedings{dabkowskiRealTimeImage2017,
  title = {Real {{Time Image Saliency}} for {{Black Box Classifiers}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Dabkowski, Piotr and Gal, Yarin},
  date = {2017},
  volume = {30},
  publisher = {Curran Associates, Inc.},
  url = {https://proceedings.neurips.cc/paper_files/paper/2017/hash/0060ef47b12160b9198302ebdb144dcf-Abstract.html},
  urldate = {2024-03-08},
  abstract = {In this work we develop a fast saliency detection method that can be applied to any differentiable image classifier. We train a masking model to manipulate the scores of the classifier by masking salient parts of the input image. Our model generalises well to unseen images and requires a single forward pass to perform saliency detection, therefore suitable for use in real-time systems. We test our approach on CIFAR-10 and ImageNet datasets and show that the produced saliency maps are easily interpretable, sharp, and free of artifacts. We suggest a new metric for saliency and test our method on the ImageNet object localisation task. We achieve results outperforming other weakly supervised methods.},
  file = {/Users/luke/Zotero/storage/LLL4JK9S/Dabkowski and Gal - 2017 - Real Time Image Saliency for Black Box Classifiers.pdf}
}

@book{daiHypergraphComputation2023,
  title = {Hypergraph {{Computation}}},
  author = {Dai, Qionghai and Gao, Yue},
  date = {2023},
  series = {Artificial {{Intelligence}}: {{Foundations}}, {{Theory}}, and {{Algorithms}}},
  publisher = {Springer Nature},
  location = {Singapore},
  url = {https://link.springer.com/10.1007/978-981-99-0185-2},
  urldate = {2023-10-11},
  isbn = {978-981-9901-84-5},
  langid = {english},
  keywords = {Complex Correlation Modelling,High-Order Correlation Modelling,Hypergraph,Hypergraph Computation,Hypergraph Learning,Hypergraph Modelling,Hypergraph Neural Network,Open Access},
  file = {/Users/luke/Zotero/storage/TDW7869F/Dai and Gao - 2023 - Hypergraph Computation.pdf}
}

@misc{daiSelfExplainableGraphNeural2021,
  title = {Towards {{Self-Explainable Graph Neural Network}}},
  author = {Dai, Enyan and Wang, Suhang},
  date = {2021-08-26},
  eprint = {2108.12055},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2108.12055},
  urldate = {2024-02-25},
  abstract = {Graph Neural Networks (GNNs), which generalize the deep neural networks to graph-structured data, have achieved great success in modeling graphs. However, as an extension of deep learning for graphs, GNNs lack explainability, which largely limits their adoption in scenarios that demand the transparency of models. Though many efforts are taken to improve the explainability of deep learning, they mainly focus on i.i.d data, which cannot be directly applied to explain the predictions of GNNs because GNNs utilize both node features and graph topology to make predictions. There are only very few work on the explainability of GNNs and they focus on post-hoc explanations. Since post-hoc explanations are not directly obtained from the GNNs, they can be biased and misrepresent the true explanations. Therefore, in this paper, we study a novel problem of self-explainable GNNs which can simultaneously give predictions and explanations. We propose a new framework which can find \$K\$-nearest labeled nodes for each unlabeled node to give explainable node classification, where nearest labeled nodes are found by interpretable similarity module in terms of both node similarity and local structure similarity. Extensive experiments on real-world and synthetic datasets demonstrate the effectiveness of the proposed framework for explainable node classification.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/luke/Zotero/storage/G5PFB4IX/Dai and Wang - 2021 - Towards Self-Explainable Graph Neural Network.pdf;/Users/luke/Zotero/storage/YP9YKWHE/2108.html},
  note = {arXiv:2108.12055}
}

@article{daviesAdvancingMathematicsGuiding2021,
  title = {Advancing mathematics by guiding human intuition with {{AI}}},
  author = {Davies, Alex and Veličković, Petar and Buesing, Lars and Blackwell, Sam and Zheng, Daniel and Tomašev, Nenad and Tanburn, Richard and Battaglia, Peter and Blundell, Charles and Juhász, András and Lackenby, Marc and Williamson, Geordie and Hassabis, Demis and Kohli, Pushmeet},
  date = {2021-12},
  journaltitle = {Nature},
  volume = {600},
  number = {7887},
  pages = {70--74},
  publisher = {Nature Publishing Group},
  issn = {1476-4687},
  url = {https://www.nature.com/articles/s41586-021-04086-x},
  urldate = {2024-03-13},
  abstract = {The practice of mathematics involves discovering patterns and using these to formulate and prove conjectures, resulting in theorems. Since the 1960s, mathematicians have used computers to assist in the discovery of patterns and formulation of conjectures1, most famously in the Birch and Swinnerton-Dyer conjecture2, a Millennium Prize Problem3. Here we provide examples of new fundamental results in pure mathematics that have been discovered with the assistance of machine learning—demonstrating a method by which machine learning can aid mathematicians in discovering new conjectures and theorems. We propose a process of using machine learning to discover potential patterns and relations between mathematical objects, understanding them with attribution techniques and using these observations to guide intuition and propose conjectures. We outline this machine-learning-guided framework and demonstrate its successful application to current research questions in distinct areas of pure mathematics, in each case showing how it led to meaningful mathematical contributions on important open problems: a new connection between the algebraic and geometric structure of knots, and a candidate algorithm predicted by the combinatorial invariance conjecture for symmetric groups4. Our work may serve as a model for collaboration between the fields of mathematics and artificial intelligence (AI) that can achieve surprising results by leveraging the respective strengths of mathematicians and machine learning.},
  langid = {english},
  keywords = {Computer science,Pure mathematics,Statistics},
  file = {/Users/luke/Zotero/storage/VCT6BJFX/Davies et al. - 2021 - Advancing mathematics by guiding human intuition w.pdf}
}

@article{dayCharacterizationModularityCongruence1969,
  title = {A {{Characterization}} of {{Modularity}} for {{Congruence Lattices}} of {{Algebras}}*},
  author = {Day, Alan},
  date = {1969-04},
  journaltitle = {Canadian Mathematical Bulletin},
  volume = {12},
  number = {2},
  pages = {167--173},
  issn = {0008-4395, 1496-4287},
  url = {https://www.cambridge.org/core/journals/canadian-mathematical-bulletin/article/characterization-of-modularity-for-congruence-lattices-of-algebras/C6BF45B464E99B4A7B2855D44A9DEEE4},
  urldate = {2024-03-08},
  abstract = {Let us call an equational class (variety) K of algebras permutable if and only if every pair of congruences on each K-algebra is permutable. Similarly, we will call K modular (distributive) if the congruence lattice of each K-algebra is modular (distributive).},
  langid = {english},
  file = {/Users/luke/Zotero/storage/TC6ALIP3/Day - 1969 - A Characterization of Modularity for Congruence La.pdf}
}

@inproceedings{dayGeometricalApplicationsModular1983,
  title = {Geometrical applications in modular lattices},
  booktitle = {Universal {{Algebra}} and {{Lattice Theory}}},
  author = {Day, Alan},
  editor = {Freese, Ralph S. and Garcia, Octavio C.},
  date = {1983},
  series = {Lecture {{Notes}} in {{Mathematics}}},
  pages = {111--141},
  publisher = {Springer},
  location = {Berlin, Heidelberg},
  isbn = {978-3-540-40954-0},
  langid = {english},
  keywords = {Division Ring,Left Ideal,Projective Geometry,Projective Plane,Word Problem},
  file = {/Users/luke/Zotero/storage/3A8IGFTY/Day - 1983 - Geometrical applications in modular lattices.pdf}
}

@article{dedekindUeberDreiModuln1900,
  title = {Ueber die von drei Moduln erzeugte Dualgruppe},
  author = {Dedekind, R.},
  date = {1900-09-01},
  journaltitle = {Mathematische Annalen},
  shortjournal = {Math. Ann.},
  volume = {53},
  number = {3},
  pages = {371--403},
  issn = {1432-1807},
  url = {https://doi.org/10.1007/BF01448979},
  urldate = {2024-03-07},
  langid = {ngerman},
  file = {/Users/luke/Zotero/storage/HU6YDC5E/Dedekind - 1900 - Ueber die von drei Moduln erzeugte Dualgruppe.pdf}
}

@inproceedings{defferrardConvolutionalNeuralNetworks2016a,
  title = {Convolutional {{Neural Networks}} on {{Graphs}} with {{Fast Localized Spectral Filtering}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Defferrard, Michaël and Bresson, Xavier and Vandergheynst, Pierre},
  date = {2016},
  volume = {29},
  publisher = {Curran Associates, Inc.},
  url = {https://proceedings.neurips.cc/paper_files/paper/2016/hash/04df4d434d481c5bb723be1b6df1ee65-Abstract.html},
  urldate = {2024-07-08},
  abstract = {In this work, we are interested in generalizing convolutional neural networks (CNNs) from low-dimensional regular grids, where image, video and speech are represented, to high-dimensional irregular domains, such as social networks, brain connectomes or words’ embedding, represented by graphs. We present a formulation of CNNs in the context of spectral graph theory, which provides the necessary mathematical background and efficient numerical schemes to design fast localized convolutional filters on graphs. Importantly, the proposed technique offers the same linear computational complexity and constant learning complexity as classical CNNs, while being universal to any graph structure. Experiments on MNIST and 20NEWS demonstrate the ability of this novel deep learning system to learn local, stationary, and compositional features on graphs.},
  file = {/Users/luke/Zotero/storage/CJFWWVNR/Defferrard et al. - 2016 - Convolutional Neural Networks on Graphs with Fast .pdf}
}

@inproceedings{dehaanNaturalGraphNetworks2020,
  title = {Natural {{Graph Networks}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {family=Haan, given=Pim, prefix=de, useprefix=true and Cohen, Taco S and Welling, Max},
  date = {2020},
  volume = {33},
  pages = {3636--3646},
  publisher = {Curran Associates, Inc.},
  url = {https://proceedings.neurips.cc/paper_files/paper/2020/hash/2517756c5a9be6ac007fe9bb7fb92611-Abstract.html},
  urldate = {2023-12-13},
  abstract = {A key requirement for graph neural networks is that they must process a graph in a way that does not depend on how the graph is described. Traditionally this has been taken to mean that a graph network must be equivariant to node permutations. Here we show that instead of equivariance, the more general concept of naturality is sufficient for a graph network to be well-defined, opening up a larger class of graph networks. We define global and local natural graph networks, the latter of which are as scalable as conventional message passing graph neural networks while being more flexible. We give one practical instantiation of a natural network on graphs which uses an equivariant message network parameterization, yielding good performance on several benchmarks.}
}

@misc{derrow-pinionETAPredictionGraph2021,
  title = {{{ETA Prediction}} with {{Graph Neural Networks}} in {{Google Maps}}},
  booktitle = {Proceedings of the 30th {{ACM International Conference}} on {{Information}} \& {{Knowledge Management}}},
  author = {Derrow-Pinion, Austin and She, Jennifer and Wong, David and Lange, Oliver and Hester, Todd and Perez, Luis and Nunkesser, Marc and Lee, Seongjae and Guo, Xueying and Wiltshire, Brett and Battaglia, Peter W. and Gupta, Vishal and Li, Ang and Xu, Zhongwen and Sanchez-Gonzalez, Alvaro and Li, Yujia and Veličković, Petar},
  date = {2021-10-26},
  eprint = {2108.11482},
  eprinttype = {arXiv},
  eprintclass = {cs},
  pages = {3767--3776},
  url = {http://arxiv.org/abs/2108.11482},
  urldate = {2023-11-26},
  abstract = {Travel-time prediction constitutes a task of high importance in transportation networks, with web mapping services like Google Maps regularly serving vast quantities of travel time queries from users and enterprises alike. Further, such a task requires accounting for complex spatiotemporal interactions (modelling both the topological properties of the road network and anticipating events -- such as rush hours -- that may occur in the future). Hence, it is an ideal target for graph representation learning at scale. Here we present a graph neural network estimator for estimated time of arrival (ETA) which we have deployed in production at Google Maps. While our main architecture consists of standard GNN building blocks, we further detail the usage of training schedule methods such as MetaGradients in order to make our model robust and production-ready. We also provide prescriptive studies: ablating on various architectural decisions and training regimes, and qualitative analyses on real-world situations where our model provides a competitive edge. Our GNN proved powerful when deployed, significantly reducing negative ETA outcomes in several regions compared to the previous production baseline (40+\% in cities like Sydney).},
  eventtitle = {{{CIKM}} 2021},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Social and Information Networks},
  file = {/Users/luke/Zotero/storage/S5ZFR8D9/Derrow-Pinion et al. - 2021 - ETA Prediction with Graph Neural Networks in Googl.pdf;/Users/luke/Zotero/storage/BGSYFE4Q/2108.html},
  note = {arXiv:2108.11482}
}

@article{diHypergraphLearningIdentification2021,
  title = {Hypergraph learning for identification of {{COVID-19}} with {{CT}} imaging},
  author = {Di, Donglin and Shi, Feng and Yan, Fuhua and Xia, Liming and Mo, Zhanhao and Ding, Zhongxiang and Shan, Fei and Song, Bin and Li, Shengrui and Wei, Ying and Shao, Ying and Han, Miaofei and Gao, Yaozong and Sui, He and Gao, Yue and Shen, Dinggang},
  date = {2021-02},
  journaltitle = {Medical Image Analysis},
  shortjournal = {Med Image Anal},
  volume = {68},
  eprint = {33285483},
  eprinttype = {pmid},
  pages = {101910},
  issn = {1361-8423},
  abstract = {The coronavirus disease, named COVID-19, has become the largest global public health crisis since it started in early 2020. CT imaging has been used as a complementary tool to assist early screening, especially for the rapid identification of COVID-19 cases from community acquired pneumonia (CAP) cases. The main challenge in early screening is how to model the confusing cases in the COVID-19 and CAP groups, with very similar clinical manifestations and imaging features. To tackle this challenge, we propose an Uncertainty Vertex-weighted Hypergraph Learning (UVHL) method to identify COVID-19 from CAP using CT images. In particular, multiple types of features (including regional features and radiomics features) are first extracted from CT image for each case. Then, the relationship among different cases is formulated by a hypergraph structure, with each case represented as a vertex in the hypergraph. The uncertainty of each vertex is further computed with an uncertainty score measurement and used as a weight in the hypergraph. Finally, a learning process of the vertex-weighted hypergraph is used to predict whether a new testing case belongs to COVID-19 or not. Experiments on a large multi-center pneumonia dataset, consisting of 2148 COVID-19 cases and 1182 CAP cases from five hospitals, are conducted to evaluate the prediction accuracy of the proposed method. Results demonstrate the effectiveness and robustness of our proposed method on the identification of COVID-19 in comparison to state-of-the-art methods.},
  langid = {english},
  pmcid = {PMC7690277},
  keywords = {China,Community-Acquired Infections,COVID-19,COVID-19 pneumonia,Datasets as Topic,Diagnosis Computer-Assisted,Diagnosis Differential,Humans,Hypergraph learning,Machine Learning,Pneumonia Viral,Radiographic Image Interpretation Computer-Assisted,SARS-CoV-2,Tomography X-Ray Computed,Uncertainty calculation,Vertex-weighted},
  file = {/Users/luke/Zotero/storage/MYCBZTBS/Di et al. - 2021 - Hypergraph learning for identification of COVID-19.pdf}
}

@misc{dongHNHNHypergraphNetworks2020,
  title = {{{HNHN}}: {{Hypergraph Networks}} with {{Hyperedge Neurons}}},
  shorttitle = {{{HNHN}}},
  author = {Dong, Yihe and Sawin, Will and Bengio, Yoshua},
  date = {2020-06-22},
  eprint = {2006.12278},
  eprinttype = {arXiv},
  eprintclass = {cs.LG},
  url = {http://arxiv.org/abs/2006.12278},
  urldate = {2024-02-26},
  abstract = {Hypergraphs provide a natural representation for many real world datasets. We propose a novel framework, HNHN, for hypergraph representation learning. HNHN is a hypergraph convolution network with nonlinear activation functions applied to both hypernodes and hyperedges, combined with a normalization scheme that can flexibly adjust the importance of high-cardinality hyperedges and high-degree vertices depending on the dataset. We demonstrate improved performance of HNHN in both classification accuracy and speed on real world datasets when compared to state of the art methods.},
  pubstate = {prepublished},
  version = {1},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/luke/Zotero/storage/W5RCDEIU/Dong et al. - 2020 - HNHN Hypergraph Networks with Hyperedge Neurons.pdf;/Users/luke/Zotero/storage/6A5ZJ8VS/2006.html},
  note = {arXiv:2006.12278}
}

@inproceedings{dongMetapath2vecScalableRepresentation2017,
  title = {metapath2vec: {{Scalable Representation Learning}} for {{Heterogeneous Networks}}},
  shorttitle = {metapath2vec},
  booktitle = {Proceedings of the 23rd {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} and {{Data Mining}}},
  author = {Dong, Yuxiao and Chawla, Nitesh V. and Swami, Ananthram},
  date = {2017-08-04},
  series = {{{KDD}} '17},
  pages = {135--144},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  url = {https://dl.acm.org/doi/10.1145/3097983.3098036},
  urldate = {2024-08-26},
  abstract = {We study the problem of representation learning in heterogeneous networks. Its unique challenges come from the existence of multiple types of nodes and links, which limit the feasibility of the conventional network embedding techniques. We develop two scalable representation learning models, namely metapath2vec and metapath2vec++. The metapath2vec model formalizes meta-path-based random walks to construct the heterogeneous neighborhood of a node and then leverages a heterogeneous skip-gram model to perform node embeddings. The metapath2vec++ model further enables the simultaneous modeling of structural and semantic correlations in heterogeneous networks. Extensive experiments show that metapath2vec and metapath2vec++ are able to not only outperform state-of-the-art embedding models in various heterogeneous network mining tasks, such as node classification, clustering, and similarity search, but also discern the structural and semantic correlations between diverse network objects.},
  isbn = {978-1-4503-4887-4},
  file = {/Users/luke/Zotero/storage/4UZ3W3E2/Dong et al. - 2017 - metapath2vec Scalable Representation Learning for.pdf}
}

@inproceedings{duEfficientSharpnessawareMinimization2021,
  title = {Efficient {{Sharpness-aware Minimization}} for {{Improved Training}} of {{Neural Networks}}},
  author = {Du, Jiawei and Yan, Hanshu and Feng, Jiashi and Zhou, Joey Tianyi and Zhen, Liangli and Goh, Rick Siow Mong and Tan, Vincent},
  date = {2021-10-06},
  url = {https://openreview.net/forum?id=n0OeTdNRG0Q},
  urldate = {2024-02-19},
  abstract = {Overparametrized Deep Neural Networks (DNNs) often achieve astounding performances, but may potentially result in severe generalization error. Recently, the relation between the sharpness of the loss landscape and the generalization error has been established by Foret et al. (2020), in which the Sharpness Aware Minimizer (SAM) was proposed to mitigate the degradation of the generalization. Unfortunately, SAM’s computational cost is roughly double that of base optimizers, such as Stochastic Gradient Descent (SGD). This paper thus proposes Efficient Sharpness Aware Minimizer (ESAM), which boosts SAM’s efficiency at no cost to its generalization performance. ESAM includes two novel and efficient training strategies—StochasticWeight Perturbation and Sharpness-Sensitive Data Selection. In the former, the sharpness measure is approximated by perturbing a stochastically chosen set of weights in each iteration; in the latter, the SAM loss is optimized using only a judiciously selected subset of data that is sensitive to the sharpness. We provide theoretical explanations as to why these strategies perform well. We also show, via extensive experiments on the CIFAR and ImageNet datasets, that ESAM enhances the efficiency over SAM from requiring 100\% extra computations to 40\% vis-`a-vis base optimizers, while test accuracies are preserved or even improved.},
  eventtitle = {International {{Conference}} on {{Learning Representations}}},
  langid = {english},
  file = {/Users/luke/Zotero/storage/TY8D9QS4/Du et al. - 2021 - Efficient Sharpness-aware Minimization for Improve.pdf}
}

@inproceedings{duGraphNeuralTangent2019,
  title = {Graph {{Neural Tangent Kernel}}: {{Fusing Graph Neural Networks}} with {{Graph Kernels}}},
  shorttitle = {Graph {{Neural Tangent Kernel}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Du, Simon S and Hou, Kangcheng and Salakhutdinov, Russ R and Poczos, Barnabas and Wang, Ruosong and Xu, Keyulu},
  date = {2019},
  volume = {32},
  publisher = {Curran Associates, Inc.},
  url = {https://proceedings.neurips.cc/paper_files/paper/2019/hash/663fd3c5144fd10bd5ca6611a9a5b92d-Abstract.html},
  urldate = {2024-02-18},
  abstract = {While graph kernels (GKs) are easy to train and enjoy provable theoretical guarantees, their practical performances are limited by their expressive power, as the kernel function often depends on hand-crafted combinatorial features of graphs. Compared to graph kernels, graph neural networks (GNNs) usually achieve better practical performance, as GNNs use multi-layer architectures and non-linear activation functions to extract high-order information of graphs as features. However, due to the large number of hyper-parameters and the non-convex nature of the training procedure, GNNs are harder to train. Theoretical guarantees of GNNs are also not well-understood. Furthermore, the expressive power of GNNs scales with the number of parameters, and thus it is hard to exploit the full power of GNNs when computing resources are limited. The current paper presents a new class of graph kernels, Graph Neural Tangent Kernels (GNTKs), which correspond to \textbackslash emph\{infinitely wide\} multi-layer GNNs trained by gradient descent. GNTKs enjoy the full expressive power of GNNs and inherit advantages of GKs. Theoretically, we show GNTKs provably learn a class of smooth functions on graphs. Empirically, we test GNTKs on graph classification datasets and show they achieve strong performance.},
  file = {/Users/luke/Zotero/storage/4D23NWVT/Du et al. - 2019 - Graph Neural Tangent Kernel Fusing Graph Neural N.pdf}
}

@misc{dutaSheafHypergraphNetworks2023,
  title = {Sheaf {{Hypergraph Networks}}},
  author = {Duta, Iulia and Cassarà, Giulia and Silvestri, Fabrizio and Liò, Pietro},
  date = {2023-09-29},
  eprint = {2309.17116},
  eprinttype = {arXiv},
  eprintclass = {cs.LG},
  url = {http://arxiv.org/abs/2309.17116},
  urldate = {2023-10-11},
  abstract = {Higher-order relations are widespread in nature, with numerous phenomena involving complex interactions that extend beyond simple pairwise connections. As a result, advancements in higher-order processing can accelerate the growth of various fields requiring structured data. Current approaches typically represent these interactions using hypergraphs. We enhance this representation by introducing cellular sheaves for hypergraphs, a mathematical construction that adds extra structure to the conventional hypergraph while maintaining their local, higherorder connectivity. Drawing inspiration from existing Laplacians in the literature, we develop two unique formulations of sheaf hypergraph Laplacians: linear and non-linear. Our theoretical analysis demonstrates that incorporating sheaves into the hypergraph Laplacian provides a more expressive inductive bias than standard hypergraph diffusion, creating a powerful instrument for effectively modelling complex data structures. We employ these sheaf hypergraph Laplacians to design two categories of models: Sheaf Hypergraph Neural Networks and Sheaf Hypergraph Convolutional Networks. These models generalize classical Hypergraph Networks often found in the literature. Through extensive experimentation, we show that this generalization significantly improves performance, achieving top results on multiple benchmark datasets for hypergraph node classification.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/luke/Zotero/storage/96759VV6/Duta et al. - 2023 - Sheaf Hypergraph Networks.pdf;/Users/luke/Zotero/storage/34QUJPTS/2309.html},
  note = {arXiv:2309.17116}
}

@inproceedings{dutaSheafHypergraphNetworks2023a,
  title = {Sheaf {{Hypergraph Networks}}},
  author = {Duta, Iulia and Cassarà, Giulia and Silvestri, Fabrizio and Lio, Pietro},
  date = {2023-11-02},
  location = {NeurIPS 2023},
  url = {https://openreview.net/forum?id=NvcVXzJvhX&referrer=%5Bthe%20profile%20of%20Iulia%20Duta%5D(%2Fprofile%3Fid%3D~Iulia_Duta1)},
  urldate = {2024-04-20},
  abstract = {Higher-order relations are widespread in nature, with numerous phenomena involving complex interactions that extend beyond simple pairwise connections. As a result, advancements in higher-order processing can accelerate the growth of various fields requiring structured data. Current approaches typically represent these interactions using hypergraphs. We enhance this representation by introducing cellular sheaves for hypergraphs, a mathematical construction that adds extra structure to the conventional hypergraph while maintaining their local, higher-order connectivity. Drawing inspiration from existing Laplacians in the literature, we develop two unique formulations of sheaf hypergraph Laplacians: linear and non-linear. Our theoretical analysis demonstrates that incorporating sheaves into the hypergraph Laplacian provides a more expressive inductive bias than standard hypergraph diffusion, creating a powerful instrument for effectively modelling complex data structures. We employ these sheaf hypergraph Laplacians to design two categories of models: Sheaf Hypergraph Neural Networks and Sheaf Hypergraph Convolutional Networks. These models generalize classical Hypergraph Networks often found in the literature. Through extensive experimentation, we show that this generalization significantly improves performance, achieving top results on multiple benchmark datasets for hypergraph node classification.},
  eventtitle = {Thirty-seventh {{Conference}} on {{Neural Information Processing Systems}}},
  langid = {english},
  file = {/Users/luke/Zotero/storage/4CK64R7E/Duta et al. - 2023 - Sheaf Hypergraph Networks.pdf}
}

@misc{dwivediGeneralizationTransformerNetworks2021,
  title = {A {{Generalization}} of {{Transformer Networks}} to {{Graphs}}},
  author = {Dwivedi, Vijay Prakash and Bresson, Xavier},
  date = {2021-01-24},
  eprint = {2012.09699},
  eprinttype = {arXiv},
  eprintclass = {cs.LG},
  url = {http://arxiv.org/abs/2012.09699},
  urldate = {2023-12-20},
  abstract = {We propose a generalization of transformer neural network architecture for arbitrary graphs. The original transformer was designed for Natural Language Processing (NLP), which operates on fully connected graphs representing all connections between the words in a sequence. Such architecture does not leverage the graph connectivity inductive bias, and can perform poorly when the graph topology is important and has not been encoded into the node features. We introduce a graph transformer with four new properties compared to the standard model. First, the attention mechanism is a function of the neighborhood connectivity for each node in the graph. Second, the positional encoding is represented by the Laplacian eigenvectors, which naturally generalize the sinusoidal positional encodings often used in NLP. Third, the layer normalization is replaced by a batch normalization layer, which provides faster training and better generalization performance. Finally, the architecture is extended to edge feature representation, which can be critical to tasks s.a. chemistry (bond type) or link prediction (entity relationship in knowledge graphs). Numerical experiments on a graph benchmark demonstrate the performance of the proposed graph transformer architecture. This work closes the gap between the original transformer, which was designed for the limited case of line graphs, and graph neural networks, that can work with arbitrary graphs. As our architecture is simple and generic, we believe it can be used as a black box for future applications that wish to consider transformer and graphs.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/luke/Zotero/storage/U26FH84L/Dwivedi and Bresson - 2021 - A Generalization of Transformer Networks to Graphs.pdf;/Users/luke/Zotero/storage/APJJFYI3/2012.html},
  note = {arXiv:2012.09699}
}

@book{evansPartialDifferentialEquations2010,
  title = {Partial differential equations},
  author = {Evans, Lawrence C},
  date = {2010},
  series = {Graduate studies in mathematics},
  edition = {Second edition.},
  publisher = {American Mathematical Society},
  location = {Providence, R.I.},
  abstract = {"This is the second edition of the now definitive text on partial differential equations (PDE). It offers a comprehensive survey of modern techniques in the theoretical study of PDE with particular emphasis on nonlinear equations. Its wide scope and clear exposition make it a great text for a graduate course in PDE. For this edition, the author has made numerous changes, including: a new chapter on nonlinear wave equations, more than 80 new exercises, several new sections, and a significantly expanded bibliography."--Publisher's description.},
  isbn = {978-0-8218-4974-3},
  langid = {english},
  keywords = {Differential equations Partial,Textbooks; Differential equations Partial}
}

@inproceedings{fanGraphNeuralNetworks2019,
  title = {Graph {{Neural Networks}} for {{Social Recommendation}}},
  booktitle = {The {{World Wide Web Conference}}},
  author = {Fan, Wenqi and Ma, Yao and Li, Qing and He, Yuan and Zhao, Eric and Tang, Jiliang and Yin, Dawei},
  date = {2019-05-13},
  pages = {417--426},
  publisher = {ACM},
  location = {San Francisco CA USA},
  url = {https://dl.acm.org/doi/10.1145/3308558.3313488},
  urldate = {2024-08-26},
  eventtitle = {{{WWW}} '19: {{The Web Conference}}},
  isbn = {978-1-4503-6674-8},
  langid = {english},
  file = {/Users/luke/Zotero/storage/RSZ9LETE/Fan et al. - 2019 - Graph Neural Networks for Social Recommendation.pdf}
}

@inproceedings{fengHypergraphNeuralNetworks2019,
  title = {Hypergraph {{Neural Networks}}},
  booktitle = {Proceedings of the {{AAAI Conference}} on {{Artificial Intelligence}}},
  author = {Feng, Yifan and You, Haoxuan and Zhang, Zizhao and Ji, Rongrong and Gao, Yue},
  date = {2019-07-17},
  volume = {33},
  number = {01},
  pages = {3558--3565},
  url = {https://ojs.aaai.org/index.php/AAAI/article/view/4235},
  urldate = {2023-10-23},
  abstract = {In this paper, we present a hypergraph neural networks (HGNN) framework for data representation learning, which can encode high-order data correlation in a hypergraph structure. Confronting the challenges of learning representation for complex data in real practice, we propose to incorporate such data structure in a hypergraph, which is more flexible on data modeling, especially when dealing with complex data. In this method, a hyperedge convolution operation is designed to handle the data correlation during representation learning. In this way, traditional hypergraph learning procedure can be conducted using hyperedge convolution operations efficiently. HGNN is able to learn the hidden layer representation considering the high-order data structure, which is a general framework considering the complex data correlations. We have conducted experiments on citation network classification and visual object recognition tasks and compared HGNN with graph convolutional networks and other traditional methods. Experimental results demonstrate that the proposed HGNN method outperforms recent state-of-theart methods. We can also reveal from the results that the proposed HGNN is superior when dealing with multi-modal data compared with existing methods.},
  eventtitle = {{{AAAI}}'2019},
  langid = {english},
  file = {/Users/luke/Zotero/storage/PTZ9LYR9/Feng et al. - 2019 - Hypergraph Neural Networks.pdf}
}

@inproceedings{ferriniMetaPathLearningMultirelational2023,
  title = {Meta-{{Path Learning}} for {{Multi-relational Graph Neural Networks}}},
  author = {Ferrini, Francesco and Longa, Antonio and Passerini, Andrea and Jaeger, Manfred},
  date = {2023-11-25},
  url = {https://openreview.net/forum?id=gW9ZmT9hAe},
  urldate = {2023-11-27},
  abstract = {Existing multi-relational graph neural networks use one of two strategies for identifying informative relations: either they reduce this problem to low-level weight learning, or they rely on handcrafted chains of relational dependencies, called meta-paths. However, the former approach faces challenges in the presence of many relations (e.g., knowledge graphs), while the latter requires substantial domain expertise to identify relevant meta-paths. In this work we propose a novel approach to learn meta-paths and meta-path GNNs that are highly accurate based on a small number of informative meta-paths. Key element of our approach is a scoring function for measuring the potential informativeness of a relation in the incremental construction of the meta-path. Our experimental evaluation shows that the approach manages to correctly identify relevant meta-paths even with a large number of relations, and substantially outperforms existing multi-relational GNNs on synthetic and real-world experiments.},
  eventtitle = {The {{Second Learning}} on {{Graphs Conference}}},
  langid = {english},
  file = {/Users/luke/Zotero/storage/XAKWMNIG/Ferrini et al. - 2023 - Meta-Path Learning for Multi-relational Graph Neur.pdf}
}

@misc{feyFastGraphRepresentation2019,
  title = {Fast {{Graph Representation Learning}} with {{PyTorch Geometric}}},
  author = {Fey, Matthias and Lenssen, Jan Eric},
  date = {2019-04-25},
  eprint = {1903.02428},
  eprinttype = {arXiv},
  eprintclass = {cs.LG},
  url = {http://arxiv.org/abs/1903.02428},
  urldate = {2023-12-20},
  abstract = {We introduce PyTorch Geometric, a library for deep learning on irregularly structured input data such as graphs, point clouds and manifolds, built upon PyTorch. In addition to general graph data structures and processing methods, it contains a variety of recently published methods from the domains of relational learning and 3D data processing. PyTorch Geometric achieves high data throughput by leveraging sparse GPU acceleration, by providing dedicated CUDA kernels and by introducing efficient mini-batch handling for input examples of different size. In this work, we present the library in detail and perform a comprehensive comparative study of the implemented methods in homogeneous evaluation scenarios.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/luke/Zotero/storage/EMXHCN8D/Fey and Lenssen - 2019 - Fast Graph Representation Learning with PyTorch Ge.pdf;/Users/luke/Zotero/storage/9CZ53ESD/1903.html},
  note = {arXiv:1903.02428}
}

@misc{finkelshteinCooperativeGraphNeural2023,
  title = {Cooperative {{Graph Neural Networks}}},
  author = {Finkelshtein, Ben and Huang, Xingyue and Bronstein, Michael and Ceylan, İsmail İlkan},
  date = {2023-10-02},
  eprint = {2310.01267},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2310.01267},
  urldate = {2024-02-25},
  abstract = {Graph neural networks are popular architectures for graph machine learning, based on iterative computation of node representations of an input graph through a series of invariant transformations. A large class of graph neural networks follow a standard message-passing paradigm: at every layer, each node state is updated based on an aggregate of messages from its neighborhood. In this work, we propose a novel framework for training graph neural networks, where every node is viewed as a player that can choose to either 'listen', 'broadcast', 'listen and broadcast', or to 'isolate'. The standard message propagation scheme can then be viewed as a special case of this framework where every node 'listens and broadcasts' to all neighbors. Our approach offers a more flexible and dynamic message-passing paradigm, where each node can determine its own strategy based on their state, effectively exploring the graph topology while learning. We provide a theoretical analysis of the new message-passing scheme which is further supported by an extensive empirical analysis on a synthetic dataset and on real-world datasets.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/luke/Zotero/storage/I4WKQR3G/Finkelshtein et al. - 2023 - Cooperative Graph Neural Networks.pdf;/Users/luke/Zotero/storage/W4EMTKNS/2310.html},
  note = {arXiv:2310.01267}
}

@inproceedings{foretSharpnessawareMinimizationEfficiently2020,
  title = {Sharpness-aware {{Minimization}} for {{Efficiently Improving Generalization}}},
  author = {Foret, Pierre and Kleiner, Ariel and Mobahi, Hossein and Neyshabur, Behnam},
  date = {2020-10-02},
  url = {https://openreview.net/forum?id=6Tm1mposlrM},
  urldate = {2024-02-18},
  abstract = {In today's heavily overparameterized models, the value of the training loss provides few guarantees on model generalization ability. Indeed, optimizing only the training loss value, as is commonly done, can easily lead to suboptimal model quality. Motivated by the connection between geometry of the loss landscape and generalization---including a generalization bound that we prove here---we introduce a novel, effective procedure for instead simultaneously minimizing loss value and loss sharpness. In particular, our procedure, Sharpness-Aware Minimization (SAM), seeks parameters that lie in neighborhoods having uniformly low loss; this formulation results in a min-max optimization problem on which gradient descent can be performed efficiently. We present empirical results showing that SAM improves model generalization across a variety of benchmark datasets (e.g., CIFAR-\{10, 100\}, ImageNet, finetuning tasks) and models, yielding novel state-of-the-art performance for several. Additionally, we find that SAM natively provides robustness to label noise on par with that provided by state-of-the-art procedures that specifically target learning with noisy labels.},
  eventtitle = {International {{Conference}} on {{Learning Representations}}},
  langid = {english},
  file = {/Users/luke/Zotero/storage/39FN766W/Foret et al. - 2020 - Sharpness-aware Minimization for Efficiently Impro.pdf}
}

@inproceedings{foutProteinInterfacePrediction2017,
  title = {Protein {{Interface Prediction}} using {{Graph Convolutional Networks}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Fout, Alex and Byrd, Jonathon and Shariat, Basir and Ben-Hur, Asa},
  date = {2017},
  volume = {30},
  publisher = {Curran Associates, Inc.},
  url = {https://proceedings.neurips.cc/paper_files/paper/2017/hash/f507783927f2ec2737ba40afbd17efb5-Abstract.html},
  urldate = {2024-08-26},
  abstract = {We consider the prediction of interfaces between proteins, a challenging problem with important applications in drug discovery and design, and examine the performance of existing and newly proposed spatial graph convolution operators for this task. By performing convolution over a local neighborhood of a node of interest, we are able to stack multiple layers of convolution and learn effective latent representations that integrate information across the graph that represent the three dimensional structure of a protein of interest. An architecture that combines the learned features across pairs of proteins is then used to classify pairs of amino acid residues as part of an interface or not. In our experiments, several graph convolution operators yielded accuracy that is better than the state-of-the-art SVM method in this task.},
  file = {/Users/luke/Zotero/storage/VFHRHR9Q/Fout et al. - 2017 - Protein Interface Prediction using Graph Convoluti.pdf}
}

@article{frasconiGeneralFrameworkAdaptive1998,
  title = {A general framework for adaptive processing of data structures},
  author = {Frasconi, P. and Gori, M. and Sperduti, A.},
  date = {1998-09},
  journaltitle = {IEEE Transactions on Neural Networks},
  volume = {9},
  number = {5},
  pages = {768--786},
  issn = {1941-0093},
  url = {https://ieeexplore.ieee.org/document/712151},
  urldate = {2024-02-23},
  abstract = {A structured organization of information is typically required by symbolic processing. On the other hand, most connectionist models assume that data are organized according to relatively poor structures, like arrays or sequences. The framework described in this paper is an attempt to unify adaptive models like artificial neural nets and belief nets for the problem of processing structured information. In particular, relations between data variables are expressed by directed acyclic graphs, where both numerical and categorical values coexist. The general framework proposed in this paper can be regarded as an extension of both recurrent neural networks and hidden Markov models to the case of acyclic graphs. In particular we study the supervised learning problem as the problem of learning transductions from an input structured space to an output structured space, where transductions are assumed to admit a recursive hidden state-space representation. We introduce a graphical formalism for representing this class of adaptive transductions by means of recursive networks, i.e., cyclic graphs where nodes are labeled by variables and edges are labeled by generalized delay elements. This representation makes it possible to incorporate the symbolic and subsymbolic nature of data. Structures are processed by unfolding the recursive network into an acyclic graph called encoding network. In so doing, inference and learning algorithms can be easily inherited from the corresponding algorithms for artificial neural networks or probabilistic graphical model.},
  eventtitle = {{{IEEE Transactions}} on {{Neural Networks}}},
  keywords = {Artificial neural networks,Data structures,Encoding,Graphical models,Hidden Markov models,Inference algorithms,Neural networks,Problem-solving,Recurrent neural networks,Supervised learning},
  file = {/Users/luke/Zotero/storage/UK96IZNX/Frasconi et al. - 1998 - A general framework for adaptive processing of dat.pdf;/Users/luke/Zotero/storage/UIL5TYE5/712151.html}
}

@article{freeseFinitelyBasedCongruence2024,
  title = {Finitely based congruence varieties},
  author = {Freese, Ralph and Lipparini, Paolo},
  date = {2024-01-12},
  journaltitle = {Algebra universalis},
  shortjournal = {Algebra Univers.},
  volume = {85},
  number = {1},
  pages = {11},
  issn = {1420-8911},
  url = {https://doi.org/10.1007/s00012-023-00840-6},
  urldate = {2024-03-09},
  abstract = {We show that for a large class of varieties of algebras, the equational theory of the congruence lattices of the members is not finitely based.},
  langid = {english},
  keywords = {06B15,06C05,08B99,Congruence lattice,Congruence variety,Finite (equational) basis,Higher Arguesian identities,Projective lattices},
  file = {/Users/luke/Zotero/storage/K3DKWJ3N/Freese and Lipparini - 2024 - Finitely based congruence varieties.pdf}
}

@inproceedings{fuMAGNNMetapathAggregated2020,
  title = {{{MAGNN}}: {{Metapath Aggregated Graph Neural Network}} for {{Heterogeneous Graph Embedding}}},
  shorttitle = {{{MAGNN}}},
  booktitle = {Proceedings of {{The Web Conference}} 2020},
  author = {Fu, Xinyu and Zhang, Jiani and Meng, Ziqiao and King, Irwin},
  date = {2020-04-20},
  pages = {2331--2341},
  abstract = {A large number of real-world graphs or networks are inherently heterogeneous, involving a diversity of node types and relation types. Heterogeneous graph embedding is to embed rich structural and semantic information of a heterogeneous graph into low-dimensional node representations. Existing models usually define multiple metapaths in a heterogeneous graph to capture the composite relations and guide neighbor selection. However, these models either omit node content features, discard intermediate nodes along the metapath, or only consider one metapath. To address these three limitations, we propose a new model named Metapath Aggregated Graph Neural Network (MAGNN) to boost the final performance. Specifically, MAGNN employs three major components, i.e., the node content transformation to encapsulate input node attributes, the intra-metapath aggregation to incorporate intermediate semantic nodes, and the inter-metapath aggregation to combine messages from multiple metapaths. Extensive experiments on three real-world heterogeneous graph datasets for node classification, node clustering, and link prediction show that MAGNN achieves more accurate prediction results than state-of-the-art baselines.},
  eventtitle = {{{WWW}} ’20},
  keywords = {Computer Science - Machine Learning,Computer Science - Social and Information Networks},
  file = {/Users/luke/Zotero/storage/E5H6Q3Q4/Fu et al. - 2020 - MAGNN Metapath Aggregated Graph Neural Network fo.pdf;/Users/luke/Zotero/storage/I2THRUPK/2002.html}
}

@article{funkeHardMaskingExplaining2020,
  title = {Hard {{Masking}} for {{Explaining Graph Neural Networks}}},
  author = {Funke, Thorben and Khosla, Megha and Anand, Avishek},
  date = {2020-10-02},
  url = {https://openreview.net/forum?id=uDN8pRAdsoC},
  urldate = {2024-03-08},
  abstract = {Graph Neural Networks (GNNs) are a flexible and powerful family of models that build nodes' representations on irregular graph-structured data. This paper focuses on explaining or interpreting the rationale underlying a given prediction of already trained graph neural networks for the node classification task. Existing approaches for interpreting GNNs try to find subsets of important features and nodes by learning a continuous mask. Our objective is to find discrete masks that are arguably more interpretable while minimizing the expected deviation from the underlying model's prediction. We empirically show that our explanations are both more predictive and sparse. Additionally, we find that multiple diverse explanations are possible, which sufficiently explain a prediction. Finally, we analyze the explanations to find the effect of network homophily on the decision-making process of GNNs.},
  langid = {english},
  file = {/Users/luke/Zotero/storage/CPHCNW3B/Funke et al. - 2020 - Hard Masking for Explaining Graph Neural Networks.pdf}
}

@misc{galkinFoundationModelsKnowledge2023,
  title = {Towards {{Foundation Models}} for {{Knowledge Graph Reasoning}}},
  author = {Galkin, Mikhail and Yuan, Xinyu and Mostafa, Hesham and Tang, Jian and Zhu, Zhaocheng},
  date = {2023-10-06},
  eprint = {2310.04562},
  eprinttype = {arXiv},
  eprintclass = {cs.CL},
  url = {http://arxiv.org/abs/2310.04562},
  urldate = {2024-01-05},
  abstract = {Foundation models in language and vision have the ability to run inference on any textual and visual inputs thanks to the transferable representations such as a vocabulary of tokens in language. Knowledge graphs (KGs) have different entity and relation vocabularies that generally do not overlap. The key challenge of designing foundation models on KGs is to learn such transferable representations that enable inference on any graph with arbitrary entity and relation vocabularies. In this work, we make a step towards such foundation models and present ULTRA, an approach for learning universal and transferable graph representations. ULTRA builds relational representations as a function conditioned on their interactions. Such a conditioning strategy allows a pre-trained ULTRA model to inductively generalize to any unseen KG with any relation vocabulary and to be fine-tuned on any graph. Conducting link prediction experiments on 57 different KGs, we find that the zero-shot inductive inference performance of a single pre-trained ULTRA model on unseen graphs of various sizes is often on par or better than strong baselines trained on specific graphs. Fine-tuning further boosts the performance.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/luke/Zotero/storage/QMI67E4G/Galkin et al. - 2023 - Towards Foundation Models for Knowledge Graph Reas.pdf;/Users/luke/Zotero/storage/SM6I6W9K/2310.html},
  note = {arXiv:2310.04562}
}

@inproceedings{galkinMessagePassingHyperRelational2020,
  title = {Message {{Passing}} for {{Hyper-Relational Knowledge Graphs}}},
  booktitle = {Proceedings of the 2020 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} ({{EMNLP}})},
  author = {Galkin, Mikhail and Trivedi, Priyansh and Maheshwari, Gaurav and Usbeck, Ricardo and Lehmann, Jens},
  editor = {Webber, Bonnie and Cohn, Trevor and He, Yulan and Liu, Yang},
  date = {2020-11},
  pages = {7346--7359},
  publisher = {Association for Computational Linguistics},
  location = {Online},
  url = {https://aclanthology.org/2020.emnlp-main.596},
  urldate = {2023-12-04},
  abstract = {Hyper-relational knowledge graphs (KGs) (e.g., Wikidata) enable associating additional key-value pairs along with the main triple to disambiguate, or restrict the validity of a fact. In this work, we propose a message passing based graph encoder - StarE capable of modeling such hyper-relational KGs. Unlike existing approaches, StarE can encode an arbitrary number of additional information (qualifiers) along with the main triple while keeping the semantic roles of qualifiers and triples intact. We also demonstrate that existing benchmarks for evaluating link prediction (LP) performance on hyper-relational KGs suffer from fundamental flaws and thus develop a new Wikidata-based dataset - WD50K. Our experiments demonstrate that StarE based LP model outperforms existing approaches across multiple benchmarks. We also confirm that leveraging qualifiers is vital for link prediction with gains up to 25 MRR points compared to triple-based representations.},
  eventtitle = {{{EMNLP}} 2020},
  file = {/Users/luke/Zotero/storage/P9YDYFZ2/Galkin et al. - 2020 - Message Passing for Hyper-Relational Knowledge Gra.pdf}
}

@article{gaoGeometrySynchronizationProblems2021,
  title = {The {{Geometry}} of {{Synchronization Problems}} and {{Learning Group Actions}}},
  author = {Gao, Tingran and Brodzki, Jacek and Mukherjee, Sayan},
  date = {2021-01-01},
  journaltitle = {Discrete \& Computational Geometry},
  shortjournal = {Discrete Comput Geom},
  volume = {65},
  number = {1},
  pages = {150--211},
  issn = {1432-0444},
  url = {https://doi.org/10.1007/s00454-019-00100-2},
  urldate = {2024-04-01},
  abstract = {We develop a geometric framework, based on the classical theory of fibre bundles, to characterize the cohomological nature of a large class of synchronization-type problems in the context of graph inference and combinatorial optimization. We identify each synchronization problem in topological group G on connected graph \$\$\textbackslash Gamma \$\$with a flat principal G-bundle over \$\$\textbackslash Gamma \$\$, thus establishing a classification result for synchronization problems using the representation variety of the fundamental group of \$\$\textbackslash Gamma \$\$into G. We then develop a twisted Hodge theory on flat vector bundles associated with these flat principal G-bundles, and provide a geometric realization of the graph connection Laplacian as the lowest-degree Hodge Laplacian in the twisted de Rham–Hodge cochain complex. Motivated by these geometric intuitions, we propose to study the problem of learning group actions—partitioning a collection of objects based on the local synchronizability of pairwise correspondence relations—and provide a heuristic synchronization-based algorithm for solving this type of problems. We demonstrate the efficacy of this algorithm on simulated and real datasets.},
  langid = {english},
  keywords = {05C50,57R22,58A14,62-07,Fibre bundle,Graph connection Laplacian,Hodge theory,Holonomy,Synchronization problem},
  file = {/Users/luke/Zotero/storage/TQT9CYC5/Gao et al. - 2021 - The Geometry of Synchronization Problems and Learn.pdf}
}

@article{gaoMCIIdentificationJoint2015,
  title = {{{MCI Identification}} by {{Joint Learning}} on {{Multiple MRI Data}}},
  author = {Gao, Yue and Wee, Chong-Yaw and Kim, Minjeong and Giannakopoulos, Panteleimon and Montandon, Marie-Louise and Haller, Sven and Shen, Dinggang},
  date = {2015-10},
  journaltitle = {Medical image computing and computer-assisted intervention : MICCAI ... International Conference on Medical Image Computing and Computer-Assisted Intervention},
  shortjournal = {Med Image Comput Comput Assist Interv},
  volume = {9350},
  eprint = {26942232},
  eprinttype = {pmid},
  pages = {78--85},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4773025/},
  urldate = {2023-11-26},
  abstract = {The identification of subtle brain changes that are associated with mild cognitive impairment (MCI), the at-risk stage of Alzheimer’s disease, is still a challenging task. Different from existing works, which employ multimodal data (e.g., MRI, PET or CSF) to identify MCI subjects from normal elderly controls, we use four MRI sequences, including T1-weighted MRI (T1), Diffusion Tensor Imaging (DTI), Resting-State functional MRI (RS-fMRI) and Arterial Spin Labeling (ASL) perfusion imaging. Since these MRI sequences simultaneously capture various aspects of brain structure and function during clinical routine scan, it simplifies finding the relationship between subjects by incorporating the mutual information among them. To this end, we devise a hypergraph-based semi-supervised learning algorithm. In particular, we first construct a hypergraph for each of MRI sequences separately using a star expansion method with both the training and testing data. A centralized learning is then performed to model the optimal relevance between subjects by incorporating mutual information between different MRI sequences. We then combine all centralized hypergraphs by learning the optimal weight of each hypergraph based on the minimum Laplacian. We apply our proposed method on a cohort of 41 consecutive MCI subjects and 63 age-and-gender matched controls with four MRI sequences. Our method achieves at least a 7.61\% improvement in classification accuracy compared to state-of-the-art methods using multiple MRI data.},
  pmcid = {PMC4773025},
  file = {/Users/luke/Zotero/storage/JVRE99KU/Gao et al. - 2015 - MCI Identification by Joint Learning on Multiple M.pdf}
}

@article{gaoMedicalImageRetrieval2015,
  title = {Medical {{Image Retrieval Using Multi-graph Learning}} for {{MCI Diagnostic Assistance}}},
  author = {Gao, Yue and Adeli-M, Ehsan and Kim, Minjeong and Giannakopoulos, Panteleimon and Haller, Sven and Shen, Dinggang},
  date = {2015-10},
  journaltitle = {Medical image computing and computer-assisted intervention: MICCAI ... International Conference on Medical Image Computing and Computer-Assisted Intervention},
  shortjournal = {Med Image Comput Comput Assist Interv},
  volume = {9350},
  eprint = {27054200},
  eprinttype = {pmid},
  pages = {86--93},
  abstract = {Alzheimer's disease (AD) is an irreversible neurodegenerative disorder that can lead to progressive memory loss and cognition impairment. Therefore, diagnosing AD during the risk stage, a.k.a. Mild Cognitive Impairment (MCI), has attracted ever increasing interest. Besides the automated diagnosis of MCI, it is important to provide physicians with related MCI cases with visually similar imaging data for case-based reasoning or evidence-based medicine in clinical practices. To this end, we propose a multi-graph learning based medical image retrieval technique for MCI diagnostic assistance. Our method is comprised of two stages, the query category prediction and ranking. In the first stage, the query is formulated into a multi-graph structure with a set of selected subjects in the database to learn the relevance between the query subject and the existing subject categories through learning the multi-graph combination weights. This predicts the category that the query belongs to, based on which a set of subjects in the database are selected as candidate retrieval results. In the second stage, the relationship between these candidates and the query is further learned with a new multi-graph, which is used to rank the candidates. The returned subjects can be demonstrated to physicians as reference cases for MCI diagnosing. We evaluated the proposed method on a cohort of 60 consecutive MCI subjects and 350 normal controls with MRI data under three imaging parameters: T1 weighted imaging (T1), Diffusion Tensor Imaging (DTI) and Arterial Spin Labeling (ASL). The proposed method can achieve average 3.45 relevant samples in top 5 returned results, which significantly outperforms the baseline methods compared.},
  langid = {english},
  pmcid = {PMC4820016},
  file = {/Users/luke/Zotero/storage/VXHT49BX/Gao et al. - 2015 - Medical Image Retrieval Using Multi-graph Learning.pdf}
}

@inproceedings{gasteigerDiffusionImprovesGraph2019,
  title = {Diffusion {{Improves Graph Learning}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Gasteiger, Johannes and Weiß enberger, Stefan and Günnemann, Stephan},
  date = {2019},
  volume = {32},
  publisher = {Curran Associates, Inc.},
  url = {https://proceedings.neurips.cc/paper_files/paper/2019/hash/23c894276a2c5a16470e6a31f4618d73-Abstract.html},
  urldate = {2023-12-20},
  abstract = {Graph convolution is the core of most Graph Neural Networks (GNNs) and usually approximated by message passing between direct (one-hop) neighbors. In this work, we remove the restriction of using only the direct neighbors by introducing a powerful, yet spatially localized graph convolution: Graph diffusion convolution (GDC). GDC leverages generalized graph diffusion, examples of which are the heat kernel and personalized PageRank. It alleviates the problem of noisy and often arbitrarily defined edges in real graphs. We show that GDC is closely related to spectral-based models and thus combines the strengths of both spatial (message passing) and spectral methods. We demonstrate that replacing message passing with graph diffusion convolution consistently leads to significant performance improvements across a wide range of models on both supervised and unsupervised tasks and a variety of datasets. Furthermore, GDC is not limited to GNNs but can trivially be combined with any graph-based model or algorithm (e.g. spectral clustering) without requiring any changes to the latter or affecting its computational complexity. Our implementation is available online.},
  eventtitle = {{{NeurIPS}} 2019},
  file = {/Users/luke/Zotero/storage/LRTA832S/Gasteiger et al. - 2019 - Diffusion Improves Graph Learning.pdf}
}

@article{georgievHEATHyperedgeAttention2022,
  title = {{{HEAT}}: {{Hyperedge Attention Networks}}},
  shorttitle = {{{HEAT}}},
  author = {Georgiev, Dobrik Georgiev and Brockschmidt, Marc and Allamanis, Miltiadis},
  date = {2022-06-09},
  journaltitle = {Transactions on Machine Learning Research},
  issn = {2835-8856},
  url = {https://openreview.net/forum?id=gCmQK6McbR},
  urldate = {2023-12-04},
  abstract = {Learning from structured data is a core machine learning task. Commonly, such data is represented as graphs, which normally only consider (typed) binary relationships between pairs of nodes. This is a substantial limitation for many domains with highly-structured data. One important such domain is source code, where hypergraph-based representations can better capture the semantically rich and structured nature of code. In this work, we present HEAT, a neural model capable of representing typed and qualified hypergraphs, where each hyperedge explicitly qualifies how participating nodes contribute. It can be viewed as a generalization of both message passing neural networks and Transformers. We evaluate HEAT on knowledge base completion and on bug detection and repair using a novel hypergraph representation of programs. In both settings, it outperforms strong baselines, indicating its power and generality.},
  langid = {english},
  file = {/Users/luke/Zotero/storage/4FH2WNW8/Georgiev et al. - 2022 - HEAT Hyperedge Attention Networks.pdf}
}

@inproceedings{ghorbaniAutomaticConceptbasedExplanations2019,
  title = {Towards {{Automatic Concept-based Explanations}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Ghorbani, Amirata and Wexler, James and Zou, James Y and Kim, Been},
  date = {2019},
  volume = {32},
  publisher = {Curran Associates, Inc.},
  url = {https://proceedings.neurips.cc/paper_files/paper/2019/hash/77d2afcb31f6493e350fca61764efb9a-Abstract.html},
  urldate = {2024-03-08},
  abstract = {Interpretability has become an important topic of research as more machine learning (ML) models are deployed and widely used to make important decisions.      Most of the current explanation methods provide explanations through feature importance scores, which identify features that are important for each individual input. However, how to systematically summarize and interpret such per sample feature importance scores itself is challenging. In this work, we propose principles and desiderata for \textbackslash emph\{concept\} based explanation, which goes beyond per-sample features to identify higher level human-understandable concepts that apply across the entire dataset. We develop a new algorithm, ACE, to automatically extract visual concepts. Our systematic experiments demonstrate that \textbackslash alg discovers concepts that are human-meaningful, coherent and important for the neural network's predictions.},
  file = {/Users/luke/Zotero/storage/RWJER9H3/Ghorbani et al. - 2019 - Towards Automatic Concept-based Explanations.pdf}
}

@inproceedings{gianniniInterpretableGraphNetworks2023a,
  title = {Interpretable {{Graph Networks Formulate Universal Algebra Conjectures}}},
  author = {Giannini, Francesco and Fioravanti, Stefano and Keskin, Oguzhan and Lupidi, Alisia Maria and Magister, Lucie Charlotte and Lio, Pietro and Barbiero, Pietro},
  date = {2023-11-02},
  url = {https://openreview.net/forum?id=Psnph85KYc&referrer=%5Bthe%20profile%20of%20Pietro%20Barbiero%5D(%2Fprofile%3Fid%3D~Pietro_Barbiero1)},
  urldate = {2024-02-25},
  abstract = {The rise of Artificial Intelligence (AI) recently empowered researchers to investigate hard mathematical problems which eluded traditional approaches for decades. Yet, the use of AI in Universal Algebra (UA)---one of the fields laying the foundations of modern mathematics---is still completely unexplored. This work proposes the first use of AI to investigate UA's conjectures with an equivalent equational and topological characterization. While topological representations would enable the analysis of such properties using graph neural networks, the limited transparency and brittle explainability of these models hinder their straightforward use to empirically validate existing conjectures or to formulate new ones. To bridge these gaps, we propose a general algorithm generating AI-ready datasets based on UA's conjectures, and introduce a novel neural layer to build fully interpretable graph networks. The results of our experiments demonstrate that interpretable graph networks: (i) enhance interpretability without sacrificing task accuracy, (ii) strongly generalize when predicting universal algebra's properties, (iii) generate simple explanations that empirically validate existing conjectures, and (iv) identify subgraphs suggesting the formulation of novel conjectures.},
  eventtitle = {Thirty-seventh {{Conference}} on {{Neural Information Processing Systems}}},
  langid = {english},
  file = {/Users/luke/Zotero/storage/QMXB4P4B/Giannini et al. - 2023 - Interpretable Graph Networks Formulate Universal A.pdf}
}

@inproceedings{gidarisUnsupervisedRepresentationLearning2018,
  title = {Unsupervised {{Representation Learning}} by {{Predicting Image Rotations}}},
  author = {Gidaris, Spyros and Singh, Praveer and Komodakis, Nikos},
  date = {2018-02-15},
  url = {https://openreview.net/forum?id=S1v4N2l0-},
  urldate = {2024-02-19},
  abstract = {Over the last years, deep convolutional neural networks (ConvNets) have transformed the field of computer vision thanks to their unparalleled capacity to learn high level semantic image features. However, in order to successfully learn those features, they usually require massive amounts of manually labeled data, which is both expensive and impractical to scale. Therefore, unsupervised semantic feature learning, i.e., learning without requiring manual annotation effort, is of crucial importance in order to successfully harvest the vast amount of visual data that are available today. In our work we propose to learn image features by training ConvNets to recognize the 2d rotation that is applied to the image that it gets as input. We demonstrate both qualitatively and quantitatively that this apparently simple task actually provides a very powerful supervisory signal for semantic feature learning. We exhaustively evaluate our method in various unsupervised feature learning benchmarks and we exhibit in all of them state-of-the-art performance. Specifically, our results on those benchmarks demonstrate dramatic improvements w.r.t. prior state-of-the-art approaches in unsupervised representation learning and thus significantly close the gap with supervised feature learning. For instance, in PASCAL VOC 2007 detection task our unsupervised pre-trained AlexNet model achieves the state-of-the-art (among unsupervised methods) mAP of 54.4\%\$that is only 2.4 points lower from the supervised case. We get similarly striking results when we transfer our unsupervised learned features on various other tasks, such as ImageNet classification, PASCAL classification, PASCAL segmentation, and CIFAR-10 classification. The code and models of our paper will be published on: https://github.com/gidariss/FeatureLearningRotNet},
  eventtitle = {International {{Conference}} on {{Learning Representations}}},
  langid = {english},
  file = {/Users/luke/Zotero/storage/ADANBTAG/Gidaris et al. - 2018 - Unsupervised Representation Learning by Predicting.pdf}
}

@inproceedings{gilesCiteSeerAutomaticCitation1998,
  title = {{{CiteSeer}}: an automatic citation indexing system},
  shorttitle = {{{CiteSeer}}},
  booktitle = {Proceedings of the third {{ACM}} conference on {{Digital}} libraries},
  author = {Giles, C. Lee and Bollacker, Kurt D. and Lawrence, Steve},
  date = {1998-05-11},
  series = {{{DL}} '98},
  pages = {89--98},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  url = {https://dl.acm.org/doi/10.1145/276675.276685},
  urldate = {2023-12-01},
  isbn = {978-0-89791-965-4},
  file = {/Users/luke/Zotero/storage/2U766ZX7/Giles et al. - 1998 - CiteSeer an automatic citation indexing system.pdf}
}

@misc{gilmerNeuralMessagePassing2017,
  title = {Neural {{Message Passing}} for {{Quantum Chemistry}}},
  author = {Gilmer, Justin and Schoenholz, Samuel S. and Riley, Patrick F. and Vinyals, Oriol and Dahl, George E.},
  date = {2017-06-12},
  eprint = {1704.01212},
  eprinttype = {arXiv},
  eprintclass = {cs.LG},
  url = {http://arxiv.org/abs/1704.01212},
  urldate = {2023-12-20},
  abstract = {Supervised learning on molecules has incredible potential to be useful in chemistry, drug discovery, and materials science. Luckily, several promising and closely related neural network models invariant to molecular symmetries have already been described in the literature. These models learn a message passing algorithm and aggregation procedure to compute a function of their entire input graph. At this point, the next step is to find a particularly effective variant of this general approach and apply it to chemical prediction benchmarks until we either solve them or reach the limits of the approach. In this paper, we reformulate existing models into a single common framework we call Message Passing Neural Networks (MPNNs) and explore additional novel variations within this framework. Using MPNNs we demonstrate state of the art results on an important molecular property prediction benchmark; these results are strong enough that we believe future work should focus on datasets with larger molecules or more accurate ground truth labels.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,I.2.6},
  file = {/Users/luke/Zotero/storage/9TVXEK39/Gilmer et al. - 2017 - Neural Message Passing for Quantum Chemistry.pdf;/Users/luke/Zotero/storage/TW4UKWGQ/1704.html},
  note = {arXiv:1704.01212}
}

@article{giustiTwoCompanyThree2016,
  title = {Two’s company, three (or more) is a simplex},
  author = {Giusti, Chad and Ghrist, Robert and Bassett, Danielle S.},
  date = {2016-08-01},
  journaltitle = {Journal of Computational Neuroscience},
  shortjournal = {J Comput Neurosci},
  volume = {41},
  number = {1},
  pages = {1--14},
  issn = {1573-6873},
  url = {https://doi.org/10.1007/s10827-016-0608-6},
  urldate = {2024-03-19},
  abstract = {The language of graph theory, or network science, has proven to be an exceptional tool for addressing myriad problems in neuroscience. Yet, the use of networks is predicated on a critical simplifying assumption: that the quintessential unit of interest in a brain is a dyad – two nodes (neurons or brain regions) connected by an edge. While rarely mentioned, this fundamental assumption inherently limits the types of neural structure and function that graphs can be used to model. Here, we describe a generalization of graphs that overcomes these limitations, thereby offering a broad range of new possibilities in terms of modeling and measuring neural phenomena. Specifically, we explore the use of simplicial complexes: a structure developed in the field of mathematics known as algebraic topology, of increasing applicability to real data due to a rapidly growing computational toolset. We review the underlying mathematical formalism as well as the budding literature applying simplicial complexes to neural data, from electrophysiological recordings in animal models to hemodynamic fluctuations in humans. Based on the exceptional flexibility of the tools and recent ground-breaking insights into neural function, we posit that this framework has the potential to eclipse graph theory in unraveling the fundamental mysteries of cognition.},
  langid = {english},
  keywords = {Filtration,Networks,Simplicial complex,Topology},
  file = {/Users/luke/Zotero/storage/Z8UAYHM9/Giusti et al. - 2016 - Two’s company, three (or more) is a simplex.pdf}
}

@inproceedings{gopinathRepairingIntricateFaults2016,
  title = {Repairing {{Intricate Faults}} in {{Code Using Machine Learning}} and {{Path Exploration}}},
  booktitle = {2016 {{IEEE International Conference}} on {{Software Maintenance}} and {{Evolution}} ({{ICSME}})},
  author = {Gopinath, Divya and Wang, Kaiyuan and Hua, Jinru and Khurshid, Sarfraz},
  date = {2016-10},
  pages = {453--457},
  url = {https://ieeexplore.ieee.org/document/7816493},
  urldate = {2023-12-19},
  abstract = {Debugging remains costly and tedious, especially for code that performs intricate operations that are conceptually complex to reason about. We present MLR, a novel approach for repairing faults in such operations, specifically in the context of complex data structures. Our focus is on faults in conditional statements. Our insight is that an integrated approach based on machine learning and systematic path exploration can provide effective repairs. MLR mines the data-spectra of the passing and failing executions of conditional branches to prune the search space for repair and generate patches that are likely valid beyond the existing test-suite. We apply MLR to repair faults in small but complex data structure subjects to demonstrate its efficacy. Experimental results show that MLR has the potential to repair this fault class more effectively than state-of-the-art repair tools.},
  eventtitle = {2016 {{IEEE International Conference}} on {{Software Maintenance}} and {{Evolution}} ({{ICSME}})},
  file = {/Users/luke/Zotero/storage/AW8ZHHG2/7816493.html}
}

@inproceedings{goriNewModelLearning2005,
  title = {A new model for learning in graph domains},
  booktitle = {Proceedings. {{IEEE International Joint Conference}} on {{Neural Networks}}},
  author = {Gori, Marco and Monfardini, Gabriele and Scarselli, Franco},
  date = {2005-07},
  volume = {2},
  pages = {729--734},
  issn = {2161-4407},
  url = {https://ieeexplore.ieee.org/document/1555942},
  urldate = {2024-02-20},
  abstract = {In several applications the information is naturally represented by graphs. Traditional approaches cope with graphical data structures using a preprocessing phase which transforms the graphs into a set of flat vectors. However, in this way, important topological information may be lost and the achieved results may heavily depend on the preprocessing stage. This paper presents a new neural model, called graph neural network (GNN), capable of directly processing graphs. GNNs extends recursive neural networks and can be applied on most of the practically useful kinds of graphs, including directed, undirected, labelled and cyclic graphs. A learning algorithm for GNNs is proposed and some experiments are discussed which assess the properties of the model.},
  eventtitle = {{{IEEE International Joint Conference}} on {{Neural Networks}}},
  keywords = {Application software,Data structures,Encoding,Focusing,Machine learning,Machine learning algorithms,Neural networks,Recurrent neural networks,Software engineering,Tree graphs},
  file = {/Users/luke/Zotero/storage/UGB834HC/Gori et al. - 2005 - A new model for learning in graph domains.pdf;/Users/luke/Zotero/storage/ZFDQ7ZN9/1555942.html}
}

@inproceedings{goriRecursiveNeuralNetwork2003,
  title = {A recursive neural network model for processing directed acyclic graphs with labeled edges},
  booktitle = {Proceedings of the {{International Joint Conference}} on {{Neural Networks}}, 2003.},
  author = {Gori, M. and Maggini, M. and Sarti, L.},
  date = {2003-07},
  volume = {2},
  pages = {1351-1355 vol.2},
  issn = {1098-7576},
  url = {https://ieeexplore.ieee.org/document/1223892},
  urldate = {2024-02-23},
  abstract = {The recursive paradigm extends the neural network processing and learning algorithms to deal with structured inputs. In particular, recursive neural network (RNN) models have been proposed to process information coded as directed positional acyclic graphs (DPAGs) whose maximum node outdegree is known a priori. Unfortunately, the hypothesis of processing DPAGs having a given maximum node outdegree is sometimes too restrictive, being the nature of some real-world problems intrinsically disordered. In many applications the node outdegrees can vary considerably among the nodes in the graph, it may be unnatural to define a position for each child of a given node, and it may be necessary to prune some edges to reduce the number of the network parameters, which is proportional to the maximum node outdegree. In this paper, we proposed a new recursive neural network model which allows us to process directed acyclic graphs (DAGs) with labeled edges, relaxing the positional constraint and the correlated maximum outdegree limit. The effectiveness of the new scheme is experimentally tested on an image classification task. The results show that the new RNN model outperforms the standard RNN architecture, also allowing us to use a smaller number of free parameters.},
  eventtitle = {Proceedings of the {{International Joint Conference}} on {{Neural Networks}}, 2003.},
  keywords = {Chemistry,Image classification,Internet,Labeling,Neural networks,Pattern recognition,Recommender systems,Recurrent neural networks,Testing,Tree graphs},
  file = {/Users/luke/Zotero/storage/U7UJY525/Gori et al. - 2003 - A recursive neural network model for processing di.pdf;/Users/luke/Zotero/storage/BK7W32TB/1223892.html}
}

@inproceedings{groverNode2vecScalableFeature2016,
  title = {node2vec: {{Scalable Feature Learning}} for {{Networks}}},
  shorttitle = {node2vec},
  booktitle = {Proceedings of the 22nd {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} and {{Data Mining}}},
  author = {Grover, Aditya and Leskovec, Jure},
  date = {2016-08-13},
  series = {{{KDD}} '16},
  pages = {855--864},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  url = {https://doi.org/10.1145/2939672.2939754},
  urldate = {2024-03-27},
  abstract = {Prediction tasks over nodes and edges in networks require careful effort in engineering features used by learning algorithms. Recent research in the broader field of representation learning has led to significant progress in automating prediction by learning the features themselves. However, present feature learning approaches are not expressive enough to capture the diversity of connectivity patterns observed in networks. Here we propose node2vec, an algorithmic framework for learning continuous feature representations for nodes in networks. In node2vec, we learn a mapping of nodes to a low-dimensional space of features that maximizes the likelihood of preserving network neighborhoods of nodes. We define a flexible notion of a node's network neighborhood and design a biased random walk procedure, which efficiently explores diverse neighborhoods. Our algorithm generalizes prior work which is based on rigid notions of network neighborhoods, and we argue that the added flexibility in exploring neighborhoods is the key to learning richer representations. We demonstrate the efficacy of node2vec over existing state-of-the-art techniques on multi-label classification and link prediction in several real-world networks from diverse domains. Taken together, our work represents a new way for efficiently learning state-of-the-art task-independent representations in complex networks.},
  isbn = {978-1-4503-4232-2},
  keywords = {feature learning,graph representations,information networks,node embeddings},
  file = {/Users/luke/Zotero/storage/8RL5G99J/Grover and Leskovec - 2016 - node2vec Scalable Feature Learning for Networks.pdf}
}

@article{guoAttentionBasedSpatialTemporal2019,
  title = {Attention {{Based Spatial-Temporal Graph Convolutional Networks}} for {{Traffic Flow Forecasting}}},
  author = {Guo, Shengnan and Lin, Youfang and Feng, Ning and Song, Chao and Wan, Huaiyu},
  date = {2019-07-17},
  journaltitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {33},
  number = {01},
  pages = {922--929},
  issn = {2374-3468},
  url = {https://ojs.aaai.org/index.php/AAAI/article/view/3881},
  urldate = {2024-02-26},
  abstract = {Forecasting the traffic flows is a critical issue for researchers and practitioners in the field of transportation. However, it is very challenging since the traffic flows usually show high nonlinearities and complex patterns. Most existing traffic flow prediction methods, lacking abilities of modeling the dynamic spatial-temporal correlations of traffic data, thus cannot yield satisfactory prediction results. In this paper, we propose a novel attention based spatial-temporal graph convolutional network (ASTGCN) model to solve traffic flow forecasting problem. ASTGCN mainly consists of three independent components to respectively model three temporal properties of traffic flows, i.e., recent, daily-periodic and weekly-periodic dependencies. More specifically, each component contains two major parts: 1) the spatial-temporal attention mechanism to effectively capture the dynamic spatialtemporal correlations in traffic data; 2) the spatial-temporal convolution which simultaneously employs graph convolutions to capture the spatial patterns and common standard convolutions to describe the temporal features. The output of the three components are weighted fused to generate the final prediction results. Experiments on two real-world datasets from the Caltrans Performance Measurement System (PeMS) demonstrate that the proposed ASTGCN model outperforms the state-of-the-art baselines.},
  issue = {01},
  langid = {english},
  file = {/Users/luke/Zotero/storage/7DUCXIW4/Guo et al. - 2019 - Attention Based Spatial-Temporal Graph Convolution.pdf}
}

@article{guoBrainFunctionNetwork2021,
  title = {Brain {{Function Network}}: {{Higher Order}} vs. {{More Discrimination}}},
  shorttitle = {Brain {{Function Network}}},
  author = {Guo, Tingting and Zhang, Yining and Xue, Yanfang and Qiao, Lishan and Shen, Dinggang},
  date = {2021},
  journaltitle = {Frontiers in Neuroscience},
  shortjournal = {Front Neurosci},
  volume = {15},
  eprint = {34497485},
  eprinttype = {pmid},
  pages = {696639},
  issn = {1662-4548},
  abstract = {Brain functional network (BFN) has become an increasingly important tool to explore individual differences and identify neurological/mental diseases. For estimating a "good" BFN (with more discriminative information for example), researchers have developed various methods, in which the most popular and simplest is Pearson's correlation (PC). Despite its empirical effectiveness, PC only encodes the low-order (second-order) statistics between brain regions. To model high-order statistics, researchers recently proposed to estimate BFN by conducting two sequential PCs (denoted as PC 2 in this paper), and found that PC 2-based BFN can provide additional information for group difference analysis. This inspires us to think about (1) what will happen if continuing the correlation operation to construct much higher-order BFN by PC n (n{$>$}2), and (2) whether the higher-order correlation will result in stronger discriminative ability. To answer these questions, we use PC n -based BFNs to predict individual differences (Female vs. Male) as well as identify subjects with mild cognitive impairment (MCI) from healthy controls (HCs). Through experiments, we have the following findings: (1) with the increase of n, the discriminative ability of PC n -based BFNs tends to decrease; (2) fusing the PC n -based BFNs (n{$>$}1) with the PC 1-based BFN can generally improve the sensitivity for MCI identification, but fail to help the classification accuracy. In addition, we empirically find that the sequence of BFN adjacency matrices estimated by PC n (n = 1,2,3,⋯\,) will converge to a binary matrix with elements of ± 1.},
  langid = {english},
  pmcid = {PMC8419271},
  keywords = {brain functional network,gender prediction,higher-order correlation,mild cognitive impairment,Pearson's correlation},
  file = {/Users/luke/Zotero/storage/UPRTFRGH/Guo et al. - 2021 - Brain Function Network Higher Order vs. More Disc.pdf}
}

@inproceedings{guoOnlineKnowledgeDistillation2020,
  title = {Online {{Knowledge Distillation}} via {{Collaborative Learning}}},
  author = {Guo, Qiushan and Wang, Xinjiang and Wu, Yichao and Yu, Zhipeng and Liang, Ding and Hu, Xiaolin and Luo, Ping},
  date = {2020},
  url = {https://openaccess.thecvf.com/content_CVPR_2020/html/Guo_Online_Knowledge_Distillation_via_Collaborative_Learning_CVPR_2020_paper.html},
  urldate = {2023-10-20},
  eventtitle = {{{CVPR}}},
  file = {/Users/luke/Zotero/storage/YNZWFCDM/Guo et al. - 2020 - Online Knowledge Distillation via Collaborative Le.pdf}
}

@article{haimanArguesianLatticesWhich1991,
  title = {Arguesian lattices which are not type-1},
  author = {Haiman, Mark},
  date = {1991-03-01},
  journaltitle = {algebra universalis},
  shortjournal = {Algebra Universalis},
  volume = {28},
  number = {1},
  pages = {128--137},
  issn = {1420-8911},
  url = {https://doi.org/10.1007/BF01190416},
  urldate = {2024-03-09},
  langid = {english},
  file = {/Users/luke/Zotero/storage/NNTYKJGV/Haiman - 1991 - Arguesian lattices which are not type-1.pdf}
}

@article{haimanTwoNotesArguesian1985,
  title = {Two notes on the {{Arguesian}} identity},
  author = {Haiman, Mark},
  date = {1985-06-01},
  journaltitle = {algebra universalis},
  shortjournal = {Algebra Universalis},
  volume = {21},
  number = {2},
  pages = {167--171},
  issn = {1420-8911},
  url = {https://doi.org/10.1007/BF01188053},
  urldate = {2024-03-09},
  abstract = {We find an explicitly self-dual lattice identity equivalent to the Arguesian law. We also show that any lattice identity equivalent to the Arguesian law must necessarily involve at least six variables.},
  langid = {english},
  keywords = {Arguesian Identity,Lattice Identity},
  file = {/Users/luke/Zotero/storage/322Q4T7C/Haiman - 1985 - Two notes on the Arguesian identity.pdf}
}

@misc{hajijTopologicalDeepLearning2023,
  title = {Topological {{Deep Learning}}: {{Going Beyond Graph Data}}},
  shorttitle = {Topological {{Deep Learning}}},
  author = {Hajij, Mustafa and Zamzmi, Ghada and Papamarkou, Theodore and Miolane, Nina and Guzmán-Sáenz, Aldo and Ramamurthy, Karthikeyan Natesan and Birdal, Tolga and Dey, Tamal K. and Mukherjee, Soham and Samaga, Shreyas N. and Livesay, Neal and Walters, Robin and Rosen, Paul and Schaub, Michael T.},
  date = {2023-05-19},
  eprint = {2206.00606},
  eprinttype = {arXiv},
  eprintclass = {cs.LG},
  url = {http://arxiv.org/abs/2206.00606},
  urldate = {2024-05-04},
  abstract = {Topological deep learning is a rapidly growing field that pertains to the development of deep learning models for data supported on topological domains such as simplicial complexes, cell complexes, and hypergraphs, which generalize many domains encountered in scientific computations. In this paper, we present a unifying deep learning framework built upon a richer data structure that includes widely adopted topological domains. Specifically, we first introduce combinatorial complexes, a novel type of topological domain. Combinatorial complexes can be seen as generalizations of graphs that maintain certain desirable properties. Similar to hypergraphs, combinatorial complexes impose no constraints on the set of relations. In addition, combinatorial complexes permit the construction of hierarchical higher-order relations, analogous to those found in simplicial and cell complexes. Thus, combinatorial complexes generalize and combine useful traits of both hypergraphs and cell complexes, which have emerged as two promising abstractions that facilitate the generalization of graph neural networks to topological spaces. Second, building upon combinatorial complexes and their rich combinatorial and algebraic structure, we develop a general class of message-passing combinatorial complex neural networks (CCNNs), focusing primarily on attention-based CCNNs. We characterize permutation and orientation equivariances of CCNNs, and discuss pooling and unpooling operations within CCNNs in detail. Third, we evaluate the performance of CCNNs on tasks related to mesh shape analysis and graph learning. Our experiments demonstrate that CCNNs have competitive performance as compared to state-of-the-art deep learning models specifically tailored to the same tasks. Our findings demonstrate the advantages of incorporating higher-order relations into deep learning models in different applications.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Social and Information Networks,Mathematics - Algebraic Topology,Statistics - Machine Learning},
  file = {/Users/luke/Zotero/storage/NMTJP267/Hajij et al. - 2023 - Topological Deep Learning Going Beyond Graph Data.pdf;/Users/luke/Zotero/storage/JH7M7DSV/2206.html},
  note = {arXiv:2206.00606}
}

@misc{hajijTopoXSuitePython2024,
  title = {{{TopoX}}: {{A Suite}} of {{Python Packages}} for {{Machine Learning}} on {{Topological Domains}}},
  shorttitle = {{{TopoX}}},
  author = {Hajij, Mustafa and Papillon, Mathilde and Frantzen, Florian and Agerberg, Jens and AlJabea, Ibrahem and Ballester, Ruben and Battiloro, Claudio and Bernárdez, Guillermo and Birdal, Tolga and Brent, Aiden and Chin, Peter and Escalera, Sergio and Fiorellino, Simone and Gardaa, Odin Hoff and Gopalakrishnan, Gurusankar and Govil, Devendra and Hoppe, Josef and Karri, Maneel Reddy and Khouja, Jude and Lecha, Manuel and Livesay, Neal and Meißner, Jan and Mukherjee, Soham and Nikitin, Alexander and Papamarkou, Theodore and Prílepok, Jaro and Ramamurthy, Karthikeyan Natesan and Rosen, Paul and Guzmán-Sáenz, Aldo and Salatiello, Alessandro and Samaga, Shreyas N. and Scardapane, Simone and Schaub, Michael T. and Scofano, Luca and Spinelli, Indro and Telyatnikov, Lev and Truong, Quang and Walters, Robin and Yang, Maosheng and Zaghen, Olga and Zamzmi, Ghada and Zia, Ali and Miolane, Nina},
  date = {2024-02-17},
  eprint = {2402.02441},
  eprinttype = {arXiv},
  eprintclass = {cs.LG},
  url = {http://arxiv.org/abs/2402.02441},
  urldate = {2024-05-20},
  abstract = {We introduce TopoX, a Python software suite that provides reliable and user-friendly building blocks for computing and machine learning on topological domains that extend graphs: hypergraphs, simplicial, cellular, path and combinatorial complexes. TopoX consists of three packages: TopoNetX facilitates constructing and computing on these domains, including working with nodes, edges and higher-order cells; TopoEmbedX provides methods to embed topological domains into vector spaces, akin to popular graph-based embedding algorithms such as node2vec; TopoModelx is built on top of PyTorch and offers a comprehensive toolbox of higher-order message passing functions for neural networks on topological domains. The extensively documented and unit-tested source code of TopoX is available under MIT license at https://pyt-team.github.io/.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Mathematical Software,Statistics - Computation},
  file = {/Users/luke/Zotero/storage/PUDACZHV/Hajij et al. - 2024 - TopoX A Suite of Python Packages for Machine Lear.pdf;/Users/luke/Zotero/storage/BRY2PZ2U/2402.html},
  note = {arXiv:2402.02441}
}

@book{hamiltonGraphRepresentationLearning2020,
  title = {Graph {{Representation Learning}}},
  author = {Hamilton, William L.},
  date = {2020},
  series = {Synthesis {{Lectures}} on {{Artificial Intelligence}} and {{Machine Learning}}},
  publisher = {Springer International Publishing},
  location = {Cham},
  url = {https://link.springer.com/10.1007/978-3-031-01588-5},
  urldate = {2023-10-11},
  isbn = {978-3-031-00460-5},
  langid = {english}
}

@inproceedings{hamiltonInductiveRepresentationLearning2017,
  title = {Inductive {{Representation Learning}} on {{Large Graphs}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Hamilton, Will and Ying, Zhitao and Leskovec, Jure},
  date = {2017},
  volume = {30},
  publisher = {Curran Associates, Inc.},
  url = {https://proceedings.neurips.cc/paper_files/paper/2017/hash/5dd9db5e033da9c6fb5ba83c7a7ebea9-Abstract.html},
  urldate = {2024-01-26},
  abstract = {Low-dimensional embeddings of nodes in large graphs have proved extremely useful in a variety of prediction tasks, from content recommendation to identifying protein functions. However, most existing approaches require that all nodes in the graph are present during training of the embeddings; these previous approaches are inherently transductive and do not naturally generalize to unseen nodes. Here we present GraphSAGE, a general, inductive framework that leverages node feature information (e.g., text attributes) to efficiently generate node embeddings.  Instead of training individual embeddings for each node, we learn a function that generates embeddings by sampling and aggregating features from a node's local neighborhood. Our algorithm outperforms strong baselines on three inductive node-classification benchmarks: we classify the category of unseen nodes in evolving information graphs based on citation and Reddit post data, and we show that our algorithm generalizes to completely unseen graphs using a multi-graph dataset of protein-protein interactions.},
  eventtitle = {{{NIPS}} 2017},
  file = {/Users/luke/Zotero/storage/S8CGCTG9/Hamilton et al. - 2017 - Inductive Representation Learning on Large Graphs.pdf}
}

@misc{hansenOpinionDynamicsDiscourse2020,
  title = {Opinion {{Dynamics}} on {{Discourse Sheaves}}},
  author = {Hansen, Jakob and Ghrist, Robert},
  date = {2020-05-26},
  eprint = {2005.12798},
  eprinttype = {arXiv},
  eprintclass = {math.DS},
  url = {http://arxiv.org/abs/2005.12798},
  urldate = {2023-10-26},
  abstract = {We introduce a novel class of Laplacians and diffusion dynamics on discourse sheaves as a model for network dynamics, with application to opinion dynamics on social networks. These sheaves are algebraic data structures tethered to a network (or more general space) that can represent various modes of communication, including selective opinion modulation and lying. After introducing the sheaf model, we develop a sheaf Laplacian in this context and show how to evolve both opinions and communications with diffusion dynamics over the network. Issues of controllability, reachability, bounded confidence, and harmonic extension are addressed using this framework.},
  pubstate = {prepublished},
  keywords = {91D30 (Primary) 55N30 (Secondary),Mathematics - Algebraic Topology,Mathematics - Dynamical Systems},
  file = {/Users/luke/Zotero/storage/UWKDPS86/Hansen and Ghrist - 2020 - Opinion Dynamics on Discourse Sheaves.pdf;/Users/luke/Zotero/storage/FBTCE94B/2005.html},
  note = {arXiv:2005.12798}
}

@misc{hansenSheafNeuralNetworks2020,
  title = {Sheaf {{Neural Networks}}},
  author = {Hansen, Jakob and Gebhart, Thomas},
  date = {2020-12-07},
  eprint = {2012.06333},
  eprinttype = {arXiv},
  eprintclass = {cs.LG},
  url = {http://arxiv.org/abs/2012.06333},
  urldate = {2023-11-04},
  abstract = {We present a generalization of graph convolutional networks by generalizing the diffusion operation underlying this class of graph neural networks. These sheaf neural networks are based on the sheaf Laplacian, a generalization of the graph Laplacian that encodes additional relational structure parameterized by the underlying graph. The sheaf Laplacian and associated matrices provide an extended version of the diffusion operation in graph convolutional networks, providing a proper generalization for domains where relations between nodes are non-constant, asymmetric, and varying in dimension. We show that the resulting sheaf neural networks can outperform graph convolutional networks in domains where relations between nodes are asymmetric and signed.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Mathematics - Algebraic Topology},
  file = {/Users/luke/Zotero/storage/YNWCFDZ3/Hansen and Gebhart - 2020 - Sheaf Neural Networks.pdf;/Users/luke/Zotero/storage/IW3SCLTI/2012.html},
  note = {arXiv:2012.06333}
}

@article{hansenSpectralTheoryCellular2019,
  title = {Toward a spectral theory of cellular sheaves},
  author = {Hansen, Jakob and Ghrist, Robert},
  date = {2019-12-01},
  journaltitle = {Journal of Applied and Computational Topology},
  shortjournal = {J Appl. and Comput. Topology},
  volume = {3},
  number = {4},
  pages = {315--358},
  issn = {2367-1734},
  url = {https://doi.org/10.1007/s41468-019-00038-7},
  urldate = {2024-03-16},
  abstract = {This paper outlines a program in what one might call spectral sheaf theory—an extension of spectral graph theory to cellular sheaves. By lifting the combinatorial graph Laplacian to the Hodge Laplacian on a cellular sheaf of vector spaces over a regular cell complex, one can relate spectral data to the sheaf cohomology and cell structure in a manner reminiscent of spectral graph theory. This work gives an exploratory introduction, and includes discussion of eigenvalue interlacing, sparsification, effective resistance, synchronization, and sheaf approximation. These results and subsequent applications are prefaced by an introduction to cellular sheaves and Laplacians.},
  langid = {english},
  keywords = {Cellular sheaf theory,Cohomology,Effective resistance,Eigenvalue interlacing,MSC 05C50,MSC 55N30,Spectral graph theory},
  file = {/Users/luke/Zotero/storage/D7LXYAXL/Hansen and Ghrist - 2019 - Toward a spectral theory of cellular sheaves.pdf}
}

@inproceedings{hayhoeTransferableHypergraphNeural2023,
  title = {Transferable {{Hypergraph Neural Networks}} via {{Spectral Similarity}}},
  author = {Hayhoe, Mikhail and Riess, Hans Matthew and Zavlanos, Michael M. and Preciado, Victor and Ribeiro, Alejandro},
  date = {2023-11-25},
  url = {https://openreview.net/forum?id=cHuii4NOB9},
  urldate = {2023-12-04},
  abstract = {Hypergraphs model higher-order interactions in complex systems, e.g., chemicals reacting only in the presence of an enzyme or rumors spreading across groups, and encompass both the notion of an undirected graph and a simplicial complex. Nonetheless, due to computational complexity, machine learning on hypergraph-structured data is notoriously challenging. In an effort to transfer hypergraph neural network models, addressing this challenge, we extend results on the transferability of Graph Neural Networks (GNNs) to design a convolutional architecture for processing signals supported on hypergraphs via GNNs, which we call Hypergraph Expansion Neural Networks (HENNs). Exploiting multiple spectrally-similar graph representations of hypergraphs, we establish bounds on the transferability error. Experimental results illustrate the importance of considering multiple graph representations in HENNs, and show promise of superior performance when transferability is required.},
  eventtitle = {The {{Second Learning}} on {{Graphs Conference}}},
  langid = {english},
  file = {/Users/luke/Zotero/storage/BQB62NHU/Hayhoe et al. - 2023 - Transferable Hypergraph Neural Networks via Spectr.pdf}
}

@misc{heinTotalVariationHypergraphs2013,
  title = {The {{Total Variation}} on {{Hypergraphs}} - {{Learning}} on {{Hypergraphs Revisited}}},
  author = {Hein, Matthias and Setzer, Simon and Jost, Leonardo and Rangapuram, Syama Sundar},
  date = {2013-12-18},
  eprint = {1312.5179},
  eprinttype = {arXiv},
  eprintclass = {stat.ML},
  url = {http://arxiv.org/abs/1312.5179},
  urldate = {2023-11-08},
  abstract = {Hypergraphs allow one to encode higher-order relationships in data and are thus a very flexible modeling tool. Current learning methods are either based on approximations of the hypergraphs via graphs or on tensor methods which are only applicable under special conditions. In this paper, we present a new learning framework on hypergraphs which fully uses the hypergraph structure. The key element is a family of regularization functionals based on the total variation on hypergraphs.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Mathematics - Optimization and Control,Statistics - Machine Learning},
  file = {/Users/luke/Zotero/storage/VGFZGA3A/Hein et al. - 2013 - The Total Variation on Hypergraphs - Learning on H.pdf;/Users/luke/Zotero/storage/MQQETQRQ/1312.html},
  note = {arXiv:1312.5179}
}

@inproceedings{heLightGCNSimplifyingPowering2020,
  title = {{{LightGCN}}: {{Simplifying}} and {{Powering Graph Convolution Network}} for {{Recommendation}}},
  shorttitle = {{{LightGCN}}},
  booktitle = {Proceedings of the 43rd {{International ACM SIGIR Conference}} on {{Research}} and {{Development}} in {{Information Retrieval}}},
  author = {He, Xiangnan and Deng, Kuan and Wang, Xiang and Li, Yan and Zhang, YongDong and Wang, Meng},
  date = {2020-07-25},
  series = {{{SIGIR}} '20},
  pages = {639--648},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  url = {https://dl.acm.org/doi/10.1145/3397271.3401063},
  urldate = {2024-02-15},
  abstract = {Graph Convolution Network (GCN) has become new state-of-the-art for collaborative filtering. Nevertheless, the reasons of its effectiveness for recommendation are not well understood. Existing work that adapts GCN to recommendation lacks thorough ablation analyses on GCN, which is originally designed for graph classification tasks and equipped with many neural network operations. However, we empirically find that the two most common designs in GCNs -- feature transformation and nonlinear activation -- contribute little to the performance of collaborative filtering. Even worse, including them adds to the difficulty of training and degrades recommendation performance. In this work, we aim to simplify the design of GCN to make it more concise and appropriate for recommendation. We propose a new model named LightGCN, including only the most essential component in GCN -- neighborhood aggregation -- for collaborative filtering. Specifically, LightGCN learns user and item embeddings by linearly propagating them on the user-item interaction graph, and uses the weighted sum of the embeddings learned at all layers as the final embedding. Such simple, linear, and neat model is much easier to implement and train, exhibiting substantial improvements (about 16.0\% relative improvement on average) over Neural Graph Collaborative Filtering (NGCF) -- a state-of-the-art GCN-based recommender model -- under exactly the same experimental setting. Further analyses are provided towards the rationality of the simple LightGCN from both analytical and empirical perspectives.},
  isbn = {978-1-4503-8016-4},
  keywords = {collaborative filtering,embedding propagation,graph neural network,recommendation},
  file = {/Users/luke/Zotero/storage/F7SMUPLV/He et al. - 2020 - LightGCN Simplifying and Powering Graph Convoluti.pdf}
}

@inproceedings{heSheafbasedPositionalEncodings2023,
  title = {Sheaf-based {{Positional Encodings}} for {{Graph Neural Networks}}},
  author = {He, Yu and Bodnar, Cristian and Lio, Pietro},
  date = {2023-11-29},
  url = {https://openreview.net/forum?id=ZtAabWUPu3},
  urldate = {2024-07-08},
  abstract = {Graph Neural Networks (GNNs) work directly with graph-structured data, capitalising on relational information among entities. One limitation of GNNs is their reliance on local interactions among connected nodes. GNNs may generate identical node embeddings for similar local neighbourhoods and fail to distinguish structurally distinct graphs. Positional encodings help to break the locality constraint by informing the nodes of their global positions in the graph. Furthermore, they are required by Graph Transformers to encode structural information. However, existing positional encodings based on the graph Laplacian only encode structural information and are typically fixed. To address these limitations, we propose a novel approach to design positional encodings using sheaf theory. The sheaf Laplacian can be learnt from node data, allowing it to encode both the structure and semantic information. We present two methodologies for creating sheaf-based positional encodings, showcasing their efficacy in node and graph tasks. Our work advances the integration of sheaves in graph learning, paving the way for innovative GNN techniques that draw inspiration from geometry and topology.},
  eventtitle = {{{NeurIPS}} 2023 {{Workshop}} on {{Symmetry}} and {{Geometry}} in {{Neural Representations}}},
  langid = {english},
  file = {/Users/luke/Zotero/storage/K8MM5U89/He et al. - 2023 - Sheaf-based Positional Encodings for Graph Neural .pdf}
}

@misc{hintonDistillingKnowledgeNeural2015,
  title = {Distilling the {{Knowledge}} in a {{Neural Network}}},
  author = {Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff},
  date = {2015-03-09},
  eprint = {1503.02531},
  eprinttype = {arXiv},
  eprintclass = {stat.ML},
  url = {http://arxiv.org/abs/1503.02531},
  urldate = {2023-10-19},
  abstract = {A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/Users/luke/Zotero/storage/K6FEK42G/Hinton et al. - 2015 - Distilling the Knowledge in a Neural Network.pdf;/Users/luke/Zotero/storage/W253VBFU/1503.html},
  note = {arXiv:1503.02531}
}

@article{hitchcockExpressionTensorPolyadic1927,
  title = {The {{Expression}} of a {{Tensor}} or a {{Polyadic}} as a {{Sum}} of {{Products}}},
  author = {Hitchcock, Frank L.},
  date = {1927},
  journaltitle = {Journal of Mathematics and Physics},
  volume = {6},
  number = {1-4},
  pages = {164--189},
  issn = {1467-9590},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/sapm192761164},
  urldate = {2024-05-10},
  langid = {english}
}

@article{hochreiterLongShortTermMemory1997,
  title = {Long {{Short-Term Memory}}},
  author = {Hochreiter, Sepp and Schmidhuber, Jürgen},
  date = {1997-11-01},
  journaltitle = {Neural Computation},
  shortjournal = {Neural Comput.},
  volume = {9},
  number = {8},
  pages = {1735--1780},
  issn = {0899-7667},
  url = {https://doi.org/10.1162/neco.1997.9.8.1735},
  urldate = {2024-03-28},
  abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.}
}

@misc{hongAttentionbasedGraphNeural2019,
  title = {An {{Attention-based Graph Neural Network}} for {{Heterogeneous Structural Learning}}},
  author = {Hong, Huiting and Guo, Hantao and Lin, Yucheng and Yang, Xiaoqing and Li, Zang and Ye, Jieping},
  date = {2019-12-19},
  eprint = {1912.10832},
  eprinttype = {arXiv},
  eprintclass = {cs.LG},
  url = {http://arxiv.org/abs/1912.10832},
  urldate = {2023-11-04},
  abstract = {In this paper, we focus on graph representation learning of heterogeneous information network (HIN), in which various types of vertices are connected by various types of relations. Most of the existing methods conducted on HIN revise homogeneous graph embedding models via meta-paths to learn low-dimensional vector space of HIN. In this paper, we propose a novel Heterogeneous Graph Structural Attention Neural Network (HetSANN) to directly encode structural information of HIN without meta-path and achieve more informative representations. With this method, domain experts will not be needed to design meta-path schemes and the heterogeneous information can be processed automatically by our proposed model. Specifically, we implicitly represent heterogeneous information using the following two methods: 1) we model the transformation between heterogeneous vertices through a projection in low-dimensional entity spaces; 2) afterwards, we apply the graph neural network to aggregate multi-relational information of projected neighborhood by means of attention mechanism. We also present three extensions of HetSANN, i.e., voices-sharing product attention for the pairwise relationships in HIN, cycle-consistency loss to retain the transformation between heterogeneous entity spaces, and multi-task learning with full use of information. The experiments conducted on three public datasets demonstrate that our proposed models achieve significant and consistent improvements compared to state-of-the-art solutions.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Computer Science - Social and Information Networks,Statistics - Machine Learning},
  file = {/Users/luke/Zotero/storage/H8E3EYDQ/Hong et al. - 2019 - An Attention-based Graph Neural Network for Hetero.pdf;/Users/luke/Zotero/storage/QBHBUX4D/1912.html},
  note = {arXiv:1912.10832}
}

@inproceedings{hongAttentionBasedGraphNeural2020,
  title = {An {{Attention-Based Graph Neural Network}} for {{Heterogeneous Structural Learning}}},
  booktitle = {The {{Thirty-Fourth AAAI Conference}} on {{Artificial Intelligence}}, {{AAAI}} 2020, {{The Thirty-Second Innovative Applications}} of {{Artificial Intelligence Conference}}, {{IAAI}} 2020, {{The Tenth AAAI Symposium}} on {{Educational Advances}} in {{Artificial Intelligence}}, {{EAAI}} 2020, {{New York}}, {{NY}}, {{USA}}, {{February}} 7-12, 2020},
  author = {Hong, Huiting and Guo, Hantao and Lin, Yucheng and Yang, Xiaoqing and Li, Zang and Ye, Jieping},
  date = {2020},
  pages = {4132--4139},
  publisher = {AAAI Press},
  url = {https://doi.org/10.1609/aaai.v34i04.5833},
  urldate = {2024-07-05},
  file = {/Users/luke/Zotero/storage/CZMMIGNL/Hong et al. - 2020 - An Attention-Based Graph Neural Network for Hetero.pdf}
}

@article{hornikMultilayerFeedforwardNetworks1989,
  title = {Multilayer feedforward networks are universal approximators},
  author = {Hornik, Kurt and Stinchcombe, Maxwell and White, Halbert},
  date = {1989-01-01},
  journaltitle = {Neural Networks},
  shortjournal = {Neural Networks},
  volume = {2},
  number = {5},
  pages = {359--366},
  issn = {0893-6080},
  url = {https://www.sciencedirect.com/science/article/pii/0893608089900208},
  urldate = {2024-05-08},
  abstract = {This paper rigorously establishes that standard multilayer feedforward networks with as few as one hidden layer using arbitrary squashing functions are capable of approximating any Borel measurable function from one finite dimensional space to another to any desired degree of accuracy, provided sufficiently many hidden units are available. In this sense, multilayer feedforward networks are a class of universal approximators.},
  keywords = {Back-propagation networks,Feedforward networks,Mapping networks,Network representation capability,Sigma-Pi networks,Squashing functions,Stone-Weierstrass Theorem,Universal approximation}
}

@misc{hornTopologicalGraphNeural2022,
  title = {Topological {{Graph Neural Networks}}},
  author = {Horn, Max and De Brouwer, Edward and Moor, Michael and Moreau, Yves and Rieck, Bastian and Borgwardt, Karsten},
  date = {2022-03-17},
  eprint = {2102.07835},
  eprinttype = {arXiv},
  eprintclass = {cs, math, stat},
  url = {http://arxiv.org/abs/2102.07835},
  urldate = {2024-03-02},
  abstract = {Graph neural networks (GNNs) are a powerful architecture for tackling graph learning tasks, yet have been shown to be oblivious to eminent substructures such as cycles. We present TOGL, a novel layer that incorporates global topological information of a graph using persistent homology. TOGL can be easily integrated into any type of GNN and is strictly more expressive (in terms the Weisfeiler--Lehman graph isomorphism test) than message-passing GNNs. Augmenting GNNs with TOGL leads to improved predictive performance for graph and node classification tasks, both on synthetic data sets, which can be classified by humans using their topology but not by ordinary GNNs, and on real-world data.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Mathematics - Algebraic Topology,Statistics - Machine Learning},
  file = {/Users/luke/Zotero/storage/4FYAUZRX/Horn et al. - 2022 - Topological Graph Neural Networks.pdf;/Users/luke/Zotero/storage/NKB4XR4A/2102.html},
  note = {arXiv:2102.07835}
}

@article{hotz1999mars,
  title = {Mars probe lost due to simple math error},
  author = {Hotz, Robert Lee},
  date = {1999},
  journaltitle = {The Los Angeles times},
  shortjournal = {Los Angeles Times},
  volume = {1}
}

@inproceedings{huaHighorderPoolingGraph2022,
  title = {High-order pooling for graph neural networks with tensor decomposition},
  author = {Hua, Chenqing and Rabusseau, Guillaume and Tang, Jian},
  date = {2022},
  series = {{{NIPS}} '22},
  pages = {6021--6033},
  publisher = {Curran Associates Inc.},
  location = {Red Hook, NY, USA},
  abstract = {Graph Neural Networks (GNNs) are attracting growing attention due to their effectiveness and flexibility in modeling a variety of graph-structured data. Exiting GNN architectures usually adopt simple pooling operations (e.g., sum, average, max) when aggregating messages from a local neighborhood for updating node representation or pooling node representations from the entire graph to compute the graph representation. Though simple and effective, these linear operations do not model high-order non-linear interactions among nodes. We propose the Tensorized Graph Neural Network (tGNN), a highly expressive GNN architecture relying on tensor decomposition to model high-order non-linear node interactions. tGNN leverages the symmetric CP decomposition to efficiently parameterize permutation-invariant multilinear maps for modeling node interactions. Theoretical and empirical analysis on both node and graph classification tasks show the superiority of tGNN over competitive baselines. In particular, tGNN achieves the most solid results on two OGB node classification datasets and one OGB graph classification dataset.},
  eventtitle = {{{NeurIPS}} 2022},
  isbn = {978-1-71387-108-8}
}

@misc{huangCombiningLabelPropagation2020,
  title = {Combining {{Label Propagation}} and {{Simple Models Out-performs Graph Neural Networks}}},
  author = {Huang, Qian and He, Horace and Singh, Abhay and Lim, Ser-Nam and Benson, Austin R.},
  date = {2020-11-02},
  eprint = {2010.13993},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2010.13993},
  urldate = {2024-02-20},
  abstract = {Graph Neural Networks (GNNs) are the predominant technique for learning over graphs. However, there is relatively little understanding of why GNNs are successful in practice and whether they are necessary for good performance. Here, we show that for many standard transductive node classification benchmarks, we can exceed or match the performance of state-of-the-art GNNs by combining shallow models that ignore the graph structure with two simple post-processing steps that exploit correlation in the label structure: (i) an "error correlation" that spreads residual errors in training data to correct errors in test data and (ii) a "prediction correlation" that smooths the predictions on the test data. We call this overall procedure Correct and Smooth (C\&S), and the post-processing steps are implemented via simple modifications to standard label propagation techniques from early graph-based semi-supervised learning methods. Our approach exceeds or nearly matches the performance of state-of-the-art GNNs on a wide variety of benchmarks, with just a small fraction of the parameters and orders of magnitude faster runtime. For instance, we exceed the best known GNN performance on the OGB-Products dataset with 137 times fewer parameters and greater than 100 times less training time. The performance of our methods highlights how directly incorporating label information into the learning algorithm (as was done in traditional techniques) yields easy and substantial performance gains. We can also incorporate our techniques into big GNN models, providing modest gains. Our code for the OGB results is at https://github.com/Chillee/CorrectAndSmooth.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Computer Science - Social and Information Networks},
  file = {/Users/luke/Zotero/storage/FIC5EZDN/Huang et al. - 2020 - Combining Label Propagation and Simple Models Out-.pdf;/Users/luke/Zotero/storage/HWRQEURP/2010.html},
  note = {arXiv:2010.13993}
}

@article{huangGraphLIMELocalInterpretable2023,
  title = {{{GraphLIME}}: {{Local Interpretable Model Explanations}} for {{Graph Neural Networks}}},
  shorttitle = {{{GraphLIME}}},
  author = {Huang, Qiang and Yamada, Makoto and Tian, Yuan and Singh, Dinesh and Chang, Yi},
  date = {2023-07-01},
  journaltitle = {IEEE Transactions on Knowledge and Data Engineering},
  shortjournal = {IEEE Trans. on Knowl. and Data Eng.},
  volume = {35},
  number = {7},
  pages = {6968--6972},
  issn = {1041-4347},
  url = {https://doi.org/10.1109/TKDE.2022.3187455},
  urldate = {2024-03-08},
  abstract = {Recently, graph neural networks (GNN) were shown to be successful in effectively representing graph structured data because of their good performance and generalization ability. However, explaining the effectiveness of GNN models is a challenging task because of the complex nonlinear transformations made over the iterations. In this paper, we propose GraphLIME, a local interpretable model explanation for graphs using the Hilbert-Schmidt Independence Criterion (HSIC) Lasso, which is a nonlinear feature selection method. GraphLIME is a generic GNN-model explanation framework that learns a nonlinear interpretable model locally in the subgraph of the node being explained. Through experiments on two real-world datasets, the explanations of GraphLIME are found to be of extraordinary degree and more descriptive in comparison to the existing explanation methods.},
  file = {/Users/luke/Zotero/storage/SALD27CA/Huang et al. - 2023 - GraphLIME Local Interpretable Model Explanations .pdf}
}

@article{huangSkipGNNPredictingMolecular2020,
  title = {{{SkipGNN}}: predicting molecular interactions with skip-graph networks},
  shorttitle = {{{SkipGNN}}},
  author = {Huang, Kexin and Xiao, Cao and Glass, Lucas M. and Zitnik, Marinka and Sun, Jimeng},
  date = {2020-12-03},
  journaltitle = {Scientific Reports},
  shortjournal = {Sci Rep},
  volume = {10},
  number = {1},
  pages = {21092},
  publisher = {Nature Publishing Group},
  issn = {2045-2322},
  url = {https://www.nature.com/articles/s41598-020-77766-9},
  urldate = {2023-11-03},
  abstract = {Molecular interaction networks are powerful resources for molecular discovery. They are increasingly used with machine learning methods to predict biologically meaningful interactions. While deep learning on graphs has dramatically advanced the prediction prowess, current graph neural network (GNN) methods are mainly optimized for prediction on the basis of direct similarity between interacting nodes. In biological networks, however, similarity between nodes that do not directly interact has proved incredibly useful in the last decade across a variety of interaction networks. Here, we present SkipGNN, a graph neural network approach for the prediction of molecular interactions. SkipGNN predicts molecular interactions by not only aggregating information from direct interactions but also from second-order interactions, which we call skip similarity. In contrast to existing GNNs, SkipGNN receives neural messages from two-hop neighbors as well as immediate neighbors in the interaction network and non-linearly transforms the messages to obtain useful information for prediction. To inject skip similarity into a GNN, we construct a modified version of the original network, called the skip graph. We then develop an iterative fusion scheme that optimizes a GNN using both the skip graph and the original graph. Experiments on four interaction networks, including drug–drug, drug–target, protein–protein, and gene–disease interactions, show that SkipGNN achieves superior and robust performance. Furthermore, we show that unlike popular GNNs, SkipGNN learns biologically meaningful embeddings and performs especially well on noisy, incomplete interaction networks.},
  issue = {1},
  langid = {english},
  keywords = {Biochemical reaction networks,Computational models,Data mining,Machine learning},
  file = {/Users/luke/Zotero/storage/44KVLGYE/Huang et al. - 2020 - SkipGNN predicting molecular interactions with sk.pdf}
}

@article{huaResearch3DMedical2022,
  title = {Research on {{3D}} medical image surface reconstruction based on data mining and machine learning},
  author = {Hua, Shanshan and Liu, Qi and Yin, Guanxiang and Guan, Xiaohui and Jiang, Nan and Zhang, Yuejin},
  date = {2022},
  journaltitle = {International Journal of Intelligent Systems},
  volume = {37},
  number = {8},
  pages = {4654--4669},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/int.22735},
  urldate = {2023-11-18},
  abstract = {Three-dimensional (3D) medical images are prone to overlap, and there are some problems, such as low detection efficiency and inconsistent with the actual situation. Therefore, a 3D medical image surface reconstruction method based on data mining and machine learning is proposed. The 3D medical images were classified according to different ways, the information frame of 3D medical images was established and the surface overlapping information model of 3D images was given. Based on this information framework, the nonlinear function of overlapping area information of 3D medical images was constructed. The weight of the nonlinear function was used to calculate the input and output results of overlapping area information. Combined with the input mode of 3D medical image information, the error between the information output and the expected output was set. The nonlinear function weight of the overlapping area information of 3D medical images was modified by using the learning rate and the use time of the overlapping area information, and the influence factors of the overlapping information detection were obtained by increasing the situation terms, so as to complete the detection of the surface reconstruction information of 3D medical images. The experimental results show that the information detection results of the proposed method fit well with the actual situation, and the information detection efficiency is high.},
  langid = {english},
  keywords = {3D medical images,data mining algorithm,image surface reconstruction,machine learning algorithm},
  file = {/Users/luke/Zotero/storage/LLRZZY42/Hua et al. - 2022 - Research on 3D medical image surface reconstructio.pdf}
}

@inproceedings{huGPTGNNGenerativePreTraining2020,
  title = {{{GPT-GNN}}: {{Generative Pre-Training}} of {{Graph Neural Networks}}},
  shorttitle = {{{GPT-GNN}}},
  booktitle = {Proceedings of the 26th {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} \& {{Data Mining}}},
  author = {Hu, Ziniu and Dong, Yuxiao and Wang, Kuansan and Chang, Kai-Wei and Sun, Yizhou},
  date = {2020-08-20},
  series = {{{KDD}} '20},
  pages = {1857--1867},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  url = {https://dl.acm.org/doi/10.1145/3394486.3403237},
  urldate = {2024-08-26},
  abstract = {Graph neural networks (GNNs) have been demonstrated to be powerful in modeling graph-structured data. However, training GNNs requires abundant task-specific labeled data, which is often arduously expensive to obtain. One effective way to reduce the labeling effort is to pre-train an expressive GNN model on unlabelled data with self-supervision and then transfer the learned model to downstream tasks with only a few labels. In this paper, we present the GPT-GNN framework to initialize GNNs by generative pre-training. GPT-GNN introduces a self-supervised attributed graph generation task to pre-train a GNN so that it can capture the structural and semantic properties of the graph. We factorize the likelihood of graph generation into two components: 1) attribute generation and 2) edge generation. By modeling both components, GPT-GNN captures the inherent dependency between node attributes and graph structure during the generative process. Comprehensive experiments on the billion-scale open academic graph and Amazon recommendation data demonstrate that GPT-GNN significantly outperforms state-of-the-art GNN models without pre-training by up to 9.1\% across various downstream tasks?},
  isbn = {978-1-4503-7998-4},
  file = {/Users/luke/Zotero/storage/RVQ6W4SE/Hu et al. - 2020 - GPT-GNN Generative Pre-Training of Graph Neural N.pdf}
}

@misc{huHeterogeneousGraphTransformer2020,
  title = {Heterogeneous {{Graph Transformer}}},
  author = {Hu, Ziniu and Dong, Yuxiao and Wang, Kuansan and Sun, Yizhou},
  date = {2020-03-02},
  eprint = {2003.01332},
  eprinttype = {arXiv},
  eprintclass = {cs.LG},
  url = {http://arxiv.org/abs/2003.01332},
  urldate = {2023-12-03},
  abstract = {Recent years have witnessed the emerging success of graph neural networks (GNNs) for modeling structured data. However, most GNNs are designed for homogeneous graphs, in which all nodes and edges belong to the same types, making them infeasible to represent heterogeneous structures. In this paper, we present the Heterogeneous Graph Transformer (HGT) architecture for modeling Web-scale heterogeneous graphs. To model heterogeneity, we design node- and edge-type dependent parameters to characterize the heterogeneous attention over each edge, empowering HGT to maintain dedicated representations for different types of nodes and edges. To handle dynamic heterogeneous graphs, we introduce the relative temporal encoding technique into HGT, which is able to capture the dynamic structural dependency with arbitrary durations. To handle Web-scale graph data, we design the heterogeneous mini-batch graph sampling algorithm---HGSampling---for efficient and scalable training. Extensive experiments on the Open Academic Graph of 179 million nodes and 2 billion edges show that the proposed HGT model consistently outperforms all the state-of-the-art GNN baselines by 9\%--21\% on various downstream tasks.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Computer Science - Social and Information Networks,Statistics - Machine Learning},
  file = {/Users/luke/Zotero/storage/DH2GZ5HS/Hu et al. - 2020 - Heterogeneous Graph Transformer.pdf;/Users/luke/Zotero/storage/67YCUN5D/2003.html},
  note = {arXiv:2003.01332}
}

@article{ibayashiSharpnessAwareMinimizationRobust2021,
  title = {Sharpness-{{Aware Minimization}} for {{Robust Molecular Dynamics Simulations}}},
  author = {Ibayashi, Hikaru and Nomura, Ken-ichi and Rajak, Pankaj and Mohammed, Taufeq and Mishra, Ankit and Krishnamoorthy, Aravind and Nakano, Aiichiro},
  date = {2021},
  url = {https://ml4physicalsciences.github.io/2021/files/NeurIPS_ML4PS_2021_31.pdf},
  urldate = {2024-03-17},
  file = {/Users/luke/Zotero/storage/INLPTEDW/Ibayashi et al. - Sharpness-Aware Minimization for Robust Molecular .pdf}
}

@misc{ioffeBatchNormalizationAccelerating2015,
  title = {Batch {{Normalization}}: {{Accelerating Deep Network Training}} by {{Reducing Internal Covariate Shift}}},
  shorttitle = {Batch {{Normalization}}},
  author = {Ioffe, Sergey and Szegedy, Christian},
  date = {2015-03-02},
  eprint = {1502.03167},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1502.03167},
  urldate = {2024-03-14},
  abstract = {Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization. It also acts as a regularizer, in some cases eliminating the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.9\% top-5 validation error (and 4.8\% test error), exceeding the accuracy of human raters.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/luke/Zotero/storage/JQT4TVI3/Ioffe and Szegedy - 2015 - Batch Normalization Accelerating Deep Network Tra.pdf;/Users/luke/Zotero/storage/ESECZ5KE/1502.html},
  note = {arXiv:1502.03167}
}

@inproceedings{jacotNeuralTangentKernel2018,
  title = {Neural {{Tangent Kernel}}: {{Convergence}} and {{Generalization}} in {{Neural Networks}}},
  shorttitle = {Neural {{Tangent Kernel}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Jacot, Arthur and Gabriel, Franck and Hongler, Clement},
  date = {2018},
  volume = {31},
  publisher = {Curran Associates, Inc.},
  url = {https://proceedings.neurips.cc/paper_files/paper/2018/hash/5a4be1fa34e62bb8a6ec6b91d2462f5a-Abstract.html},
  urldate = {2024-02-18},
  abstract = {At initialization, artificial neural networks (ANNs) are equivalent to Gaussian processes in the infinite-width limit, thus connecting them to kernel methods. We prove that the evolution of an ANN during training can also be described by a kernel: during gradient descent on the parameters of an ANN, the network function (which maps input vectors to output vectors) follows the so-called kernel gradient associated with a new object, which we call the Neural Tangent Kernel (NTK). This kernel is central to describe the generalization features of ANNs. While the NTK is random at initialization and varies during training, in the infinite-width limit it converges to an explicit limiting kernel and stays constant during training. This makes it possible to study the training of ANNs in function space instead of parameter space. Convergence of the training can then be related to the positive-definiteness of the limiting NTK.},
  file = {/Users/luke/Zotero/storage/IBXFRNRI/Jacot et al. - 2018 - Neural Tangent Kernel Convergence and Generalizat.pdf}
}

@online{jetpropoulsionlaboratoryMarsClimateOrbiter,
  title = {Mars {{Climate Orbiter}} - {{Mars Missions}} - {{NASA Jet Propulsion Laboratory}}},
  author = {{Jet Propoulsion Laboratory}},
  url = {https://www.jpl.nasa.gov/missions/mars-climate-orbiter},
  urldate = {2023-12-31},
  abstract = {Launch and mission information for NASA's Mars Climate Orbiter, which was unsuccessful due to a navigation error.},
  langid = {american},
  organization = {NASA Jet Propulsion Laboratory (JPL)},
  file = {/Users/luke/Zotero/storage/KIBIIKHN/mars-climate-orbiter.html}
}

@article{jiangCouldGraphNeural2021,
  title = {Could graph neural networks learn better molecular representation for drug discovery? {{A}} comparison study of descriptor-based and graph-based models},
  shorttitle = {Could graph neural networks learn better molecular representation for drug discovery?},
  author = {Jiang, Dejun and Wu, Zhenxing and Hsieh, Chang-Yu and Chen, Guangyong and Liao, Ben and Wang, Zhe and Shen, Chao and Cao, Dongsheng and Wu, Jian and Hou, Tingjun},
  date = {2021-02-17},
  journaltitle = {Journal of Cheminformatics},
  shortjournal = {Journal of Cheminformatics},
  volume = {13},
  number = {1},
  pages = {12},
  issn = {1758-2946},
  url = {https://doi.org/10.1186/s13321-020-00479-8},
  urldate = {2023-11-26},
  abstract = {Graph neural networks (GNN) has been considered as an attractive modelling method for molecular property prediction, and numerous studies have shown that GNN could yield more promising results than traditional descriptor-based methods. In this study, based on 11 public datasets covering various property endpoints, the predictive capacity and computational efficiency of the prediction models developed by eight machine learning (ML) algorithms, including four descriptor-based models (SVM, XGBoost, RF and DNN) and four graph-based models (GCN, GAT, MPNN and Attentive FP), were extensively tested and compared. The results demonstrate that on average the descriptor-based models outperform the graph-based models in terms of prediction accuracy and computational efficiency. SVM generally achieves the best predictions for the regression tasks. Both RF and XGBoost can achieve reliable predictions for the classification tasks, and some of the graph-based models, such as Attentive FP and GCN, can yield outstanding performance for a fraction of larger or multi-task datasets. In terms of computational cost, XGBoost and RF are the two most efficient algorithms and only need a few seconds to train a model even for a large dataset. The model interpretations by the SHAP method can effectively explore the established domain knowledge for the descriptor-based models. Finally, we explored use of these models for virtual screening (VS) towards HIV and demonstrated that different ML algorithms offer diverse VS profiles. All in all, we believe that the off-the-shelf descriptor-based models still can be directly employed to accurately predict various chemical endpoints with excellent computability and interpretability.},
  keywords = {ADME/T prediction,Deep learning,Ensemble learning,Extreme gradient boosting,Graph neural networks},
  file = {/Users/luke/Zotero/storage/M35HKN7T/Jiang et al. - 2021 - Could graph neural networks learn better molecular.pdf;/Users/luke/Zotero/storage/2TAWIIPL/s13321-020-00479-8.html}
}

@book{jipsenVarietiesLattices1992,
  title = {Varieties of {{Lattices}}},
  author = {Jipsen, Peter and Rose, Henry},
  date = {1992},
  series = {Lecture {{Notes}} in {{Mathematics}}},
  volume = {1533},
  publisher = {Springer},
  location = {Berlin, Heidelberg},
  url = {http://link.springer.com/10.1007/BFb0090224},
  urldate = {2024-03-07},
  isbn = {978-3-540-56314-3},
  keywords = {algebra,DEX,diagrams,equation,field,form,history of mathematics,Lattice,Microsoft Access,proof,variety,visualization},
  file = {/Users/luke/Zotero/storage/DB4JK289/Jipsen and Rose - 1992 - Varieties of Lattices.pdf}
}

@article{johnsonMIMICIIIFreelyAccessible2016,
  title = {{{MIMIC-III}}, a freely accessible critical care database},
  author = {Johnson, Alistair E. W. and Pollard, Tom J. and Shen, Lu and Lehman, Li-wei H. and Feng, Mengling and Ghassemi, Mohammad and Moody, Benjamin and Szolovits, Peter and Anthony Celi, Leo and Mark, Roger G.},
  date = {2016-05-24},
  journaltitle = {Scientific Data},
  shortjournal = {Sci Data},
  volume = {3},
  number = {1},
  pages = {160035},
  publisher = {Nature Publishing Group},
  issn = {2052-4463},
  url = {https://www.nature.com/articles/sdata201635},
  urldate = {2024-03-27},
  abstract = {MIMIC-III (‘Medical Information Mart for Intensive Care’) is a large, single-center database comprising information relating to patients admitted to critical care units at a large tertiary care hospital. Data includes vital signs, medications, laboratory measurements, observations and notes charted by care providers, fluid balance, procedure codes, diagnostic codes, imaging reports, hospital length of stay, survival data, and more. The database supports applications including academic and industrial research, quality improvement initiatives, and higher education coursework.},
  langid = {english},
  keywords = {Diagnosis,Health care,Medical research,Outcomes research,Prognosis},
  file = {/Users/luke/Zotero/storage/43PR44JE/Johnson et al. - 2016 - MIMIC-III, a freely accessible critical care datab.pdf}
}

@article{johnsonMIMICIVFreelyAccessible2023,
  title = {{{MIMIC-IV}}, a freely accessible electronic health record dataset},
  author = {Johnson, Alistair E. W. and Bulgarelli, Lucas and Shen, Lu and Gayles, Alvin and Shammout, Ayad and Horng, Steven and Pollard, Tom J. and Hao, Sicheng and Moody, Benjamin and Gow, Brian and Lehman, Li-wei H. and Celi, Leo A. and Mark, Roger G.},
  date = {2023-01-03},
  journaltitle = {Scientific Data},
  shortjournal = {Sci Data},
  volume = {10},
  number = {1},
  pages = {1},
  publisher = {Nature Publishing Group},
  issn = {2052-4463},
  url = {https://www.nature.com/articles/s41597-022-01899-x},
  urldate = {2024-03-27},
  abstract = {Digital data collection during routine clinical practice is now ubiquitous within hospitals. The data contains valuable information on the care of patients and their response to treatments, offering exciting opportunities for research. Typically, data are stored within archival systems that are not intended to support research. These systems are often inaccessible to researchers and structured for optimal storage, rather than interpretability and analysis. Here we present MIMIC-IV, a publicly available database sourced from the electronic health record of the Beth Israel Deaconess Medical Center. Information available includes patient measurements, orders, diagnoses, procedures, treatments, and deidentified free-text clinical notes. MIMIC-IV is intended to support a wide array of research studies and educational material, helping to reduce barriers to conducting clinical research.},
  langid = {english},
  keywords = {Epidemiology,Health services,Public health},
  file = {/Users/luke/Zotero/storage/CR3IPWNP/Johnson et al. - 2023 - MIMIC-IV, a freely accessible electronic health re.pdf}
}

@article{jonssonAlgebrasWhoseCongruence1967,
  title = {Algebras {{Whose Congruence Lattices Are Distributive}}},
  author = {Jónsson, Bjarni},
  date = {1967},
  journaltitle = {Mathematica Scandinavica},
  volume = {21},
  number = {1},
  eprint = {24489650},
  eprinttype = {jstor},
  pages = {110--121},
  publisher = {Mathematica Scandinavica},
  issn = {0025-5521},
  url = {https://www.jstor.org/stable/24489650},
  urldate = {2024-03-08},
  file = {/Users/luke/Zotero/storage/SFPAN6QV/Jónsson - 1967 - Algebras Whose Congruence Lattices Are Distributiv.pdf}
}

@article{jonssonLatticeVarietiesCovering1979,
  title = {Lattice varieties covering the smallest nonmodular variety},
  author = {Jónsson, Bjarni and Rival, Ivan},
  date = {1979-06-01},
  journaltitle = {Pacific Journal of Mathematics},
  shortjournal = {Pacific J. Math.},
  volume = {82},
  number = {2},
  pages = {463--478},
  issn = {0030-8730, 0030-8730},
  url = {http://msp.org/pjm/1979/82-2/p15.xhtml},
  urldate = {2024-03-07},
  abstract = {Semantic Scholar extracted view of "Lattice varieties covering the smallest nonmodular variety." by B. Jónsson et al.},
  langid = {english},
  file = {/Users/luke/Zotero/storage/QECTB6TA/Jónsson and Rival - 1979 - Lattice varieties covering the smallest nonmodular.pdf}
}

@article{jonssonRepresentationLattices1953,
  title = {On the {{Representation}} of {{Lattices}}},
  author = {Jónsson, Bjarni},
  date = {1953},
  journaltitle = {Mathematica Scandinavica},
  volume = {1},
  number = {2},
  pages = {193--206},
  publisher = {Mathematica Scandinavica},
  issn = {0025-5521},
  abstract = {Whitman has shown that every lattice A is isomorphic to a lattice 𝒜 of equivalence relations. If for every R and S in 𝒜 the relative product R; S; R; S;... with n+1 factors is an equivalence relation, and hence equal to the lattice sum of R and S, then we speak of a representation of A of type n. Results: 1° Every lattice A has a representation of type 3. 2° In order for A to have a representation of type 2, it is necessary and sufficient that A be modular. 3° There exist modular lattices which do not have a representation of type 1 (a representation by commuting equivalence relations).},
  file = {/Users/luke/Zotero/storage/DLBTJPPT/Jónsson - 1953 - On the Representation of Lattices.pdf}
}

@article{jostHypergraphLaplaceOperators2019,
  title = {Hypergraph {{Laplace}} operators for chemical reaction networks},
  author = {Jost, Jürgen and Mulas, Raffaella},
  date = {2019-07-31},
  journaltitle = {Advances in Mathematics},
  shortjournal = {Advances in Mathematics},
  volume = {351},
  pages = {870--896},
  issn = {0001-8708},
  url = {https://www.sciencedirect.com/science/article/pii/S0001870819302671},
  urldate = {2024-05-17},
  abstract = {We generalize the normalized combinatorial Laplace operator for graphs by defining two Laplace operators for hypergraphs that can be useful in the study of chemical reaction networks. We also investigate some properties of their spectra.},
  keywords = {Eigenvalues,Hypergraphs,Laplace operator,Spectral theory},
  file = {/Users/luke/Zotero/storage/F45J3BSY/Jost and Mulas - 2019 - Hypergraph Laplace operators for chemical reaction.pdf}
}

@article{kanehisaKEGGKyotoEncyclopedia2000,
  title = {{{KEGG}}: {{Kyoto Encyclopedia}} of {{Genes}} and {{Genomes}}},
  shorttitle = {{{KEGG}}},
  author = {Kanehisa, Minoru and Goto, Susumu},
  date = {2000-01-01},
  journaltitle = {Nucleic Acids Research},
  shortjournal = {Nucleic Acids Res},
  volume = {28},
  number = {1},
  eprint = {10592173},
  eprinttype = {pmid},
  pages = {27--30},
  issn = {0305-1048},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC102409/},
  urldate = {2024-05-14},
  abstract = {KEGG (Kyoto Encyclopedia of Genes and Genomes) is a knowledge base for systematic analysis of gene functions, linking genomic information with higher order functional information. The genomic information is stored in the GENES database, which is a collection of gene catalogs for all the completely sequenced genomes and some partial genomes with up-to-date annotation of gene functions. The higher order functional information is stored in the PATHWAY database, which contains graphical representations of cellular processes, such as metabolism, membrane transport, signal transduction and cell cycle. The PATHWAY database is supplemented by a set of ortholog group tables for the information about conserved subpathways (pathway motifs), which are often encoded by positionally coupled genes on the chromosome and which are especially useful in predicting gene functions. A third database in KEGG is LIGAND for the information about chemical compounds, enzyme molecules and enzymatic reactions. KEGG provides Java graphics tools for browsing genome maps, comparing two genome maps and manipulating expression maps, as well as computational tools for sequence comparison, graph comparison and path computation. The KEGG databases are daily updated and made freely available (http://www.genome.ad.jp/kegg/ ).},
  pmcid = {PMC102409},
  file = {/Users/luke/Zotero/storage/258L33HS/Kanehisa and Goto - 2000 - KEGG Kyoto Encyclopedia of Genes and Genomes.pdf}
}

@misc{kazemiRepresentationLearningDynamic2020,
  title = {Representation {{Learning}} for {{Dynamic Graphs}}: {{A Survey}}},
  shorttitle = {Representation {{Learning}} for {{Dynamic Graphs}}},
  author = {Kazemi, Seyed Mehran and Goel, Rishab and Jain, Kshitij and Kobyzev, Ivan and Sethi, Akshay and Forsyth, Peter and Poupart, Pascal},
  date = {2020-04-27},
  eprint = {1905.11485},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1905.11485},
  urldate = {2024-02-26},
  abstract = {Graphs arise naturally in many real-world applications including social networks, recommender systems, ontologies, biology, and computational finance. Traditionally, machine learning models for graphs have been mostly designed for static graphs. However, many applications involve evolving graphs. This introduces important challenges for learning and inference since nodes, attributes, and edges change over time. In this survey, we review the recent advances in representation learning for dynamic graphs, including dynamic knowledge graphs. We describe existing models from an encoder-decoder perspective, categorize these encoders and decoders based on the techniques they employ, and analyze the approaches in each category. We also review several prominent applications and widely used datasets and highlight directions for future research.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/luke/Zotero/storage/W3JL3WVU/Kazemi et al. - 2020 - Representation Learning for Dynamic Graphs A Surv.pdf;/Users/luke/Zotero/storage/3METRQ8H/1905.html},
  note = {arXiv:1905.11485}
}

@article{kimDatasetsTasksTraining2023,
  title = {Datasets, tasks, and training methods for large-scale hypergraph learning},
  author = {Kim, Sunwoo and Lee, Dongjin and Kim, Yul and Park, Jungho and Hwang, Taeho and Shin, Kijung},
  date = {2023-11-01},
  journaltitle = {Data Mining and Knowledge Discovery},
  shortjournal = {Data Min Knowl Disc},
  volume = {37},
  number = {6},
  pages = {2216--2254},
  issn = {1573-756X},
  url = {https://doi.org/10.1007/s10618-023-00952-6},
  urldate = {2024-03-24},
  abstract = {Relations among multiple entities are prevalent in many fields, and hypergraphs are widely used to represent such group relations. Hence, machine learning on hypergraphs has received considerable attention, and especially much effort has been made in neural network architectures for hypergraphs (a.k.a., hypergraph neural networks). However, existing studies mostly focused on small datasets for a few single-entity-level downstream tasks and overlooked scalability issues, although most real-world group relations are large-scale. In this work, we propose new tasks, datasets, and scalable training methods for addressing these limitations. First, we introduce two pair-level hypergraph-learning tasks to formulate a wide range of real-world problems. Then, we build and publicly release two large-scale hypergraph datasets with tens of millions of nodes, rich features, and labels. After that, we propose PCL, a scalable learning method for hypergraph neural networks. To tackle scalability issues, PCL splits a given hypergraph into partitions and trains a neural network via contrastive learning. Our extensive experiments demonstrate that hypergraph neural networks can be trained for large-scale hypergraphs by PCL while outperforming 16 baseline models. Specifically, the performance is comparable, or surprisingly even better than that achieved by training hypergraph neural networks on the entire hypergraphs without partitioning.},
  langid = {english},
  keywords = {Contrastive learning,Hypergraph neural networks,Large-scale hypergraph datasets,Partitioning,Scalable hypergraph learning},
  file = {/Users/luke/Zotero/storage/PGW4CMVY/Kim et al. - 2023 - Datasets, tasks, and training methods for large-sc.pdf}
}

@inproceedings{kimHypergraphAttentionNetworks2020,
  title = {Hypergraph {{Attention Networks}} for {{Multimodal Learning}}},
  author = {Kim, Eun-Sol and Kang, Woo Young and On, Kyoung-Woon and Heo, Yu-Jung and Zhang, Byoung-Tak},
  date = {2020},
  pages = {14581--14590},
  url = {https://openaccess.thecvf.com/content_CVPR_2020/html/Kim_Hypergraph_Attention_Networks_for_Multimodal_Learning_CVPR_2020_paper.html},
  urldate = {2023-11-04},
  eventtitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  file = {/Users/luke/Zotero/storage/VHUFXBJY/Kim et al. - 2020 - Hypergraph Attention Networks for Multimodal Learn.pdf}
}

@inproceedings{kingmaAutoEncodingVariationalBayes2013,
  title = {Auto-{{Encoding Variational Bayes}}},
  author = {Kingma, Diederik P. and Welling, Max},
  date = {2013-12-23},
  url = {https://openreview.net/forum?id=33X9fd2-9FyZd},
  urldate = {2024-04-20},
  abstract = {Can we efficiently learn the parameters of directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions? We introduce an unsupervised on-line learning method that efficiently optimizes the variational lower bound on the marginal likelihood and that, under some mild conditions, even works in the intractable case. The method optimizes a probabilistic encoder (also called a recognition network) to approximate the intractable posterior distribution of the latent variables. The crucial element is a reparameterization of the variational bound with an independent noise variable, yielding a stochastic objective function which can be jointly optimized w.r.t. variational and generative parameters using standard gradient-based stochastic optimization methods. Theoretical advantages are reflected in experimental results.},
  eventtitle = {{{ICLR}} 2014},
  langid = {english}
}

@inproceedings{kipfNeuralRelationalInference2018,
  title = {Neural {{Relational Inference}} for {{Interacting Systems}}},
  booktitle = {Proceedings of the 35th {{International Conference}} on {{Machine Learning}}},
  author = {Kipf, Thomas and Fetaya, Ethan and Wang, Kuan-Chieh and Welling, Max and Zemel, Richard},
  date = {2018-07-03},
  pages = {2688--2697},
  publisher = {PMLR},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v80/kipf18a.html},
  urldate = {2024-04-21},
  abstract = {Interacting systems are prevalent in nature, from dynamical systems in physics to complex societal dynamics. The interplay of components can give rise to complex behavior, which can often be explained using a simple model of the system’s constituent parts. In this work, we introduce the neural relational inference (NRI) model: an unsupervised model that learns to infer interactions while simultaneously learning the dynamics purely from observational data. Our model takes the form of a variational auto-encoder, in which the latent code represents the underlying interaction graph and the reconstruction is based on graph neural networks. In experiments on simulated physical systems, we show that our NRI model can accurately recover ground-truth interactions in an unsupervised manner. We further demonstrate that we can find an interpretable structure and predict complex dynamics in real motion capture and sports tracking data.},
  eventtitle = {{{ICML}} 2018},
  langid = {english},
  file = {/Users/luke/Zotero/storage/VU525JJK/Kipf et al. - 2018 - Neural Relational Inference for Interacting System.pdf}
}

@inproceedings{kipfSemiSupervisedClassificationGraph2016,
  title = {Semi-{{Supervised Classification}} with {{Graph Convolutional Networks}}},
  author = {Kipf, Thomas N. and Welling, Max},
  date = {2016-11-03},
  url = {https://openreview.net/forum?id=SJU4ayYgl},
  urldate = {2023-10-11},
  abstract = {We present a scalable approach for semi-supervised learning on graph-structured data that is based on an efficient variant of convolutional neural networks which operate directly on graphs. We motivate the choice of our convolutional architecture via a localized first-order approximation of spectral graph convolutions. Our model scales linearly in the number of graph edges and learns hidden layer representations that encode both local graph structure and features of nodes. In a number of experiments on citation networks and on a knowledge graph dataset we demonstrate that our approach outperforms related methods by a significant margin.},
  eventtitle = {{{ICLR}} 2016},
  langid = {english},
  file = {/Users/luke/Zotero/storage/7GXRENV6/Kipf and Welling - 2016 - Semi-Supervised Classification with Graph Convolut.pdf}
}

@misc{kipfVariationalGraphAutoEncoders2016,
  title = {Variational {{Graph Auto-Encoders}}},
  author = {Kipf, Thomas N. and Welling, Max},
  date = {2016-11-21},
  eprint = {1611.07308},
  eprinttype = {arXiv},
  eprintclass = {stat.ML},
  url = {http://arxiv.org/abs/1611.07308},
  urldate = {2024-04-18},
  abstract = {We introduce the variational graph auto-encoder (VGAE), a framework for unsupervised learning on graph-structured data based on the variational auto-encoder (VAE). This model makes use of latent variables and is capable of learning interpretable latent representations for undirected graphs. We demonstrate this model using a graph convolutional network (GCN) encoder and a simple inner product decoder. Our model achieves competitive results on a link prediction task in citation networks. In contrast to most existing models for unsupervised learning on graph-structured data and link prediction, our model can naturally incorporate node features, which significantly improves predictive performance on a number of benchmark datasets.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/luke/Zotero/storage/4GV5GYYB/Kipf and Welling - 2016 - Variational Graph Auto-Encoders.pdf;/Users/luke/Zotero/storage/P3FY6IRI/1611.html},
  note = {arXiv:1611.07308}
}

@article{klamtHypergraphsCellularNetworks2009,
  title = {Hypergraphs and {{Cellular Networks}}},
  author = {Klamt, Steffen and Haus, Utz-Uwe and Theis, Fabian},
  date = {2009-05-29},
  journaltitle = {PLOS Computational Biology},
  shortjournal = {PLOS Computational Biology},
  volume = {5},
  number = {5},
  pages = {e1000385},
  publisher = {Public Library of Science},
  issn = {1553-7358},
  url = {https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1000385},
  urldate = {2024-03-19},
  langid = {english},
  keywords = {Body weight,Clustering coefficients,Computational biology,Metabolic networks,Network analysis,Protein complexes,Protein interaction networks,Stoichiometry},
  file = {/Users/luke/Zotero/storage/E59HHZLJ/Klamt et al. - 2009 - Hypergraphs and Cellular Networks.pdf}
}

@article{knoxDrugBankDrugBankKnowledgebase2024,
  title = {{{DrugBank}} 6.0: the {{DrugBank Knowledgebase}} for 2024},
  shorttitle = {{{DrugBank}} 6.0},
  author = {Knox, Craig and Wilson, Mike and Klinger, Christen~M and Franklin, Mark and Oler, Eponine and Wilson, Alex and Pon, Allison and Cox, Jordan and Chin, Na Eun~(Lucy) and Strawbridge, Seth~A and Garcia-Patino, Marysol and Kruger, Ray and Sivakumaran, Aadhavya and Sanford, Selena and Doshi, Rahil and Khetarpal, Nitya and Fatokun, Omolola and Doucet, Daphnee and Zubkowski, Ashley and Rayat, Dorsa~Yahya and Jackson, Hayley and Harford, Karxena and Anjum, Afia and Zakir, Mahi and Wang, Fei and Tian, Siyang and Lee, Brian and Liigand, Jaanus and Peters, Harrison and Wang, Ruo Qi~(Rachel) and Nguyen, Tue and So, Denise and Sharp, Matthew and {da~Silva}, Rodolfo and Gabriel, Cyrella and Scantlebury, Joshua and Jasinski, Marissa and Ackerman, David and Jewison, Timothy and Sajed, Tanvir and Gautam, Vasuk and Wishart, David~S},
  date = {2024-01-05},
  journaltitle = {Nucleic Acids Research},
  volume = {52},
  number = {D1},
  pages = {D1265-D1275},
  issn = {0305-1048, 1362-4962},
  url = {https://academic.oup.com/nar/article/52/D1/D1265/7416367},
  urldate = {2024-05-14},
  abstract = {Abstract             First released in 2006, DrugBank (https://go.drugbank.com) has grown to become the ‘gold standard’ knowledge resource for drug, drug–target and related pharmaceutical information. DrugBank is widely used across many diverse biomedical research and clinical applications, and averages more than 30 million views/year. Since its last update in 2018, we have been actively enhancing the quantity and quality of the drug data in this knowledgebase. In this latest release (DrugBank 6.0), the number of FDA approved drugs has grown from 2646 to 4563 (a 72\% increase), the number of investigational drugs has grown from 3394 to 6231 (a 38\% increase), the number of drug–drug interactions increased from 365 984 to 1 413 413 (a 300\% increase), and the number of drug–food interactions expanded from 1195 to 2475 (a 200\% increase). In addition to this notable expansion in database size, we have added thousands of new, colorful, richly annotated pathways depicting drug mechanisms and drug metabolism. Likewise, existing datasets have been significantly improved and expanded, by adding more information on drug indications, drug–drug interactions, drug–food interactions and many other relevant data types for 11~891 drugs. We have also added experimental and predicted MS/MS spectra, 1D/2D-NMR spectra, CCS (collision cross section), RT (retention time)~and RI (retention index) data for 9464 of DrugBank's 11~710 small molecule drugs. These and other improvements should make DrugBank 6.0 even more useful to a much wider research audience ranging from medicinal chemists to metabolomics specialists to pharmacologists.},
  langid = {english},
  file = {/Users/luke/Zotero/storage/XYISYH5C/Knox et al. - 2024 - DrugBank 6.0 the DrugBank Knowledgebase for 2024.pdf}
}

@inproceedings{kulenovicSurveyStaticCode2014,
  title = {A survey of static code analysis methods for security vulnerabilities detection},
  booktitle = {2014 37th {{International Convention}} on {{Information}} and {{Communication Technology}}, {{Electronics}} and {{Microelectronics}} ({{MIPRO}})},
  author = {Kulenovic, Melina and Donko, Dzenana},
  date = {2014-05},
  pages = {1381--1386},
  url = {https://ieeexplore.ieee.org/document/6859783},
  urldate = {2023-12-18},
  abstract = {Software security is becoming highly important for universal acceptance of applications for many kinds of transactions. Automated code analyzers can be utilized to detect security vulnerabilities during the development phase. This paper is aimed to provide a survey on Static code analysis and how it can be used to detect security vulnerabilities. The most recent findings and publications are summarized and presented in this paper. This paper provides an overview of the gains, flows and algorithms of static code analyzers. It can be considered a stepping stone for further research in this domain.},
  eventtitle = {2014 37th {{International Convention}} on {{Information}} and {{Communication Technology}}, {{Electronics}} and {{Microelectronics}} ({{MIPRO}})},
  file = {/Users/luke/Zotero/storage/MFFRGY62/6859783.html}
}

@inproceedings{l.shiTwoStreamAdaptiveGraph2019,
  title = {Two-{{Stream Adaptive Graph Convolutional Networks}} for {{Skeleton-Based Action Recognition}}},
  booktitle = {2019 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {{L. Shi} and {Y. Zhang} and {J. Cheng} and {H. Lu}},
  date = {2019-06-15/2019-06-20},
  pages = {12018--12027},
  url = {http://doi.ieeecomputersociety.org/10.1109/CVPR.2019.01230},
  abstract = {In skeleton-based action recognition, graph convolutional networks (GCNs), which model the human body skeletons as spatiotemporal graphs, have achieved remarkable performance. However, in existing GCN-based methods, the topology of the graph is set manually, and it is fixed over all layers and input samples. This may not be optimal for the hierarchical GCN and diverse samples in action recognition tasks. In addition, the second-order information (the lengths and directions of bones) of the skeleton data, which is naturally more informative and discriminative for action recognition, is rarely investigated in existing methods. In this work, we propose a novel two-stream adaptive graph convolutional network (2s-AGCN) for skeleton-based action recognition. The topology of the graph in our model can be either uniformly or individually learned by the BP algorithm in an end-to-end manner. This data-driven method increases the flexibility of the model for graph construction and brings more generality to adapt to various data samples. Moreover, a two-stream framework is proposed to model both the first-order and the second-order information simultaneously, which shows notable improvement for the recognition accuracy. Extensive experiments on the two large-scale datasets, NTU-RGBD and Kinetics-Skeleton, demonstrate that the performance of our model exceeds the state-of-the-art with a significant margin.},
  eventtitle = {2019 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  keywords = {Action Recognition,Deep Learning}
}

@article{lamLearningSkillfulMediumrange2023,
  title = {Learning skillful medium-range global weather forecasting},
  author = {Lam, Remi and Sanchez-Gonzalez, Alvaro and Willson, Matthew and Wirnsberger, Peter and Fortunato, Meire and Alet, Ferran and Ravuri, Suman and Ewalds, Timo and Eaton-Rosen, Zach and Hu, Weihua and Merose, Alexander and Hoyer, Stephan and Holland, George and Vinyals, Oriol and Stott, Jacklynn and Pritzel, Alexander and Mohamed, Shakir and Battaglia, Peter},
  date = {2023-11-14},
  journaltitle = {Science},
  publisher = {American Association for the Advancement of Science},
  url = {https://www.science.org/doi/10.1126/science.adi2336},
  urldate = {2023-11-23},
  abstract = {Global medium-range weather forecasting is critical to decision-making across many social and economic domains. Traditional numerical weather prediction uses increased compute resources to improve forecast accuracy, but does not directly use historical ...},
  langid = {english},
  file = {/Users/luke/Zotero/storage/S4BPL786/full.html}
}

@misc{lampleDeepLearningSymbolic2019,
  title = {Deep {{Learning}} for {{Symbolic Mathematics}}},
  author = {Lample, Guillaume and Charton, François},
  date = {2019-12-02},
  eprint = {1912.01412},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1912.01412},
  urldate = {2024-03-13},
  abstract = {Neural networks have a reputation for being better at solving statistical or approximate problems than at performing calculations or working with symbolic data. In this paper, we show that they can be surprisingly good at more elaborated tasks in mathematics, such as symbolic integration and solving differential equations. We propose a syntax for representing mathematical problems, and methods for generating large datasets that can be used to train sequence-to-sequence models. We achieve results that outperform commercial Computer Algebra Systems such as Matlab or Mathematica.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Computer Science - Symbolic Computation},
  file = {/Users/luke/Zotero/storage/RY4JTNFY/Lample and Charton - 2019 - Deep Learning for Symbolic Mathematics.pdf;/Users/luke/Zotero/storage/L4PQS4BB/1912.html},
  note = {arXiv:1912.01412}
}

@misc{lanKnowledgeDistillationOntheFly2018,
  title = {Knowledge {{Distillation}} by {{On-the-Fly Native Ensemble}}},
  author = {Lan, Xu and Zhu, Xiatian and Gong, Shaogang},
  date = {2018-09-08},
  eprint = {1806.04606},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1806.04606},
  urldate = {2023-11-22},
  abstract = {Knowledge distillation is effective to train small and generalisable network models for meeting the low-memory and fast running requirements. Existing offline distillation methods rely on a strong pre-trained teacher, which enables favourable knowledge discovery and transfer but requires a complex two-phase training procedure. Online counterparts address this limitation at the price of lacking a highcapacity teacher. In this work, we present an On-the-fly Native Ensemble (ONE) strategy for one-stage online distillation. Specifically, ONE trains only a single multi-branch network while simultaneously establishing a strong teacher on-the- fly to enhance the learning of target network. Extensive evaluations show that ONE improves the generalisation performance a variety of deep neural networks more significantly than alternative methods on four image classification dataset: CIFAR10, CIFAR100, SVHN, and ImageNet, whilst having the computational efficiency advantages.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/luke/Zotero/storage/HQAA9E4Y/Lan et al. - 2018 - Knowledge Distillation by On-the-Fly Native Ensemb.pdf;/Users/luke/Zotero/storage/7TTFTDZ8/1806.html},
  note = {arXiv:1806.04606}
}

@inproceedings{leeSetTransformerFramework2019,
  title = {Set {{Transformer}}: {{A Framework}} for {{Attention-based Permutation-Invariant Neural Networks}}},
  shorttitle = {Set {{Transformer}}},
  booktitle = {Proceedings of the 36th {{International Conference}} on {{Machine Learning}}},
  author = {Lee, Juho and Lee, Yoonho and Kim, Jungtaek and Kosiorek, Adam and Choi, Seungjin and Teh, Yee Whye},
  date = {2019-05-24},
  pages = {3744--3753},
  publisher = {PMLR},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v97/lee19d.html},
  urldate = {2024-05-08},
  abstract = {Many machine learning tasks such as multiple instance learning, 3D shape recognition, and few-shot image classification are defined on sets of instances. Since solutions to such problems do not depend on the order of elements of the set, models used to address them should be permutation invariant. We present an attention-based neural network module, the Set Transformer, specifically designed to model interactions among elements in the input set. The model consists of an encoder and a decoder, both of which rely on attention mechanisms. In an effort to reduce computational complexity, we introduce an attention scheme inspired by inducing point methods from sparse Gaussian process literature. It reduces the computation time of self-attention from quadratic to linear in the number of elements in the set. We show that our model is theoretically attractive and we evaluate it on a range of tasks, demonstrating the state-of-the-art performance compared to recent methods for set-structured data.},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english},
  file = {/Users/luke/Zotero/storage/NCXRXRBK/Lee et al. - 2019 - Set Transformer A Framework for Attention-based P.pdf}
}

@inproceedings{liDeepLearningCasebased2018,
  title = {Deep learning for case-based reasoning through prototypes: a neural network that explains its predictions},
  shorttitle = {Deep learning for case-based reasoning through prototypes},
  booktitle = {Proceedings of the {{Thirty-Second AAAI Conference}} on {{Artificial Intelligence}} and {{Thirtieth Innovative Applications}} of {{Artificial Intelligence Conference}} and {{Eighth AAAI Symposium}} on {{Educational Advances}} in {{Artificial Intelligence}}},
  author = {Li, Oscar and Liu, Hao and Chen, Chaofan and Rudin, Cynthia},
  date = {2018-02-02},
  series = {{{AAAI}}'18/{{IAAI}}'18/{{EAAI}}'18},
  pages = {3530--3537},
  publisher = {AAAI Press},
  location = {New Orleans, Louisiana, USA},
  abstract = {Deep neural networks are widely used for classification. These deep models often suffer from a lack of interpretability - they are particularly difficult to understand because of their non-linear nature. As a result, neural networks are often treated as "black box" models, and in the past, have been trained purely to optimize the accuracy of predictions. In this work, we create a novel network architecture for deep learning that naturally explains its own reasoning for each prediction. This architecture contains an autoencoder and a special prototype layer, where each unit of that layer stores a weight vector that resembles an encoded training input. The encoder of the autoencoder allows us to do comparisons within the latent space, while the decoder allows us to visualize the learned prototypes. The training objective has four terms: an accuracy term, a term that encourages every prototype to be similar to at least one encoded input, a term that encourages every encoded input to be close to at least one prototype, and a term that encourages faithful reconstruction by the autoen-coder. The distances computed in the prototype layer are used as part of the classification process. Since the prototypes are learned during training, the learned network naturally comes with explanations for each prediction, and the explanations are loyal to what the network actually computes.},
  isbn = {978-1-57735-800-8},
  file = {/Users/luke/Zotero/storage/4SXXPHRQ/Li et al. - 2018 - Deep learning for case-based reasoning through pro.pdf}
}

@misc{liDiffusionConvolutionalRecurrent2018,
  title = {Diffusion {{Convolutional Recurrent Neural Network}}: {{Data-Driven Traffic Forecasting}}},
  shorttitle = {Diffusion {{Convolutional Recurrent Neural Network}}},
  author = {Li, Yaguang and Yu, Rose and Shahabi, Cyrus and Liu, Yan},
  date = {2018-02-22},
  eprint = {1707.01926},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1707.01926},
  urldate = {2024-02-26},
  abstract = {Spatiotemporal forecasting has various applications in neuroscience, climate and transportation domain. Traffic forecasting is one canonical example of such learning task. The task is challenging due to (1) complex spatial dependency on road networks, (2) non-linear temporal dynamics with changing road conditions and (3) inherent difficulty of long-term forecasting. To address these challenges, we propose to model the traffic flow as a diffusion process on a directed graph and introduce Diffusion Convolutional Recurrent Neural Network (DCRNN), a deep learning framework for traffic forecasting that incorporates both spatial and temporal dependency in the traffic flow. Specifically, DCRNN captures the spatial dependency using bidirectional random walks on the graph, and the temporal dependency using the encoder-decoder architecture with scheduled sampling. We evaluate the framework on two real-world large scale road network traffic datasets and observe consistent improvement of 12\% - 15\% over state-of-the-art baselines.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/luke/Zotero/storage/PNFPZ54N/Li et al. - 2018 - Diffusion Convolutional Recurrent Neural Network .pdf;/Users/luke/Zotero/storage/K7M4SWVL/1707.html},
  note = {arXiv:1707.01926}
}

@inproceedings{liDLFixContextbasedCode2020,
  title = {{{DLFix}}: {{Context-based Code Transformation Learning}} for {{Automated Program Repair}}},
  shorttitle = {{{DLFix}}},
  booktitle = {2020 {{IEEE}}/{{ACM}} 42nd {{International Conference}} on {{Software Engineering}} ({{ICSE}})},
  author = {Li, Yi and Wang, Shaohua and Nguyen, Tien N.},
  date = {2020-10},
  pages = {602--614},
  issn = {1558-1225},
  url = {https://ieeexplore.ieee.org/document/9284100},
  urldate = {2023-12-19},
  abstract = {Automated Program Repair (APR) is very useful in helping developers in the process of software development and maintenance. Despite recent advances in deep learning (DL), the DL-based APR approaches still have limitations in learning bug-fixing code changes and the context of the surrounding source code of the bug-fixing code changes. These limitations lead to incorrect fixing locations or fixes. In this paper, we introduce DLFix, a two-tier DL model that treats APR as code transformation learning from the prior bug fixes and the surrounding code contexts of the fixes. The first layer is a tree-based RNN model that learns the contexts of bug fixes and its result is used as an additional weighting input for the second layer designed to learn the bug-fixing code transformations. We conducted several experiments to evaluate DLFix in two benchmarks: Defect4J and Bugs.jar, and a newly built bug datasets with a total of +20K real-world bugs in eight projects. We compared DLFix against a total of 13 state-of-the-art pattern-based APR tools. Our results show that DLFix can auto-fix more bugs than 11 of them, and is comparable and complementary to the top two pattern-based APR tools in which there are 7 and 11 unique bugs that they cannot detect, respectively, but we can. Importantly, DLFix is fully automated and data-driven, and does not require hard-coding of bug-fixing patterns as in those tools. We compared DLFix against 4 state-of-the-art deep learning based APR models. DLFix is able to fix 2.5 times more bugs than the best performing baseline.},
  eventtitle = {2020 {{IEEE}}/{{ACM}} 42nd {{International Conference}} on {{Software Engineering}} ({{ICSE}})},
  file = {/Users/luke/Zotero/storage/BMSIDAL3/9284100.html}
}

@inproceedings{liEncodingSocialInformation2019,
  title = {Encoding {{Social Information}} with {{Graph Convolutional Networks forPolitical Perspective Detection}} in {{News Media}}},
  booktitle = {Proceedings of the 57th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Li, Chang and Goldwasser, Dan},
  editor = {Korhonen, Anna and Traum, David and Màrquez, Lluís},
  date = {2019-07},
  pages = {2594--2604},
  publisher = {Association for Computational Linguistics},
  location = {Florence, Italy},
  url = {https://aclanthology.org/P19-1247},
  urldate = {2024-08-26},
  abstract = {Identifying the political perspective shaping the way news events are discussed in the media is an important and challenging task. In this paper, we highlight the importance of contextualizing social information, capturing how this information is disseminated in social networks. We use Graph Convolutional Networks, a recently proposed neural architecture for representing relational information, to capture the documents' social context. We show that social information can be used effectively as a source of distant supervision, and when direct supervision is available, even little social information can significantly improve performance.},
  eventtitle = {{{ACL}} 2019},
  file = {/Users/luke/Zotero/storage/JYDBHAX5/Li and Goldwasser - 2019 - Encoding Social Information with Graph Convolution.pdf}
}

@misc{liGatedGraphSequence2017,
  title = {Gated {{Graph Sequence Neural Networks}}},
  author = {Li, Yujia and Tarlow, Daniel and Brockschmidt, Marc and Zemel, Richard},
  date = {2017-09-22},
  eprint = {1511.05493},
  eprinttype = {arXiv},
  eprintclass = {cs.LG},
  url = {http://arxiv.org/abs/1511.05493},
  urldate = {2023-12-20},
  abstract = {Graph-structured data appears frequently in domains including chemistry, natural language semantics, social networks, and knowledge bases. In this work, we study feature learning techniques for graph-structured inputs. Our starting point is previous work on Graph Neural Networks (Scarselli et al., 2009), which we modify to use gated recurrent units and modern optimization techniques and then extend to output sequences. The result is a flexible and broadly useful class of neural network models that has favorable inductive biases relative to purely sequence-based models (e.g., LSTMs) when the problem is graph-structured. We demonstrate the capabilities on some simple AI (bAbI) and graph algorithm learning tasks. We then show it achieves state-of-the-art performance on a problem from program verification, in which subgraphs need to be matched to abstract data structures.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/Users/luke/Zotero/storage/KEAPH8YW/Li et al. - 2017 - Gated Graph Sequence Neural Networks.pdf;/Users/luke/Zotero/storage/JFT4L9JS/1511.html},
  note = {arXiv:1511.05493}
}

@article{liHyperbandNovelBanditbased2017,
  title = {Hyperband: a novel bandit-based approach to hyperparameter optimization},
  shorttitle = {Hyperband},
  author = {Li, Lisha and Jamieson, Kevin and DeSalvo, Giulia and Rostamizadeh, Afshin and Talwalkar, Ameet},
  date = {2017-01-01},
  journaltitle = {The Journal of Machine Learning Research},
  shortjournal = {J. Mach. Learn. Res.},
  volume = {18},
  number = {1},
  pages = {6765--6816},
  issn = {1532-4435},
  abstract = {Performance of machine learning algorithms depends critically on identifying a good set of hyperparameters. While recent approaches use Bayesian optimization to adaptively select configurations, we focus on speeding up random search through adaptive resource allocation and early-stopping. We formulate hyperparameter optimization as a pure-exploration nonstochastic infinite-armed bandit problem where a predefined resource like iterations, data samples, or features is allocated to randomly sampled configurations. We introduce a novel algorithm, Hyperband, for this framework and analyze its theoretical properties, providing several desirable guarantees. Furthermore, we compare Hyperband with popular Bayesian optimization methods on a suite of hyperparameter optimization problems. We observe that Hyperband can provide over an order-of-magnitude speedup over our competitor set on a variety of deep-learning and kernel-based learning problems.},
  keywords = {deep learning,hyperparameter optimization,infinite-armed bandits,model selection,online optimization},
  file = {/Users/luke/Zotero/storage/77ESXPXG/Li et al. - 2017 - Hyperband a novel bandit-based approach to hyperp.pdf}
}

@misc{linFocalLossDense2018,
  title = {Focal {{Loss}} for {{Dense Object Detection}}},
  author = {Lin, Tsung-Yi and Goyal, Priya and Girshick, Ross and He, Kaiming and Dollár, Piotr},
  date = {2018-02-07},
  eprint = {1708.02002},
  eprinttype = {arXiv},
  eprintclass = {cs.CV},
  url = {http://arxiv.org/abs/1708.02002},
  urldate = {2024-04-19},
  abstract = {The highest accuracy object detectors to date are based on a two-stage approach popularized by R-CNN, where a classifier is applied to a sparse set of candidate object locations. In contrast, one-stage detectors that are applied over a regular, dense sampling of possible object locations have the potential to be faster and simpler, but have trailed the accuracy of two-stage detectors thus far. In this paper, we investigate why this is the case. We discover that the extreme foreground-background class imbalance encountered during training of dense detectors is the central cause. We propose to address this class imbalance by reshaping the standard cross entropy loss such that it down-weights the loss assigned to well-classified examples. Our novel Focal Loss focuses training on a sparse set of hard examples and prevents the vast number of easy negatives from overwhelming the detector during training. To evaluate the effectiveness of our loss, we design and train a simple dense detector we call RetinaNet. Our results show that when trained with the focal loss, RetinaNet is able to match the speed of previous one-stage detectors while surpassing the accuracy of all existing state-of-the-art two-stage detectors. Code is at: https://github.com/facebookresearch/Detectron.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/luke/Zotero/storage/M2L74LES/Lin et al. - 2018 - Focal Loss for Dense Object Detection.pdf;/Users/luke/Zotero/storage/VJQYDCU3/1708.html},
  note = {arXiv:1708.02002}
}

@inproceedings{liPredictingPathFailure2019,
  title = {Predicting {{Path Failure In Time-Evolving Graphs}}},
  booktitle = {Proceedings of the 25th {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} \& {{Data Mining}}},
  author = {Li, Jia and Han, Zhichao and Cheng, Hong and Su, Jiao and Wang, Pengyun and Zhang, Jianfeng and Pan, Lujia},
  date = {2019-07-25},
  series = {{{KDD}} '19},
  pages = {1279--1289},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  url = {https://dl.acm.org/doi/10.1145/3292500.3330847},
  urldate = {2024-02-26},
  abstract = {In this paper we use a time-evolving graph which consists of a sequence of graph snapshots over time to model many real-world networks. We study the path classification problem in a time-evolving graph, which has many applications in real-world scenarios, for example, predicting path failure in a telecommunication network and predicting path congestion in a traffic network in the near future. In order to capture the temporal dependency and graph structure dynamics, we design a novel deep neural network named Long Short-Term Memory R-GCN (LRGCN). LRGCN considers temporal dependency between time-adjacent graph snapshots as a special relation with memory, and uses relational GCN to jointly process both intra-time and inter-time relations. We also propose a new path representation method named self-attentive path embedding (SAPE), to embed paths of arbitrary length into fixed-length vectors. Through experiments on a real-world telecommunication network and a traffic network in California, we demonstrate the superiority of LRGCN to other competing methods in path failure prediction, and prove the effectiveness of SAPE on path representation.},
  isbn = {978-1-4503-6201-6},
  keywords = {classification,path representation,time-evolving graph},
  file = {/Users/luke/Zotero/storage/S6PAY3RN/Li et al. - 2019 - Predicting Path Failure In Time-Evolving Graphs.pdf}
}

@misc{liuSophiaScalableStochastic2023,
  title = {Sophia: {{A Scalable Stochastic Second-order Optimizer}} for {{Language Model Pre-training}}},
  shorttitle = {Sophia},
  author = {Liu, Hong and Li, Zhiyuan and Hall, David and Liang, Percy and Ma, Tengyu},
  date = {2023-10-17},
  eprint = {2305.14342},
  eprinttype = {arXiv},
  eprintclass = {cs.LG},
  url = {http://arxiv.org/abs/2305.14342},
  urldate = {2024-02-13},
  abstract = {Given the massive cost of language model pre-training, a non-trivial improvement of the optimization algorithm would lead to a material reduction on the time and cost of training. Adam and its variants have been state-of-the-art for years, and more sophisticated second-order (Hessian-based) optimizers often incur too much per-step overhead. In this paper, we propose Sophia, Second-order Clipped Stochastic Optimization, a simple scalable second-order optimizer that uses a light-weight estimate of the diagonal Hessian as the pre-conditioner. The update is the moving average of the gradients divided by the moving average of the estimated Hessian, followed by element-wise clipping. The clipping controls the worst-case update size and tames the negative impact of non-convexity and rapid change of Hessian along the trajectory. Sophia only estimates the diagonal Hessian every handful of iterations, which has negligible average per-step time and memory overhead. On language modeling with GPT models of sizes ranging from 125M to 1.5B, Sophia achieves a 2x speed-up compared to Adam in the number of steps, total compute, and wall-clock time, achieving the same perplexity with 50\% fewer steps, less total compute, and reduced wall-clock time. Theoretically, we show that Sophia, in a much simplified setting, adapts to the heterogeneous curvatures in different parameter dimensions, and thus has a run-time bound that does not depend on the condition number of the loss.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Mathematics - Optimization and Control},
  file = {/Users/luke/Zotero/storage/HDDNRU95/Liu et al. - 2023 - Sophia A Scalable Stochastic Second-order Optimiz.pdf;/Users/luke/Zotero/storage/LPL789NG/2305.html},
  note = {arXiv:2305.14342}
}

@article{longaGraphNeuralNetworks2023,
  title = {Graph {{Neural Networks}} for {{Temporal Graphs}}: {{State}} of the {{Art}}, {{Open Challenges}}, and {{Opportunities}}},
  shorttitle = {Graph {{Neural Networks}} for {{Temporal Graphs}}},
  author = {Longa, Antonio and Lachi, Veronica and Santin, Gabriele and Bianchini, Monica and Lepri, Bruno and Lio, Pietro and Scarselli, Franco and Passerini, Andrea},
  date = {2023-05-25},
  journaltitle = {Transactions on Machine Learning Research},
  issn = {2835-8856},
  url = {https://openreview.net/forum?id=pHCdMat0gI},
  urldate = {2024-02-23},
  abstract = {Graph Neural Networks (GNNs) have become the leading paradigm for learning on (static) graph-structured data. However, many real-world systems are dynamic in nature, since the graph and node/edge attributes change over time. In recent years, GNN-based models for temporal graphs have emerged as a promising area of research to extend the capabilities of GNNs. In this work, we provide the first comprehensive overview of the current state-of-the-art of temporal GNN, introducing a rigorous formalization of learning settings and tasks and a novel taxonomy categorizing existing approaches in terms of how the temporal aspect is represented and processed. We conclude the survey with a discussion of the most relevant open challenges for the field, from both research and application perspectives.},
  langid = {english},
  file = {/Users/luke/Zotero/storage/PG7IE565/Longa et al. - 2023 - Graph Neural Networks for Temporal Graphs State o.pdf}
}

@inproceedings{luoParameterizedExplainerGraph2020,
  title = {Parameterized {{Explainer}} for {{Graph Neural Network}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Luo, Dongsheng and Cheng, Wei and Xu, Dongkuan and Yu, Wenchao and Zong, Bo and Chen, Haifeng and Zhang, Xiang},
  date = {2020},
  volume = {33},
  pages = {19620--19631},
  publisher = {Curran Associates, Inc.},
  url = {https://proceedings.neurips.cc/paper/2020/hash/e37b08dd3015330dcbb5d6663667b8b8-Abstract.html},
  urldate = {2024-03-08},
  abstract = {Despite recent progress in Graph Neural Networks (GNNs), explaining predictions made by GNNs remains a challenging open problem. The leading method mainly addresses the local explanations (i.e., important subgraph structure and node features) to interpret why a GNN model makes the prediction for a single instance, e.g. a node or a graph. As a result, the explanation generated is painstakingly customized for each instance. The unique explanation interpreting each instance independently is not sufficient to provide a global understanding of the learned GNN model, leading to the lack of generalizability and hindering it from being used in the inductive setting. Besides, as it is designed for explaining a single instance, it is challenging to explain a set of instances naturally (e.g., graphs of a given class). In this study, we address these key challenges and propose PGExplainer, a parameterized explainer for GNNs. PGExplainer adopts a deep neural network to parameterize the generation process of explanations, which enables PGExplainer a natural approach to multi-instance explanations. Compared to the existing work, PGExplainer has a better generalization power and can be utilized in an inductive setting easily. Experiments on both synthetic and real-life datasets show highly competitive performance with up to 24.7\textbackslash\% relative improvement in AUC on explaining graph classification over the leading baseline.},
  file = {/Users/luke/Zotero/storage/SDPP6BLX/Luo et al. - 2020 - Parameterized Explainer for Graph Neural Network.pdf}
}

@misc{lvAreWeReally2021,
  title = {Are we really making much progress? {{Revisiting}}, benchmarking, and refining heterogeneous graph neural networks},
  shorttitle = {Are we really making much progress?},
  author = {Lv, Qingsong and Ding, Ming and Liu, Qiang and Chen, Yuxiang and Feng, Wenzheng and He, Siming and Zhou, Chang and Jiang, Jianguo and Dong, Yuxiao and Tang, Jie},
  date = {2021-12-30},
  eprint = {2112.14936},
  eprinttype = {arXiv},
  eprintclass = {cs.LG},
  url = {http://arxiv.org/abs/2112.14936},
  urldate = {2023-10-30},
  abstract = {Heterogeneous graph neural networks (HGNNs) have been blossoming in recent years, but the unique data processing and evaluation setups used by each work obstruct a full understanding of their advancements. In this work, we present a systematical reproduction of 12 recent HGNNs by using their official codes, datasets, settings, and hyperparameters, revealing surprising findings about the progress of HGNNs. We find that the simple homogeneous GNNs, e.g., GCN and GAT, are largely underestimated due to improper settings. GAT with proper inputs can generally match or outperform all existing HGNNs across various scenarios. To facilitate robust and reproducible HGNN research, we construct the Heterogeneous Graph Benchmark (HGB), consisting of 11 diverse datasets with three tasks. HGB standardizes the process of heterogeneous graph data splits, feature processing, and performance evaluation. Finally, we introduce a simple but very strong baseline Simple-HGN--which significantly outperforms all previous models on HGB--to accelerate the advancement of HGNNs in the future.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Computer Science - Social and Information Networks},
  file = {/Users/luke/Zotero/storage/2WPDMEQK/Lv et al. - 2021 - Are we really making much progress Revisiting, be.pdf;/Users/luke/Zotero/storage/LR9UTKS6/2112.html},
  note = {arXiv:2112.14936}
}

@inproceedings{lvAreWeReally2021a,
  title = {Are we really making much progress? {{Revisiting}}, benchmarking and refining heterogeneous graph neural networks},
  shorttitle = {Are we really making much progress?},
  booktitle = {Proceedings of the 27th {{ACM SIGKDD Conference}} on {{Knowledge Discovery}} \& {{Data Mining}}},
  author = {Lv, Qingsong and Ding, Ming and Liu, Qiang and Chen, Yuxiang and Feng, Wenzheng and He, Siming and Zhou, Chang and Jiang, Jianguo and Dong, Yuxiao and Tang, Jie},
  date = {2021-08-14},
  series = {{{KDD}} '21},
  pages = {1150--1160},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  url = {https://doi.org/10.1145/3447548.3467350},
  urldate = {2024-07-05},
  abstract = {Heterogeneous graph neural networks (HGNNs) have been blossoming in recent years, but the unique data processing and evaluation setups used by each work obstruct a full understanding of their advancements. In this work, we present a systematical reproduction of 12 recent HGNNs by using their official codes, datasets, settings, and hyperparameters, revealing surprising findings about the progress of HGNNs. We find that the simple homogeneous GNNs, e.g., GCN and GAT, are largely underestimated due to improper settings. GAT with proper inputs can generally match or outperform all existing HGNNs across various scenarios. To facilitate robust and reproducible HGNN research, we construct the Heterogeneous Graph Benchmark (HGB) , consisting of 11 diverse datasets with three tasks. HGB standardizes the process of heterogeneous graph data splits, feature processing, and performance evaluation. Finally, we introduce a simple but very strong baseline Simple-HGN-which significantly outperforms all previous models on HGB-to accelerate the advancement of HGNNs in the future.},
  isbn = {978-1-4503-8332-5}
}

@article{maatenVisualizingDataUsing2008,
  title = {Visualizing {{Data}} using t-{{SNE}}},
  author = {family=Maaten, given=Laurens, prefix=van der, useprefix=false and Hinton, Geoffrey},
  date = {2008},
  journaltitle = {Journal of Machine Learning Research},
  volume = {9},
  number = {86},
  pages = {2579--2605},
  issn = {1533-7928},
  url = {http://jmlr.org/papers/v9/vandermaaten08a.html},
  urldate = {2024-02-08},
  abstract = {We present a new technique called "t-SNE" that visualizes high-dimensional data by giving each datapoint a location in a two or three-dimensional map. The technique is a variation of Stochastic Neighbor Embedding (Hinton and Roweis, 2002) that is much easier to optimize, and produces significantly better visualizations by reducing the tendency to crowd points together in the center of the map. t-SNE is better than existing techniques at creating a single map that reveals structure at many different scales. This is particularly important for high-dimensional data that lie on several different, but related, low-dimensional manifolds, such as images ofobjects from multiple classes seen from multiple viewpoints. For visualizing the structure of very large data sets, we show how t-SNE can use random walks on neighborhood graphs to allow the implicit structure of all of the data to influence the way in which a subset of the data is displayed. We illustrate the performance of t-SNE on a wide variety of data sets and compare it with many other non-parametric visualization techniques, including Sammon mapping, Isomap, and Locally Linear Embedding. The visualizations produced by t-SNE are significantly better than those produced by the other techniques on almost all of the data sets.},
  file = {/Users/luke/Zotero/storage/6E9T576P/Maaten and Hinton - 2008 - Visualizing Data using t-SNE.pdf}
}

@misc{magisterGCExplainerHumanintheLoopConceptbased2021,
  title = {{{GCExplainer}}: {{Human-in-the-Loop Concept-based Explanations}} for {{Graph Neural Networks}}},
  shorttitle = {{{GCExplainer}}},
  author = {Magister, Lucie Charlotte and Kazhdan, Dmitry and Singh, Vikash and Liò, Pietro},
  date = {2021-07-25},
  eprint = {2107.11889},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2107.11889},
  urldate = {2024-02-25},
  abstract = {While graph neural networks (GNNs) have been shown to perform well on graph-based data from a variety of fields, they suffer from a lack of transparency and accountability, which hinders trust and consequently the deployment of such models in high-stake and safety-critical scenarios. Even though recent research has investigated methods for explaining GNNs, these methods are limited to single-instance explanations, also known as local explanations. Motivated by the aim of providing global explanations, we adapt the well-known Automated Concept-based Explanation approach (Ghorbani et al., 2019) to GNN node and graph classification, and propose GCExplainer. GCExplainer is an unsupervised approach for post-hoc discovery and extraction of global concept-based explanations for GNNs, which puts the human in the loop. We demonstrate the success of our technique on five node classification datasets and two graph classification datasets, showing that we are able to discover and extract high-quality concept representations by putting the human in the loop. We achieve a maximum completeness score of 1 and an average completeness score of 0.753 across the datasets. Finally, we show that the concept-based explanations provide an improved insight into the datasets and GNN models compared to the state-of-the-art explanations produced by GNNExplainer (Ying et al., 2019).},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/luke/Zotero/storage/GBQVARP9/Magister et al. - 2021 - GCExplainer Human-in-the-Loop Concept-based Expla.pdf;/Users/luke/Zotero/storage/3PFGEYXE/2107.html},
  note = {arXiv:2107.11889}
}

@article{majdSLDeepStatementlevelSoftware2020,
  title = {{{SLDeep}}: {{Statement-level}} software defect prediction using deep-learning model on static code features},
  shorttitle = {{{SLDeep}}},
  author = {Majd, Amirabbas and Vahidi-Asl, Mojtaba and Khalilian, Alireza and Poorsarvi-Tehrani, Pooria and Haghighi, Hassan},
  date = {2020-06-01},
  journaltitle = {Expert Systems with Applications},
  shortjournal = {Expert Systems with Applications},
  volume = {147},
  pages = {113156},
  issn = {0957-4174},
  url = {https://www.sciencedirect.com/science/article/pii/S0957417419308735},
  urldate = {2023-12-19},
  abstract = {Software defect prediction (SDP) seeks to estimate fault-prone areas of the code to focus testing activities on more suspicious portions. Consequently, high-quality software is released with less time and effort. The current SDP techniques however work at coarse-grained units, such as a module or a class, putting some burden on the developers to locate the fault. To address this issue, we propose a new technique called as Statement-Level software defect prediction using Deep-learning model (SLDeep). The significance of SLDeep for intelligent and expert systems is that it demonstrates a novel use of deep-learning models to the solution of a practical problem faced by software developers. To reify our proposal, we defined a suite of 32 statement-level metrics, such as the number of binary and unary operators used in a statement. Then, we applied as learning model, long short-term memory (LSTM). We conducted experiments using 119,989 C/C++ programs within Code4Bench. The programs comprise 2,356,458 lines of code of which 292,064 lines are faulty. The benchmark comprises a diverse set of programs and versions, written by thousands of developers. Therefore, it tends to give a model that can be used for cross-project SDP. In the experiments, our trained model could successfully classify the unseen data (that is, fault-proneness of new statements) with average performance measures 0.979, 0.570, and 0.702 in terms of recall, precision, and accuracy, respectively. These experimental results suggest that SLDeep is effective for statement-level SDP. The impact of this work is twofold. Working at statement-level further alleviates developer's burden in pinpointing the fault locations. Second, cross-project feature of SLDeep helps defect prediction research become more industrially-viable.},
  keywords = {Defect,Fault prediction model,Machine learning,Software fault proneness,Software metric}
}

@inproceedings{malekiLearningExplainHypergraph2023,
  title = {Learning to {{Explain Hypergraph Neural Networks}}},
  author = {Maleki, Sepideh and Hajiramezanali, Ehsan and Scalia, Gabriele and Biancalani, Tommaso and Chuang, Kangway V.},
  date = {2023-06-18},
  url = {https://openreview.net/forum?id=B6YeDatcFw},
  urldate = {2024-04-03},
  abstract = {Hypergraphs are expressive structures for describing higher-order relationships among entities, with widespread applications across biology and drug discovery. Hypergraph neural networks (HGNNs) have recently emerged as a promising representation learning approach on these structures for clustering, classification, and more. However, despite their promising performance, HGNNs remain a black box, and explaining how they make predictions remains an open challenge. To address this problem, we propose HyperEX, a post-hoc explainability framework for hypergraphs that can be applied to any trained HGNN. HyperEX computes node-hyperedge pair importance to identify sub-hypergraphs as explanations. Our experiments demonstrate how HyperEX learns important sub-hypergraphs responsible for driving node classification to give useful insight into HGNNs.},
  eventtitle = {2nd {{Annual Topology}}, {{Algebra}}, and {{Geometry}} in {{Machine Learning Workshop}} ({{TAGML2023}})},
  langid = {english},
  file = {/Users/luke/Zotero/storage/J5H3SXRP/Maleki et al. - 2023 - Learning to Explain Hypergraph Neural Networks.pdf}
}

@inproceedings{maoUltraGCNUltraSimplification2021,
  title = {{{UltraGCN}}: {{Ultra Simplification}} of {{Graph Convolutional Networks}} for {{Recommendation}}},
  shorttitle = {{{UltraGCN}}},
  booktitle = {Proceedings of the 30th {{ACM International Conference}} on {{Information}} \& {{Knowledge Management}}},
  author = {Mao, Kelong and Zhu, Jieming and Xiao, Xi and Lu, Biao and Wang, Zhaowei and He, Xiuqiang},
  date = {2021-10-30},
  series = {{{CIKM}} '21},
  pages = {1253--1262},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  url = {https://dl.acm.org/doi/10.1145/3459637.3482291},
  urldate = {2024-02-15},
  abstract = {With the recent success of graph convolutional networks (GCNs), they have been widely applied for recommendation, and achieved impressive performance gains. The core of GCNs lies in its message passing mechanism to aggregate neighborhood information. However, we observed that message passing largely slows down the convergence of GCNs during training, especially for large-scale recommender systems, which hinders their wide adoption. LightGCN makes an early attempt to simplify GCNs for collaborative filtering by omitting feature transformations and nonlinear activations. In this paper, we take one step further to propose an ultra-simplified formulation of GCNs (dubbed UltraGCN), which skips infinite layers of message passing for efficient recommendation. Instead of explicit message passing, UltraGCN resorts to directly approximate the limit of infinite-layer graph convolutions via a constraint loss. Meanwhile, UltraGCN allows for more appropriate edge weight assignments and flexible adjustment of the relative importances among different types of relationships. This finally yields a simple yet effective UltraGCN model, which is easy to implement and efficient to train. Experimental results on four benchmark datasets show that UltraGCN not only outperforms the state-of-the-art GCN models but also achieves more than 10x speedup over LightGCN.},
  isbn = {978-1-4503-8446-9},
  keywords = {collaborative filtering,graph convolutional networks,recommender systems},
  file = {/Users/luke/Zotero/storage/GB6IR83Q/Mao et al. - 2021 - UltraGCN Ultra Simplification of Graph Convolutio.pdf}
}

@inproceedings{maUnifiedViewGraph2021,
  title = {A {{Unified View}} on {{Graph Neural Networks}} as {{Graph Signal Denoising}}},
  booktitle = {Proceedings of the 30th {{ACM International Conference}} on {{Information}} \& {{Knowledge Management}}},
  author = {Ma, Yao and Liu, Xiaorui and Zhao, Tong and Liu, Yozen and Tang, Jiliang and Shah, Neil},
  date = {2021-10-30},
  series = {{{CIKM}} '21},
  pages = {1202--1211},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  url = {https://dl.acm.org/doi/10.1145/3459637.3482225},
  urldate = {2024-05-02},
  abstract = {Graph Neural Networks (GNNs) have risen to prominence in learning representations for graph structured data. A single GNN layer typically consists of a feature transformation and a feature aggregation operation. The former normally uses feed-forward networks to transform features, while the latter aggregates the transformed features over the graph. Numerous recent works have proposed GNN models with different designs in the aggregation operation. In this work, we establish mathematically that the aggregation processes in a group of representative GNN models including GCN, GAT, PPNP, and APPNP can be regarded as (approximately) solving a graph denoising problem with a smoothness assumption. Such a unified view across GNNs not only provides a new perspective to understand a variety of aggregation operations but also enables us to develop a unified graph neural network framework UGNN. To demonstrate its promising potential, we instantiate a novel GNN model, ADA-UGNN, derived from UGNN, to handle graphs with adaptive smoothness across nodes. Comprehensive experiments show the effectiveness of ADA-UGNN.},
  isbn = {978-1-4503-8446-9},
  keywords = {graph neural networks,graph signal denoising,semi-supervised classification},
  file = {/Users/luke/Zotero/storage/33XQLD4V/Ma et al. - 2021 - A Unified View on Graph Neural Networks as Graph S.pdf}
}

@online{mccuneProver9Mace42010,
  title = {Prover9 and {{Mace4}}},
  author = {McCune, William},
  date = {2010},
  url = {https://www.cs.unm.edu/~mccune/prover9/},
  urldate = {2024-03-13},
  file = {/Users/luke/Zotero/storage/GQXDARV8/prover9.html}
}

@misc{mcinnesUMAPUniformManifold2020,
  title = {{{UMAP}}: {{Uniform Manifold Approximation}} and {{Projection}} for {{Dimension Reduction}}},
  shorttitle = {{{UMAP}}},
  author = {McInnes, Leland and Healy, John and Melville, James},
  date = {2020-09-17},
  eprint = {1802.03426},
  eprinttype = {arXiv},
  eprintclass = {stat.ML},
  url = {http://arxiv.org/abs/1802.03426},
  urldate = {2024-02-12},
  abstract = {UMAP (Uniform Manifold Approximation and Projection) is a novel manifold learning technique for dimension reduction. UMAP is constructed from a theoretical framework based in Riemannian geometry and algebraic topology. The result is a practical scalable algorithm that applies to real world data. The UMAP algorithm is competitive with t-SNE for visualization quality, and arguably preserves more of the global structure with superior run time performance. Furthermore, UMAP has no computational restrictions on embedding dimension, making it viable as a general purpose dimension reduction technique for machine learning.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computational Geometry,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/luke/Zotero/storage/YW9SFH37/McInnes et al. - 2020 - UMAP Uniform Manifold Approximation and Projectio.pdf;/Users/luke/Zotero/storage/ICR2PNHM/1802.html},
  note = {arXiv:1802.03426}
}

@article{mcphersonBirdsFeatherHomophily2001,
  title = {Birds of a {{Feather}}: {{Homophily}} in {{Social Networks}}},
  shorttitle = {Birds of a {{Feather}}},
  author = {McPherson, Miller and Smith-Lovin, Lynn and Cook, James M.},
  date = {2001-08-01},
  journaltitle = {Annual Review of Sociology},
  volume = {27},
  pages = {415--444},
  publisher = {Annual Reviews},
  issn = {0360-0572, 1545-2115},
  url = {https://www.annualreviews.org/content/journals/10.1146/annurev.soc.27.1.415},
  urldate = {2024-05-26},
  abstract = {Similarity breeds connection. This principle—the homophily principle—structures network ties of every type, including marriage, friendship, work, advice, support, information transfer, exchange, comembership, and other types of relationship. The result is that people\&apos;s personal networks are homogeneous with regard to many sociodemographic, behavioral, and intrapersonal characteristics. Homophily limits people\&apos;s social worlds in a way that has powerful implications for the information they receive, the attitudes they form, and the interactions they experience. Homophily in race and ethnicity creates the strongest divides in our personal environments, with age, religion, education, occupation, and gender following in roughly that order. Geographic propinquity, families, organizations, and isomorphic positions in social systems all create contexts in which homophilous relations form. Ties between nonsimilar individuals also dissolve at a higher rate, which sets the stage for the formation of niches (localized positions) within social space. We argue for more research on: (a) the basic ecological processes that link organizations, associations, cultural communities, social movements, and many other social forms; (b) the impact of multiplex ties on the patterns of homophily; and (c) the dynamics of network change over time through which networks and other social entities co-evolve.},
  issue = {Volume 27, 2001},
  langid = {english},
  file = {/Users/luke/Zotero/storage/LNFW94Y9/annurev.soc.27.1.html}
}

@inproceedings{mhammediEfficientOrthogonalParametrisation2017,
  title = {Efficient {{Orthogonal Parametrisation}} of {{Recurrent Neural Networks Using Householder Reflections}}},
  booktitle = {Proceedings of the 34th {{International Conference}} on {{Machine Learning}}},
  author = {Mhammedi, Zakaria and Hellicar, Andrew and Rahman, Ashfaqur and Bailey, James},
  date = {2017-07-17},
  pages = {2401--2409},
  publisher = {PMLR},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v70/mhammedi17a.html},
  urldate = {2024-04-01},
  abstract = {The problem of learning long-term dependencies in sequences using Recurrent Neural Networks (RNNs) is still a major challenge. Recent methods have been suggested to solve this problem by constraining the transition matrix to be unitary during training which ensures that its norm is equal to one and prevents exploding gradients. These methods either have limited expressiveness or scale poorly with the size of the network when compared with the simple RNN case, especially when using stochastic gradient descent with a small mini-batch size. Our contributions are as follows; we first show that constraining the transition matrix to be unitary is a special case of an orthogonal constraint. Then we present a new parametrisation of the transition matrix which allows efficient training of an RNN while ensuring that the matrix is always orthogonal. Our results show that the orthogonal constraint on the transition matrix applied through our parametrisation gives similar benefits to the unitary constraint, without the time complexity limitations.},
  eventtitle = {{{ICML}} 2017},
  langid = {english},
  file = {/Users/luke/Zotero/storage/5CVVTI7V/Mhammedi et al. - 2017 - Efficient Orthogonal Parametrisation of Recurrent .pdf}
}

@misc{mildenhallLocalLightField2019,
  title = {Local {{Light Field Fusion}}: {{Practical View Synthesis}} with {{Prescriptive Sampling Guidelines}}},
  shorttitle = {Local {{Light Field Fusion}}},
  author = {Mildenhall, Ben and Srinivasan, Pratul P. and Ortiz-Cayon, Rodrigo and Kalantari, Nima Khademi and Ramamoorthi, Ravi and Ng, Ren and Kar, Abhishek},
  date = {2019-05-02},
  eprint = {1905.00889},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1905.00889},
  urldate = {2023-11-15},
  abstract = {We present a practical and robust deep learning solution for capturing and rendering novel views of complex real world scenes for virtual exploration. Previous approaches either require intractably dense view sampling or provide little to no guidance for how users should sample views of a scene to reliably render high-quality novel views. Instead, we propose an algorithm for view synthesis from an irregular grid of sampled views that first expands each sampled view into a local light field via a multiplane image (MPI) scene representation, then renders novel views by blending adjacent local light fields. We extend traditional plenoptic sampling theory to derive a bound that specifies precisely how densely users should sample views of a given scene when using our algorithm. In practice, we apply this bound to capture and render views of real world scenes that achieve the perceptual quality of Nyquist rate view sampling while using up to 4000x fewer views. We demonstrate our approach's practicality with an augmented reality smartphone app that guides users to capture input images of a scene and viewers that enable realtime virtual exploration on desktop and mobile platforms.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics},
  file = {/Users/luke/Zotero/storage/WENMCXKZ/Mildenhall et al. - 2019 - Local Light Field Fusion Practical View Synthesis.pdf;/Users/luke/Zotero/storage/TIJ83Q3T/1905.html},
  note = {arXiv:1905.00889}
}

@misc{mildenhallNeRFRepresentingScenes2020,
  title = {{{NeRF}}: {{Representing Scenes}} as {{Neural Radiance Fields}} for {{View Synthesis}}},
  shorttitle = {{{NeRF}}},
  author = {Mildenhall, Ben and Srinivasan, Pratul P. and Tancik, Matthew and Barron, Jonathan T. and Ramamoorthi, Ravi and Ng, Ren},
  date = {2020-08-03},
  eprint = {2003.08934},
  eprinttype = {arXiv},
  eprintclass = {cs.CV},
  url = {http://arxiv.org/abs/2003.08934},
  urldate = {2023-10-19},
  abstract = {We present a method that achieves state-of-the-art results for synthesizing novel views of complex scenes by optimizing an underlying continuous volumetric scene function using a sparse set of input views. Our algorithm represents a scene using a fully-connected (non-convolutional) deep network, whose input is a single continuous 5D coordinate (spatial location \$(x,y,z)\$ and viewing direction \$(\textbackslash theta, \textbackslash phi)\$) and whose output is the volume density and view-dependent emitted radiance at that spatial location. We synthesize views by querying 5D coordinates along camera rays and use classic volume rendering techniques to project the output colors and densities into an image. Because volume rendering is naturally differentiable, the only input required to optimize our representation is a set of images with known camera poses. We describe how to effectively optimize neural radiance fields to render photorealistic novel views of scenes with complicated geometry and appearance, and demonstrate results that outperform prior work on neural rendering and view synthesis. View synthesis results are best viewed as videos, so we urge readers to view our supplementary video for convincing comparisons.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics},
  file = {/Users/luke/Zotero/storage/P9EYRKVR/Mildenhall et al. - 2020 - NeRF Representing Scenes as Neural Radiance Field.pdf;/Users/luke/Zotero/storage/VPFT59SE/2003.html},
  note = {arXiv:2003.08934}
}

@article{mohamedDiscoveringProteinDrug2020,
  title = {Discovering protein drug targets using knowledge graph embeddings},
  author = {Mohamed, Sameh K and Nováček, Vít and Nounu, Aayah},
  date = {2020-01-15},
  journaltitle = {Bioinformatics},
  shortjournal = {Bioinformatics},
  volume = {36},
  number = {2},
  pages = {603--610},
  issn = {1367-4803},
  url = {https://doi.org/10.1093/bioinformatics/btz600},
  urldate = {2024-05-13},
  abstract = {Computational approaches for predicting drug–target interactions (DTIs) can provide valuable insights into the drug mechanism of action. DTI predictions can help to quickly identify new promising (on-target) or unintended (off-target) effects of drugs. However, existing models face several challenges. Many can only process a limited number of drugs and/or have poor proteome coverage. The current approaches also often suffer from high false positive prediction rates.We propose a novel computational approach for predicting drug target proteins. The approach is based on formulating the problem as a link prediction in knowledge graphs (robust, machine-readable representations of networked knowledge). We use biomedical knowledge bases to create a knowledge graph of entities connected to both drugs and their potential targets. We propose a specific knowledge graph embedding model, TriModel, to learn vector representations (i.e. embeddings) for all drugs and targets in the created knowledge graph. These representations are consequently used to infer candidate drug target interactions based on their scores computed by the trained TriModel model. We have experimentally evaluated our method using computer simulations and compared it to five existing models. This has shown that our approach outperforms all previous ones in terms of both area under ROC and precision–recall curves in standard benchmark tests.The data, predictions and models are available at: drugtargets.insight-centre.org.Supplementary data are available at Bioinformatics online.},
  file = {/Users/luke/Zotero/storage/GQ4DCMG9/Mohamed et al. - 2020 - Discovering protein drug targets using knowledge g.pdf}
}

@misc{muellerNormalizationLayersAre2023,
  title = {Normalization {{Layers Are All That Sharpness-Aware Minimization Needs}}},
  author = {Mueller, Maximilian and Vlaar, Tiffany and Rolnick, David and Hein, Matthias},
  date = {2023-11-17},
  eprint = {2306.04226},
  eprinttype = {arXiv},
  eprintclass = {cs.LG},
  url = {http://arxiv.org/abs/2306.04226},
  urldate = {2024-02-13},
  abstract = {Sharpness-aware minimization (SAM) was proposed to reduce sharpness of minima and has been shown to enhance generalization performance in various settings. In this work we show that perturbing only the affine normalization parameters (typically comprising 0.1\% of the total parameters) in the adversarial step of SAM can outperform perturbing all of the parameters.This finding generalizes to different SAM variants and both ResNet (Batch Normalization) and Vision Transformer (Layer Normalization) architectures. We consider alternative sparse perturbation approaches and find that these do not achieve similar performance enhancement at such extreme sparsity levels, showing that this behaviour is unique to the normalization layers. Although our findings reaffirm the effectiveness of SAM in improving generalization performance, they cast doubt on whether this is solely caused by reduced sharpness.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/luke/Zotero/storage/YPWB7E59/Mueller et al. - 2023 - Normalization Layers Are All That Sharpness-Aware .pdf;/Users/luke/Zotero/storage/VSVC88KT/2306.html},
  note = {arXiv:2306.04226}
}

@inproceedings{nguyenReGVDRevisitingGraph2022,
  title = {{{ReGVD}}: revisiting graph neural networks for vulnerability detection},
  shorttitle = {{{ReGVD}}},
  booktitle = {Proceedings of the {{ACM}}/{{IEEE}} 44th {{International Conference}} on {{Software Engineering}}: {{Companion Proceedings}}},
  author = {Nguyen, Van-Anh and Nguyen, Dai Quoc and Nguyen, Van and Le, Trung and Tran, Quan Hung and Phung, Dinh},
  date = {2022-10-19},
  series = {{{ICSE}} '22},
  pages = {178--182},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  url = {https://doi.org/10.1145/3510454.3516865},
  urldate = {2023-12-20},
  abstract = {Identifying vulnerabilities in the source code is essential to protect the software systems from cyber security attacks. It, however, is also a challenging step that requires specialized expertise in security and code representation. To this end, we aim to develop a general, practical, and programming language-independent model capable of running on various source codes and libraries without difficulty. Therefore, we consider vulnerability detection as an inductive text classification problem and propose ReGVD, a simple yet effective graph neural network-based model for the problem. In particular, ReGVD views each raw source code as a flat sequence of tokens to build a graph, wherein node features are initialized by only the token embedding layer of a pre-trained programming language (PL) model. ReGVD then leverages residual connection among GNN layers and examines a mixture of graph-level sum and max poolings to return a graph embedding for the source code. ReGVD outperforms the existing state-of-the-art models and obtains the highest accuracy on the real-world benchmark dataset from CodeXGLUE for vulnerability detection. Our code is available at: https://github.com/daiquocnguyen/GNN-ReGVD.},
  isbn = {978-1-4503-9223-5},
  keywords = {graph neural networks,security,text classification,vulnerability detection}
}

@inproceedings{NIPS2017_8a1d6947,
  title = {{{GANs}} trained by a two time-scale update rule converge to a local nash equilibrium},
  booktitle = {Advances in neural information processing systems},
  author = {Heusel, Martin and Ramsauer, Hubert and Unterthiner, Thomas and Nessler, Bernhard and Hochreiter, Sepp},
  editor = {Guyon, I. and Luxburg, U. Von and Bengio, S. and Wallach, H. and Fergus, R. and Vishwanathan, S. and Garnett, R.},
  date = {2017},
  volume = {30},
  publisher = {Curran Associates, Inc.},
  url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/8a1d694707eb0fefe65871369074926d-Paper.pdf}
}

@article{olahFeatureVisualization2017,
  title = {Feature {{Visualization}}},
  author = {Olah, Chris and Mordvintsev, Alexander and Schubert, Ludwig},
  date = {2017-11-07},
  journaltitle = {Distill},
  shortjournal = {Distill},
  volume = {2},
  number = {11},
  pages = {e7},
  issn = {2476-0757},
  url = {https://distill.pub/2017/feature-visualization},
  urldate = {2024-03-08},
  abstract = {How neural networks build up their understanding of images},
  langid = {english},
  file = {/Users/luke/Zotero/storage/XQXMKHII/feature-visualization.html}
}

@inproceedings{oonoGraphNeuralNetworks2019,
  title = {Graph {{Neural Networks Exponentially Lose Expressive Power}} for {{Node Classification}}},
  author = {Oono, Kenta and Suzuki, Taiji},
  date = {2019-09-23},
  url = {https://openreview.net/forum?id=S1ldO2EFPr},
  urldate = {2023-12-30},
  abstract = {Graph Neural Networks (graph NNs) are a promising deep learning approach for analyzing graph-structured data. However, it is known that they do not improve (or sometimes worsen) their predictive performance as we pile up many layers and add non-lineality. To tackle this problem, we investigate the expressive power of graph NNs via their asymptotic behaviors as the layer size tends to infinity. Our strategy is to generalize the forward propagation of a Graph Convolutional Network (GCN), which is a popular graph NN variant, as a specific dynamical system. In the case of a GCN, we show that when its weights satisfy the conditions determined by the spectra of the (augmented) normalized Laplacian, its output exponentially approaches the set of signals that carry information of the connected components and node degrees only for distinguishing nodes. Our theory enables us to relate the expressive power of GCNs with the topological information of the underlying graphs inherent in the graph spectra. To demonstrate this, we characterize the asymptotic behavior of GCNs on the Erd\textbackslash H\{o\}s -- R\textbackslash '\{e\}nyi graph. We show that when the Erd\textbackslash H\{o\}s -- R\textbackslash '\{e\}nyi graph is sufficiently dense and large, a broad range of GCNs on it suffers from the ``information loss" in the limit of infinite layers with high probability. Based on the theory, we provide a principled guideline for weight normalization of graph NNs. We experimentally confirm that the proposed weight scaling enhances the predictive performance of GCNs in real data. Code is available at https://github.com/delta2323/gnn-asymptotics.},
  eventtitle = {International {{Conference}} on {{Learning Representations}}},
  langid = {english},
  file = {/Users/luke/Zotero/storage/QSR5JTPY/Oono and Suzuki - 2019 - Graph Neural Networks Exponentially Lose Expressiv.pdf}
}

@online{owaspOWASPTop10,
  title = {{{OWASP Top}} 10:2021},
  author = {{OWASP}},
  url = {https://owasp.org/Top10/},
  urldate = {2023-12-18},
  organization = {OWASP Top 10:2021},
  file = {/Users/luke/Zotero/storage/XB4APN44/Top10.html}
}

@article{panagopoulosTransferGraphNeural2021,
  title = {Transfer {{Graph Neural Networks}} for {{Pandemic Forecasting}}},
  author = {Panagopoulos, George and Nikolentzos, Giannis and Vazirgiannis, Michalis},
  date = {2021-05-18},
  journaltitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {35},
  number = {6},
  pages = {4838--4845},
  issn = {2374-3468},
  url = {https://ojs.aaai.org/index.php/AAAI/article/view/16616},
  urldate = {2024-02-26},
  abstract = {The recent outbreak of COVID-19 has affected millions of individuals around the world and has posed a significant challenge to global healthcare. From the early days of the pandemic, it became clear that it is highly contagious and that human mobility contributes significantly to its spread. In this paper, we utilize graph representation learning to capitalize on the underlying relationship of population movement with the spread of COVID-19. Specifically, we create a graph where the nodes correspond to a country's regions, the features include the region's history of COVID-19, and the edge weights denote human mobility from one region to another. Subsequently, we employ graph neural networks to predict the number of future cases, encoding the underlying diffusion patterns that govern the spread into our learning model. Furthermore, to account for the limited amount of training data, we capitalize on the pandemic's asynchronous outbreaks across countries and use a model-agnostic meta-learning based method to transfer knowledge from one country's model to another's. We compare the proposed approach against simple baselines and more traditional forecasting techniques in 4 European countries. Experimental results demonstrate the superiority of our method, highlighting the usefulness of GNNs in epidemiological prediction. Transfer learning provides the best model, highlighting its potential to improve the accuracy of the predictions in case of secondary waves, given data from past/parallel outbreaks.},
  issue = {6},
  langid = {english},
  keywords = {AI Responses to the COVID-19 Pandemic (Covid19)},
  file = {/Users/luke/Zotero/storage/XNF3MWWH/Panagopoulos et al. - 2021 - Transfer Graph Neural Networks for Pandemic Foreca.pdf}
}

@misc{papillonArchitecturesTopologicalDeep2023,
  title = {Architectures of {{Topological Deep Learning}}: {{A Survey}} on {{Topological Neural Networks}}},
  shorttitle = {Architectures of {{Topological Deep Learning}}},
  author = {Papillon, Mathilde and Sanborn, Sophia and Hajij, Mustafa and Miolane, Nina},
  date = {2023-08-18},
  eprint = {2304.10031},
  eprinttype = {arXiv},
  eprintclass = {cs.LG},
  url = {http://arxiv.org/abs/2304.10031},
  urldate = {2023-10-13},
  abstract = {The natural world is full of complex systems characterized by intricate relations between their components: from social interactions between individuals in a social network to electrostatic interactions between atoms in a protein. Topological Deep Learning (TDL) provides a comprehensive framework to process and extract knowledge from data associated with these systems, such as predicting the social community to which an individual belongs or predicting whether a protein can be a reasonable target for drug development. TDL has demonstrated theoretical and practical advantages that hold the promise of breaking ground in the applied sciences and beyond. However, the rapid growth of the TDL literature has also led to a lack of unification in notation and language across Topological Neural Network (TNN) architectures. This presents a real obstacle for building upon existing works and for deploying TNNs to new real-world problems. To address this issue, we provide an accessible introduction to TDL, and compare the recently published TNNs using a unified mathematical and graphical notation. Through an intuitive and critical review of the emerging field of TDL, we extract valuable insights into current challenges and exciting opportunities for future development.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/luke/Zotero/storage/GCXKP42Z/Papillon et al. - 2023 - Architectures of Topological Deep Learning A Surv.pdf;/Users/luke/Zotero/storage/JAM3UBHZ/2304.html},
  note = {arXiv:2304.10031}
}

@article{parejaEvolveGCNEvolvingGraph2020,
  title = {{{EvolveGCN}}: {{Evolving Graph Convolutional Networks}} for {{Dynamic Graphs}}},
  shorttitle = {{{EvolveGCN}}},
  author = {Pareja, Aldo and Domeniconi, Giacomo and Chen, Jie and Ma, Tengfei and Suzumura, Toyotaro and Kanezashi, Hiroki and Kaler, Tim and Schardl, Tao and Leiserson, Charles},
  date = {2020-04-03},
  journaltitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
  shortjournal = {AAAI},
  volume = {34},
  number = {04},
  pages = {5363--5370},
  issn = {2374-3468, 2159-5399},
  url = {https://ojs.aaai.org/index.php/AAAI/article/view/5984},
  urldate = {2024-02-26},
  abstract = {Graph representation learning resurges as a trending research subject owing to the widespread use of deep learning for Euclidean data, which inspire various creative designs of neural networks in the non-Euclidean domain, particularly graphs. With the success of these graph neural networks (GNN) in the static setting, we approach further practical scenarios where the graph dynamically evolves. Existing approaches typically resort to node embeddings and use a recurrent neural network (RNN, broadly speaking) to regulate the embeddings and learn the temporal dynamics. These methods require the knowledge of a node in the full time span (including both training and testing) and are less applicable to the frequent change of the node set. In some extreme scenarios, the node sets at different time steps may completely differ. To resolve this challenge, we propose EvolveGCN, which adapts the graph convolutional network (GCN) model along the temporal dimension without resorting to node embeddings. The proposed approach captures the dynamism of the graph sequence through using an RNN to evolve the GCN parameters. Two architectures are considered for the parameter evolution. We evaluate the proposed approach on tasks including link prediction, edge classification, and node classification. The experimental results indicate a generally higher performance of EvolveGCN compared with related approaches. The code is available at https://github.com/IBM/EvolveGCN.},
  file = {/Users/luke/Zotero/storage/G33KTDLN/Pareja et al. - 2020 - EvolveGCN Evolving Graph Convolutional Networks f.pdf}
}

@inproceedings{paszkePyTorchImperativeStyle2019,
  title = {{{PyTorch}}: {{An Imperative Style}}, {{High-Performance Deep Learning Library}}},
  shorttitle = {{{PyTorch}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
  date = {2019},
  volume = {32},
  publisher = {Curran Associates, Inc.},
  url = {https://proceedings.neurips.cc/paper_files/paper/2019/hash/bdbca288fee7f92f2bfa9f7012727740-Abstract.html},
  urldate = {2024-05-26},
  abstract = {Deep learning frameworks have often focused on either usability or speed, but not both. PyTorch is a machine learning library that shows that these two goals are in fact compatible: it was designed from first principles to support an imperative and Pythonic programming style that supports code as a model, makes debugging easy and is consistent with other popular scientific computing libraries, while remaining efficient and supporting hardware accelerators such as GPUs. In this paper, we detail the principles that drove the implementation of PyTorch and how they are reflected in its architecture. We emphasize that every aspect of PyTorch is a regular Python program under the full control of its user. We also explain how the careful and pragmatic implementation of the key components of its runtime enables them to work together to achieve compelling performance. We demonstrate the efficiency of individual subsystems, as well as the overall speed of PyTorch on several commonly used benchmarks.},
  file = {/Users/luke/Zotero/storage/NSY5AX63/Paszke et al. - 2019 - PyTorch An Imperative Style, High-Performance Dee.pdf}
}

@article{petriHomologicalScaffoldsBrain2014,
  title = {Homological scaffolds of brain functional networks},
  author = {Petri, G. and Expert, P. and Turkheimer, F. and Carhart-Harris, R. and Nutt, D. and Hellyer, P. J. and Vaccarino, F.},
  date = {2014-12-06},
  journaltitle = {Journal of The Royal Society Interface},
  volume = {11},
  number = {101},
  pages = {20140873},
  publisher = {Royal Society},
  url = {https://royalsocietypublishing.org/doi/10.1098/rsif.2014.0873},
  urldate = {2024-03-19},
  abstract = {Networks, as efficient representations of complex systems, have appealed to scientists for a long time and now permeate many areas of science, including neuroimaging (Bullmore and Sporns 2009 Nat. Rev. Neurosci. 10, 186–198. (doi:10.1038/nrn2618)). Traditionally, the structure of complex networks has been studied through their statistical properties and metrics concerned with node and link properties, e.g. degree-distribution, node centrality and modularity. Here, we study the characteristics of functional brain networks at the mesoscopic level from a novel perspective that highlights the role of inhomogeneities in the fabric of functional connections. This can be done by focusing on the features of a set of topological objects—homological cycles—associated with the weighted functional network. We leverage the detected topological information to define the homological scaffolds, a new set of objects designed to represent compactly the homological features of the correlation network and simultaneously make their homological properties amenable to networks theoretical methods. As a proof of principle, we apply these tools to compare resting-state functional brain activity in 15 healthy volunteers after intravenous infusion of placebo and psilocybin—the main psychoactive component of magic mushrooms. The results show that the homological structure of the brain's functional patterns undergoes a dramatic change post-psilocybin, characterized by the appearance of many transient structures of low stability and of a small number of persistent ones that are not observed in the case of placebo.},
  keywords = {brain functional networks,fMRI,persistent homology,psilocybin},
  file = {/Users/luke/Zotero/storage/FFE27YEL/Petri et al. - 2014 - Homological scaffolds of brain functional networks.pdf}
}

@inproceedings{popeExplainabilityMethodsGraph2019,
  title = {Explainability {{Methods}} for {{Graph Convolutional Neural Networks}}},
  booktitle = {2019 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Pope, Phillip E. and Kolouri, Soheil and Rostami, Mohammad and Martin, Charles E. and Hoffmann, Heiko},
  date = {2019-06},
  pages = {10764--10773},
  issn = {2575-7075},
  url = {https://ieeexplore.ieee.org/document/8954227},
  urldate = {2024-03-08},
  abstract = {With the growing use of graph convolutional neural networks (GCNNs) comes the need for explainability. In this paper, we introduce explainability methods for GCNNs. We develop the graph analogues of three prominent explainability methods for convolutional neural networks: contrastive gradient-based (CG) saliency maps, Class Activation Mapping (CAM), and Excitation Back-Propagation (EB) and their variants, gradient-weighted CAM (Grad-CAM) and contrastive EB (c-EB). We show a proof-of-concept of these methods on classification problems in two application domains: visual scene graphs and molecular graphs. To compare the methods, we identify three desirable properties of explanations: (1) their importance to classification, as measured by the impact of occlusions, (2) their contrastivity with respect to different classes, and (3) their sparseness on a graph. We call the corresponding quantitative metrics fidelity, contrastivity, and sparsity and evaluate them for each method. Lastly, we analyze the salient subgraphs obtained from explanations and report frequently occurring patterns.},
  eventtitle = {2019 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  keywords = {Deep Learning},
  file = {/Users/luke/Zotero/storage/I3IUSMAI/Pope et al. - 2019 - Explainability Methods for Graph Convolutional Neu.pdf;/Users/luke/Zotero/storage/RU7WAFK9/8954227.html}
}

@misc{purificatoSheaf4RecSheafNeural2024,
  title = {{{Sheaf4Rec}}: {{Sheaf Neural Networks}} for {{Graph-based Recommender Systems}}},
  shorttitle = {{{Sheaf4Rec}}},
  author = {Purificato, Antonio and Cassarà, Giulia and Siciliano, Federico and Liò, Pietro and Silvestri, Fabrizio},
  date = {2024-03-16},
  eprint = {2304.09097},
  eprinttype = {arXiv},
  eprintclass = {cs.IR},
  url = {http://arxiv.org/abs/2304.09097},
  urldate = {2024-07-08},
  abstract = {Recent advancements in Graph Neural Networks (GNN) have facilitated their widespread adoption in various applications, including recommendation systems. GNNs have proven to be effective in addressing the challenges posed by recommendation systems by efficiently modeling graphs in which nodes represent users or items and edges denote preference relationships. However, current GNN techniques represent nodes by means of a single static vector, which may inadequately capture the intricate complexities of users and items. To overcome these limitations, we propose a solution integrating a cutting-edge model inspired by category theory: Sheaf4Rec. Unlike single vector representations, Sheaf Neural Networks and their corresponding Laplacians represent each node (and edge) using a vector space. Our approach takes advantage from this theory and results in a more comprehensive representation that can be effectively exploited during inference, providing a versatile method applicable to a wide range of graph-related tasks and demonstrating unparalleled performance. Our proposed model exhibits a noteworthy relative improvement of up to 8.53\% on F1-Score@10 and an impressive increase of up to 11.29\% on NDCG@10, outperforming existing state-of-the-art models such as Neural Graph Collaborative Filtering (NGCF), KGTORe and other recently developed GNN-based models. In addition to its superior predictive capabilities, Sheaf4Rec shows remarkable improvements in terms of efficiency: we observe substantial runtime improvements ranging from 2.5\% up to 37\% when compared to other GNN-based competitor models, indicating a more efficient way of handling information while achieving better performance. Code is available at https://github.com/antoniopurificato/Sheaf4Rec.},
  pubstate = {prepublished},
  keywords = {55,Computer Science - Information Retrieval,Computer Science - Machine Learning,H.3.3,I.2.6},
  file = {/Users/luke/Zotero/storage/EAT2HSS3/Purificato et al. - 2024 - Sheaf4Rec Sheaf Neural Networks for Graph-based R.pdf;/Users/luke/Zotero/storage/99ML5SU8/2304.html},
  note = {arXiv:2304.09097}
}

@misc{qinTemporalLinkPrediction2023,
  title = {Temporal {{Link Prediction}}: {{A Unified Framework}}, {{Taxonomy}}, and {{Review}}},
  shorttitle = {Temporal {{Link Prediction}}},
  author = {Qin, Meng and Yeung, Dit-Yan},
  date = {2023-06-29},
  eprint = {2210.08765},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2210.08765},
  urldate = {2024-02-23},
  abstract = {Dynamic graphs serve as a generic abstraction and description of the evolutionary behaviors of various complex systems (e.g., social networks and communication networks). Temporal link prediction (TLP) is a classic yet challenging inference task on dynamic graphs, which predicts possible future linkage based on historical topology. The predicted future topology can be used to support some advanced applications on real-world systems (e.g., resource pre-allocation) for better system performance. This survey provides a comprehensive review of existing TLP methods. Concretely, we first give the formal problem statements and preliminaries regarding data models, task settings, and learning paradigms that are commonly used in related research. A hierarchical fine-grained taxonomy is further introduced to categorize existing methods in terms of their data models, learning paradigms, and techniques. From a generic perspective, we propose a unified encoder-decoder framework to formulate all the methods reviewed, where different approaches only differ in terms of some components of the framework. Moreover, we envision serving the community with an open-source project OpenTLP that refactors or implements some representative TLP methods using the proposed unified framework and summarizes other public resources. As a conclusion, we finally discuss advanced topics in recent research and highlight possible future directions.},
  pubstate = {prepublished},
  keywords = {Computer Science - Social and Information Networks},
  file = {/Users/luke/Zotero/storage/ILSVF5EB/Qin and Yeung - 2023 - Temporal Link Prediction A Unified Framework, Tax.pdf;/Users/luke/Zotero/storage/3WG3RIQX/2210.html},
  note = {arXiv:2210.08765}
}

@inproceedings{qiuDeepInfSocialInfluence2018,
  title = {{{DeepInf}}: {{Social Influence Prediction}} with {{Deep Learning}}},
  shorttitle = {{{DeepInf}}},
  booktitle = {Proceedings of the 24th {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} \& {{Data Mining}}},
  author = {Qiu, Jiezhong and Tang, Jian and Ma, Hao and Dong, Yuxiao and Wang, Kuansan and Tang, Jie},
  date = {2018-07-19},
  series = {{{KDD}} '18},
  pages = {2110--2119},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  url = {https://dl.acm.org/doi/10.1145/3219819.3220077},
  urldate = {2024-08-26},
  abstract = {Social and information networking activities such as on Facebook, Twitter, WeChat, and Weibo have become an indispensable part of our everyday life, where we can easily access friends' behaviors and are in turn influenced by them. Consequently, an effective social influence prediction for each user is critical for a variety of applications such as online recommendation and advertising.Conventional social influence prediction approaches typically design various hand-crafted rules to extract user- and network-specific features. However, their effectiveness heavily relies on the knowledge of domain experts. As a result, it is usually difficult to generalize them into different domains. Inspired by the recent success of deep neural networks in a wide range of computing applications, we design an end-to-end framework, DeepInf, to learn users' latent feature representation for predicting social influence. In general, DeepInf takes a user's local network as the input to a graph neural network for learning her latent social representation. We design strategies to incorporate both network structures and user-specific features into convolutional neural and attention networks. Extensive experiments on Open Academic Graph, Twitter, Weibo, and Digg, representing different types of social and information networks, demonstrate that the proposed end-to-end model, DeepInf, significantly outperforms traditional feature engineering-based approaches, suggesting the effectiveness of representation learning for social applications.},
  isbn = {978-1-4503-5552-0},
  file = {/Users/luke/Zotero/storage/X9KMYF7Y/Qiu et al. - 2018 - DeepInf Social Influence Prediction with Deep Lea.pdf}
}

@misc{rakotosaonaNeRFMeshingDistillingNeural2023,
  title = {{{NeRFMeshing}}: {{Distilling Neural Radiance Fields}} into {{Geometrically-Accurate 3D Meshes}}},
  shorttitle = {{{NeRFMeshing}}},
  author = {Rakotosaona, Marie-Julie and Manhardt, Fabian and Arroyo, Diego Martin and Niemeyer, Michael and Kundu, Abhijit and Tombari, Federico},
  date = {2023-03-16},
  eprint = {2303.09431},
  eprinttype = {arXiv},
  eprintclass = {cs.CV},
  url = {http://arxiv.org/abs/2303.09431},
  urldate = {2023-10-22},
  abstract = {With the introduction of Neural Radiance Fields (NeRFs), novel view synthesis has recently made a big leap forward. At the core, NeRF proposes that each 3D point can emit radiance, allowing to conduct view synthesis using differentiable volumetric rendering. While neural radiance fields can accurately represent 3D scenes for computing the image rendering, 3D meshes are still the main scene representation supported by most computer graphics and simulation pipelines, enabling tasks such as real time rendering and physics-based simulations. Obtaining 3D meshes from neural radiance fields still remains an open challenge since NeRFs are optimized for view synthesis, not enforcing an accurate underlying geometry on the radiance field. We thus propose a novel compact and flexible architecture that enables easy 3D surface reconstruction from any NeRF-driven approach. Upon having trained the radiance field, we distill the volumetric 3D representation into a Signed Surface Approximation Network, allowing easy extraction of the 3D mesh and appearance. Our final 3D mesh is physically accurate and can be rendered in real time on an array of devices.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/luke/Zotero/storage/JGN96P5H/Rakotosaona et al. - 2023 - NeRFMeshing Distilling Neural Radiance Fields int.pdf;/Users/luke/Zotero/storage/DYZ2YY94/2303.html},
  note = {arXiv:2303.09431}
}

@article{randObjectiveCriteriaEvaluation1971,
  title = {Objective {{Criteria}} for the {{Evaluation}} of {{Clustering Methods}}},
  author = {Rand, William M.},
  date = {1971-12-01},
  journaltitle = {Journal of the American Statistical Association},
  volume = {66},
  number = {336},
  pages = {846--850},
  publisher = {Taylor \& Francis},
  issn = {0162-1459},
  url = {https://www.tandfonline.com/doi/abs/10.1080/01621459.1971.10482356},
  urldate = {2024-06-01},
  abstract = {Many intuitively appealing methods have been suggested for clustering data, however, interpretation of their results has been hindered by the lack of objective criteria. This article proposes several criteria which isolate specific aspects of the performance of a method, such as its retrieval of inherent structure, its sensitivity to resampling and the stability of its results in the light of new data. These criteria depend on a measure of similarity between two different clusterings of the same set of data; the measure essentially considers how each pair of data points is assigned in each clustering.}
}

@misc{ranjanL2constrainedSoftmaxLoss2017,
  title = {L2-constrained {{Softmax Loss}} for {{Discriminative Face Verification}}},
  author = {Ranjan, Rajeev and Castillo, Carlos D. and Chellappa, Rama},
  date = {2017-06-07},
  eprint = {1703.09507},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1703.09507},
  urldate = {2024-08-13},
  abstract = {In recent years, the performance of face verification systems has significantly improved using deep convolutional neural networks (DCNNs). A typical pipeline for face verification includes training a deep network for subject classification with softmax loss, using the penultimate layer output as the feature descriptor, and generating a cosine similarity score given a pair of face images. The softmax loss function does not optimize the features to have higher similarity score for positive pairs and lower similarity score for negative pairs, which leads to a performance gap. In this paper, we add an L2-constraint to the feature descriptors which restricts them to lie on a hypersphere of a fixed radius. This module can be easily implemented using existing deep learning frameworks. We show that integrating this simple step in the training pipeline significantly boosts the performance of face verification. Specifically, we achieve state-of-the-art results on the challenging IJB-A dataset, achieving True Accept Rate of 0.909 at False Accept Rate 0.0001 on the face verification protocol. Additionally, we achieve state-of-the-art performance on LFW dataset with an accuracy of 99.78\%, and competing performance on YTF dataset with accuracy of 96.08\%.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/luke/Zotero/storage/46CX6JHN/Ranjan et al. - 2017 - L2-constrained Softmax Loss for Discriminative Fac.pdf;/Users/luke/Zotero/storage/3A9YES5T/1703.html},
  note = {arXiv:1703.09507}
}

@inproceedings{raychevPredictingProgramProperties2015,
  title = {Predicting {{Program Properties}} from "{{Big Code}}"},
  booktitle = {Proceedings of the 42nd {{Annual ACM SIGPLAN-SIGACT Symposium}} on {{Principles}} of {{Programming Languages}}},
  author = {Raychev, Veselin and Vechev, Martin and Krause, Andreas},
  date = {2015-01-14},
  series = {{{POPL}} '15},
  pages = {111--124},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  url = {https://doi.org/10.1145/2676726.2677009},
  urldate = {2023-12-19},
  abstract = {We present a new approach for predicting program properties from massive codebases (aka "Big Code"). Our approach first learns a probabilistic model from existing data and then uses this model to predict properties of new, unseen programs. The key idea of our work is to transform the input program into a representation which allows us to phrase the problem of inferring program properties as structured prediction in machine learning. This formulation enables us to leverage powerful probabilistic graphical models such as conditional random fields (CRFs) in order to perform joint prediction of program properties. As an example of our approach, we built a scalable prediction engine called JSNice for solving two kinds of problems in the context of JavaScript: predicting (syntactic) names of identifiers and predicting (semantic) type annotations of variables. Experimentally, JSNice predicts correct names for 63\% of name identifiers and its type annotation predictions are correct in 81\% of the cases. In the first week since its release, JSNice was used by more than 30,000 developers and in only few months has become a popular tool in the JavaScript developer community. By formulating the problem of inferring program properties as structured prediction and showing how to perform both learning and inference in this context, our work opens up new possibilities for attacking a wide range of difficult problems in the context of "Big Code" including invariant generation, decompilation, synthesis and others.},
  isbn = {978-1-4503-3300-9},
  keywords = {big code,closure compiler,conditional random fields,javascript,names,program properties,structured prediction,types}
}

@online{refactoringguruAdapter,
  title = {Adapter},
  author = {{Refactoring Guru}},
  url = {https://refactoring.guru/design-patterns/adapter},
  urldate = {2024-05-09},
  abstract = {Design Patterns are typical solutions to commonly occurring problems in software design. They are blueprints that you can customize to solve a particular design problem in your code.},
  langid = {english},
  organization = {Refactoring Guru},
  file = {/Users/luke/Zotero/storage/QPWT7WBA/adapter.html}
}

@article{relbench,
  title = {Relational deep learning: {{Graph}} representation learning on relational tables},
  author = {{Kexin Huang} and {Rex Ying} and {Rishabh Ranjan} and {Joshua Robinson} and {Jiaxuan You} and {Jure Leskovec} and {Matthias Fey} and {Weihua Hu}},
  date = {2023}
}

@article{rendleBPRBayesianPersonalized2009,
  title = {{{BPR}}: {{Bayesian Personalized Ranking}} from {{Implicit Feedback}}},
  author = {Rendle, Steffen and Freudenthaler, Christoph and Gantner, Zeno and Schmidt-Thieme, Lars},
  date = {2009},
  abstract = {Item recommendation is the task of predicting a personalized ranking on a set of items (e.g. websites, movies, products). In this paper, we investigate the most common scenario with implicit feedback (e.g. clicks, purchases). There are many methods for item recommendation from implicit feedback like matrix factorization (MF) or adaptive knearest-neighbor (kNN). Even though these methods are designed for the item prediction task of personalized ranking, none of them is directly optimized for ranking. In this paper we present a generic optimization criterion BPR-Opt for personalized ranking that is the maximum posterior estimator derived from a Bayesian analysis of the problem. We also provide a generic learning algorithm for optimizing models with respect to BPR-Opt. The learning method is based on stochastic gradient descent with bootstrap sampling. We show how to apply our method to two state-of-the-art recommender models: matrix factorization and adaptive kNN. Our experiments indicate that for the task of personalized ranking our optimization method outperforms the standard learning techniques for MF and kNN. The results show the importance of optimizing models for the right criterion.},
  langid = {english},
  file = {/Users/luke/Zotero/storage/TMNFX6XB/Rendle et al. - 2009 - BPR Bayesian Personalized Ranking from Implicit F.pdf}
}

@inproceedings{ribeiroWhyShouldTrust2016,
  title = {"{{Why Should I Trust You}}?": {{Explaining}} the {{Predictions}} of {{Any Classifier}}},
  shorttitle = {"{{Why Should I Trust You}}?},
  booktitle = {Proceedings of the 22nd {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} and {{Data Mining}}},
  author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
  date = {2016-08-13},
  series = {{{KDD}} '16},
  pages = {1135--1144},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  url = {https://dl.acm.org/doi/10.1145/2939672.2939778},
  urldate = {2024-03-08},
  abstract = {Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one. In this work, we propose LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally varound the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem. We demonstrate the flexibility of these methods by explaining different models for text (e.g. random forests) and image classification (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classifier, and identifying why a classifier should not be trusted.},
  isbn = {978-1-4503-4232-2},
  keywords = {black box classifier,explaining machine learning,interpretability,interpretable machine learning},
  file = {/Users/luke/Zotero/storage/QTKMD6YY/Ribeiro et al. - 2016 - Why Should I Trust You Explaining the Predicti.pdf}
}

@inproceedings{rosenbergVMeasureConditionalEntropyBased2007,
  title = {V-{{Measure}}: {{A Conditional Entropy-Based External Cluster Evaluation Measure}}},
  shorttitle = {V-{{Measure}}},
  booktitle = {Proceedings of the 2007 {{Joint Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} and {{Computational Natural Language Learning}} ({{EMNLP-CoNLL}})},
  author = {Rosenberg, Andrew and Hirschberg, Julia},
  editor = {Eisner, Jason},
  date = {2007-06},
  pages = {410--420},
  publisher = {Association for Computational Linguistics},
  location = {Prague, Czech Republic},
  url = {https://aclanthology.org/D07-1043},
  urldate = {2024-06-01},
  eventtitle = {{{EMNLP-CoNLL}} 2007},
  file = {/Users/luke/Zotero/storage/MTRDPBN9/Rosenberg and Hirschberg - 2007 - V-Measure A Conditional Entropy-Based External Cl.pdf}
}

@book{rosiakSheafTheoryExamples2022,
  title = {Sheaf theory through examples},
  author = {Rosiak, Daniel},
  date = {2022},
  edition = {1st ed.},
  publisher = {MIT Press},
  location = {Cambridge, Massachusetts},
  abstract = {"This book presents copious and sometimes unexpected examples of sheaf theory, a mathematical tool with promising applications in data science and engineering and in efforts to apply category theory more widely"--},
  isbn = {978-0-262-36237-5},
  langid = {english},
  keywords = {Algebra,General,Logic; MATHEMATICS,Sheaf theory; MATHEMATICS,Topology; MATHEMATICS}
}

@misc{rozemberczkiPyTorchGeometricTemporal2021,
  title = {{{PyTorch Geometric Temporal}}: {{Spatiotemporal Signal Processing}} with {{Neural Machine Learning Models}}},
  shorttitle = {{{PyTorch Geometric Temporal}}},
  author = {Rozemberczki, Benedek and Scherer, Paul and He, Yixuan and Panagopoulos, George and Riedel, Alexander and Astefanoaei, Maria and Kiss, Oliver and Beres, Ferenc and López, Guzmán and Collignon, Nicolas and Sarkar, Rik},
  date = {2021-06-10},
  eprint = {2104.07788},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2104.07788},
  urldate = {2024-02-26},
  abstract = {We present PyTorch Geometric Temporal a deep learning framework combining state-of-the-art machine learning algorithms for neural spatiotemporal signal processing. The main goal of the library is to make temporal geometric deep learning available for researchers and machine learning practitioners in a unified easy-to-use framework. PyTorch Geometric Temporal was created with foundations on existing libraries in the PyTorch eco-system, streamlined neural network layer definitions, temporal snapshot generators for batching, and integrated benchmark datasets. These features are illustrated with a tutorial-like case study. Experiments demonstrate the predictive performance of the models implemented in the library on real world problems such as epidemiological forecasting, ridehail demand prediction and web-traffic management. Our sensitivity analysis of runtime shows that the framework can potentially operate on web-scale datasets with rich temporal features and spatial structure.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Social and Information Networks},
  file = {/Users/luke/Zotero/storage/7HRQ2IK5/Rozemberczki et al. - 2021 - PyTorch Geometric Temporal Spatiotemporal Signal .pdf;/Users/luke/Zotero/storage/DA6XNWWJ/2104.html},
  note = {arXiv:2104.07788}
}

@misc{roziereCodeLlamaOpen2024,
  title = {Code {{Llama}}: {{Open Foundation Models}} for {{Code}}},
  shorttitle = {Code {{Llama}}},
  author = {Rozière, Baptiste and Gehring, Jonas and Gloeckle, Fabian and Sootla, Sten and Gat, Itai and Tan, Xiaoqing Ellen and Adi, Yossi and Liu, Jingyu and Sauvestre, Romain and Remez, Tal and Rapin, Jérémy and Kozhevnikov, Artyom and Evtimov, Ivan and Bitton, Joanna and Bhatt, Manish and Ferrer, Cristian Canton and Grattafiori, Aaron and Xiong, Wenhan and Défossez, Alexandre and Copet, Jade and Azhar, Faisal and Touvron, Hugo and Martin, Louis and Usunier, Nicolas and Scialom, Thomas and Synnaeve, Gabriel},
  date = {2024-01-31},
  eprint = {2308.12950},
  eprinttype = {arXiv},
  eprintclass = {cs.CL},
  url = {http://arxiv.org/abs/2308.12950},
  urldate = {2024-08-27},
  abstract = {We release Code Llama, a family of large language models for code based on Llama 2 providing state-of-the-art performance among open models, infilling capabilities, support for large input contexts, and zero-shot instruction following ability for programming tasks. We provide multiple flavors to cover a wide range of applications: foundation models (Code Llama), Python specializations (Code Llama - Python), and instruction-following models (Code Llama - Instruct) with 7B, 13B, 34B and 70B parameters each. All models are trained on sequences of 16k tokens and show improvements on inputs with up to 100k tokens. 7B, 13B and 70B Code Llama and Code Llama - Instruct variants support infilling based on surrounding content. Code Llama reaches state-of-the-art performance among open models on several code benchmarks, with scores of up to 67\% and 65\% on HumanEval and MBPP, respectively. Notably, Code Llama - Python 7B outperforms Llama 2 70B on HumanEval and MBPP, and all our models outperform every other publicly available model on MultiPL-E. We release Code Llama under a permissive license that allows for both research and commercial use.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/luke/Zotero/storage/GYJVJBHF/Rozière et al. - 2024 - Code Llama Open Foundation Models for Code.pdf;/Users/luke/Zotero/storage/VN98I2C8/2308.html},
  note = {arXiv:2308.12950}
}

@article{ruanExploringComplexHeterogeneous2021,
  title = {Exploring complex and heterogeneous correlations on hypergraph for the prediction of drug-target interactions},
  author = {Ruan, Ding and Ji, Shuyi and Yan, Chenggang and Zhu, Junjie and Zhao, Xibin and Yang, Yuedong and Gao, Yue and Zou, Changqing and Dai, Qionghai},
  date = {2021-12-10},
  journaltitle = {Patterns},
  shortjournal = {Patterns},
  volume = {2},
  number = {12},
  pages = {100390},
  issn = {2666-3899},
  url = {https://www.sciencedirect.com/science/article/pii/S2666389921002579},
  urldate = {2024-04-03},
  abstract = {The continuous emergence of drug-target interaction data provides an opportunity to construct a biological network for systematically discovering unknown interactions. However, this is challenging due to complex and heterogeneous correlations between drug and target. Here, we describe a heterogeneous hypergraph-based framework for drug-target interaction (HHDTI) predictions by modeling biological networks through a hypergraph, where each vertex represents a drug or a target and a hyperedge indicates existing similar interactions or associations between the connected vertices. The hypergraph is then trained to generate suitably structured embeddings for discovering unknown interactions. Comprehensive experiments performed on four public datasets demonstrate that HHDTI achieves significant and consistently improved predictions compared with state-of-the-art methods. Our analysis indicates that this superior performance is due to the ability to integrate heterogeneous high-order information from the hypergraph learning. These results suggest that HHDTI is a scalable and practical tool for uncovering novel drug-target interactions.},
  keywords = {deep learning,drug discovery,hypergraph},
  file = {/Users/luke/Zotero/storage/NB2EHYDV/Ruan et al. - 2021 - Exploring complex and heterogeneous correlations o.pdf}
}

@article{rudinStopExplainingBlack2019,
  title = {Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead},
  author = {Rudin, Cynthia},
  date = {2019-05},
  journaltitle = {Nature Machine Intelligence},
  shortjournal = {Nat Mach Intell},
  volume = {1},
  number = {5},
  pages = {206--215},
  publisher = {Nature Publishing Group},
  issn = {2522-5839},
  url = {https://www.nature.com/articles/s42256-019-0048-x},
  urldate = {2024-03-07},
  abstract = {Black box machine learning models are currently being used for high-stakes decision making throughout society, causing problems in healthcare, criminal justice and other domains. Some people hope that creating methods for explaining these black box models will alleviate some of the problems, but trying to explain black box models, rather than creating models that are interpretable in the first place, is likely to perpetuate bad practice and can potentially cause great harm to society. The way forward is to design models that are inherently interpretable. This Perspective clarifies the chasm between explaining black boxes and using inherently interpretable models, outlines several key reasons why explainable black boxes should be avoided in high-stakes decisions, identifies challenges to interpretable machine learning, and provides several example applications where interpretable models could potentially replace black box models in criminal justice, healthcare and computer vision.},
  langid = {english},
  keywords = {Computer science,Criminology,Science,Statistics,technology and society},
  file = {/Users/luke/Zotero/storage/MEGYY88B/Rudin - 2019 - Stop explaining black box machine learning models .pdf}
}

@misc{ruggeriCommunityDetectionLarge2023,
  title = {Community {{Detection}} in {{Large Hypergraphs}}},
  author = {Ruggeri, Nicolò and Contisciani, Martina and Battiston, Federico and De Bacco, Caterina},
  date = {2023-07-14},
  journaltitle = {Science Advances},
  shortjournal = {Sci. Adv.},
  volume = {9},
  number = {28},
  eprint = {2301.11226},
  eprinttype = {arXiv},
  eprintclass = {physics},
  pages = {eadg9159},
  issn = {2375-2548},
  url = {http://arxiv.org/abs/2301.11226},
  urldate = {2024-04-20},
  abstract = {Hypergraphs, describing networks where interactions take place among any number of units, are a natural tool to model many real-world social and biological systems. In this work we propose a principled framework to model the organization of higher-order data. Our approach recovers community structure with accuracy exceeding that of currently available state-of-the-art algorithms, as tested in synthetic benchmarks with both hard and overlapping ground-truth partitions. Our model is flexible and allows capturing both assortative and disassortative community structures. Moreover, our method scales orders of magnitude faster than competing algorithms, making it suitable for the analysis of very large hypergraphs, containing millions of nodes and interactions among thousands of nodes. Our work constitutes a practical and general tool for hypergraph analysis, broadening our understanding of the organization of real-world higher-order systems.},
  keywords = {Computer Science - Social and Information Networks,Physics - Physics and Society},
  file = {/Users/luke/Zotero/storage/SEWFGTCS/Ruggeri et al. - 2023 - Community Detection in Large Hypergraphs.pdf;/Users/luke/Zotero/storage/X5SQ7MSJ/2301.html},
  note = {arXiv:2301.11226}
}

@misc{saidNeuroGraphBenchmarksGraph2023,
  title = {{{NeuroGraph}}: {{Benchmarks}} for {{Graph Machine Learning}} in {{Brain Connectomics}}},
  shorttitle = {{{NeuroGraph}}},
  author = {Said, Anwar and Bayrak, Roza G. and Derr, Tyler and Shabbir, Mudassir and Moyer, Daniel and Chang, Catie and Koutsoukos, Xenofon},
  date = {2023-11-21},
  eprint = {2306.06202},
  eprinttype = {arXiv},
  eprintclass = {cs, q-bio},
  url = {http://arxiv.org/abs/2306.06202},
  urldate = {2023-12-18},
  abstract = {Machine learning provides a valuable tool for analyzing high-dimensional functional neuroimaging data, and is proving effective in predicting various neurological conditions, psychiatric disorders, and cognitive patterns. In functional magnetic resonance imaging (MRI) research, interactions between brain regions are commonly modeled using graph-based representations. The potency of graph machine learning methods has been established across myriad domains, marking a transformative step in data interpretation and predictive modeling. Yet, despite their promise, the transposition of these techniques to the neuroimaging domain has been challenging due to the expansive number of potential preprocessing pipelines and the large parameter search space for graph-based dataset construction. In this paper, we introduce NeuroGraph, a collection of graph-based neuroimaging datasets, and demonstrated its utility for predicting multiple categories of behavioral and cognitive traits. We delve deeply into the dataset generation search space by crafting 35 datasets that encompass static and dynamic brain connectivity, running in excess of 15 baseline methods for benchmarking. Additionally, we provide generic frameworks for learning on both static and dynamic graphs. Our extensive experiments lead to several key observations. Notably, using correlation vectors as node features, incorporating larger number of regions of interest, and employing sparser graphs lead to improved performance. To foster further advancements in graph-based data driven neuroimaging analysis, we offer a comprehensive open-source Python package that includes the benchmark datasets, baseline implementations, model training, and standard evaluation.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Quantitative Biology - Neurons and Cognition},
  note = {arXiv:2306.06202}
}

@inproceedings{saputraDistillingKnowledgeDeep2019,
  title = {Distilling {{Knowledge From}} a {{Deep Pose Regressor Network}}},
  author = {Saputra, Muhamad Risqi U. and family=Gusmao, given=Pedro P. B., prefix=de, useprefix=false and Almalioglu, Yasin and Markham, Andrew and Trigoni, Niki},
  date = {2019},
  url = {https://openaccess.thecvf.com/content_ICCV_2019/html/Saputra_Distilling_Knowledge_From_a_Deep_Pose_Regressor_Network_ICCV_2019_paper.html},
  urldate = {2023-11-21},
  eventtitle = {{{ICCV}}},
  file = {/Users/luke/Zotero/storage/A2MESZ9N/Saputra et al. - 2019 - Distilling Knowledge From a Deep Pose Regressor Ne.pdf}
}

@article{scarselliGraphNeuralNetwork2009,
  title = {The {{Graph Neural Network Model}}},
  author = {Scarselli, Franco and Gori, Marco and Tsoi, Ah Chung and Hagenbuchner, Markus and Monfardini, Gabriele},
  date = {2009-01},
  journaltitle = {IEEE Transactions on Neural Networks},
  volume = {20},
  number = {1},
  pages = {61--80},
  issn = {1941-0093},
  url = {https://ieeexplore.ieee.org/document/4700287},
  urldate = {2023-10-11},
  abstract = {Many underlying relationships among data in several areas of science and engineering, e.g., computer vision, molecular chemistry, molecular biology, pattern recognition, and data mining, can be represented in terms of graphs. In this paper, we propose a new neural network model, called graph neural network (GNN) model, that extends existing neural network methods for processing the data represented in graph domains. This GNN model, which can directly process most of the practically useful types of graphs, e.g., acyclic, cyclic, directed, and undirected, implements a function tau(G,n) isin IRm that maps a graph G and one of its nodes n into an m-dimensional Euclidean space. A supervised learning algorithm is derived to estimate the parameters of the proposed GNN model. The computational cost of the proposed algorithm is also considered. Some experimental results are shown to validate the proposed learning algorithm, and to demonstrate its generalization capabilities.},
  eventtitle = {{{IEEE Transactions}} on {{Neural Networks}}},
  file = {/Users/luke/Zotero/storage/AJFAVBA3/The_Graph_Neural_Network_Model.pdf}
}

@inproceedings{schlichtkrullInterpretingGraphNeural2021,
  title = {Interpreting {{Graph Neural Networks}} for {{NLP With Differentiable Edge Masking}}},
  author = {Schlichtkrull, Michael Sejr and Cao, Nicola De and Titov, Ivan},
  date = {2021},
  url = {https://openreview.net/forum?id=WznmQa42ZAx},
  urldate = {2024-03-08},
  abstract = {Graph neural networks (GNNs) have become a popular approach to integrating structural inductive biases into NLP models. However, there has been little work on interpreting them, and specifically on understanding which parts of the graphs (e.g. syntactic trees or co-reference structures) contribute to a prediction. In this work, we introduce a post-hoc method for interpreting the predictions of GNNs which identifies unnecessary edges. Given a trained GNN model, we learn a simple classifier that, for every edge in every layer, predicts if that edge can be dropped. We demonstrate that such a classifier can be trained in a fully differentiable fashion, employing stochastic gates and encouraging sparsity through the expected \$L\_0\$ norm. We use our technique as an attribution method to analyze GNN models for two tasks -- question answering and semantic role labeling -- providing insights into the information flow in these models. We show that we can drop a large proportion of edges without deteriorating the performance of the model, while we can analyse the remaining edges for interpreting model predictions.},
  eventtitle = {International {{Conference}} on {{Learning Representations}}},
  langid = {english},
  file = {/Users/luke/Zotero/storage/KVA4MCBY/Schlichtkrull et al. - 2020 - Interpreting Graph Neural Networks for NLP With Di.pdf}
}

@misc{schlichtkrullModelingRelationalData2017,
  title = {Modeling {{Relational Data}} with {{Graph Convolutional Networks}}},
  author = {Schlichtkrull, Michael and Kipf, Thomas N. and Bloem, Peter and family=Berg, given=Rianne, prefix=van den, useprefix=false and Titov, Ivan and Welling, Max},
  date = {2017-10-26},
  eprint = {1703.06103},
  eprinttype = {arXiv},
  eprintclass = {stat.ML},
  url = {http://arxiv.org/abs/1703.06103},
  urldate = {2023-10-30},
  abstract = {Knowledge graphs enable a wide variety of applications, including question answering and information retrieval. Despite the great effort invested in their creation and maintenance, even the largest (e.g., Yago, DBPedia or Wikidata) remain incomplete. We introduce Relational Graph Convolutional Networks (R-GCNs) and apply them to two standard knowledge base completion tasks: Link prediction (recovery of missing facts, i.e. subject-predicate-object triples) and entity classification (recovery of missing entity attributes). R-GCNs are related to a recent class of neural networks operating on graphs, and are developed specifically to deal with the highly multi-relational data characteristic of realistic knowledge bases. We demonstrate the effectiveness of R-GCNs as a stand-alone model for entity classification. We further show that factorization models for link prediction such as DistMult can be significantly improved by enriching them with an encoder model to accumulate evidence over multiple inference steps in the relational graph, demonstrating a large improvement of 29.8\% on FB15k-237 over a decoder-only baseline.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Databases,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/luke/Zotero/storage/IBNJCJJK/Schlichtkrull et al. - 2017 - Modeling Relational Data with Graph Convolutional .pdf;/Users/luke/Zotero/storage/5CH9TP8K/1703.html},
  note = {arXiv:1703.06103}
}

@inproceedings{schlichtkrullModelingRelationalData2018,
  title = {Modeling {{Relational Data}} with {{Graph Convolutional Networks}}},
  booktitle = {The {{Semantic Web}}},
  author = {Schlichtkrull, Michael and Kipf, Thomas N. and Bloem, Peter and family=Berg, given=Rianne, prefix=van~den, useprefix=true and Titov, Ivan and Welling, Max},
  editor = {Gangemi, Aldo and Navigli, Roberto and Vidal, Maria-Esther and Hitzler, Pascal and Troncy, Raphaël and Hollink, Laura and Tordai, Anna and Alam, Mehwish},
  date = {2018},
  pages = {593--607},
  publisher = {Springer International Publishing},
  location = {Cham},
  abstract = {Knowledge graphs enable a wide variety of applications, including question answering and information retrieval. Despite the great effort invested in their creation and maintenance, even the largest (e.g., Yago, DBPedia or Wikidata) remain incomplete. We introduce Relational Graph Convolutional Networks (R-GCNs) and apply them to two standard knowledge base completion tasks: Link prediction (recovery of missing facts, i.e.~subject-predicate-object triples) and entity classification (recovery of missing entity attributes). R-GCNs are related to a recent class of neural networks operating on graphs, and are developed specifically to handle the highly multi-relational data characteristic of realistic knowledge bases. We demonstrate the effectiveness of R-GCNs as a stand-alone model for entity classification. We further show that factorization models for link prediction such as DistMult can be significantly improved through the use of an R-GCN encoder model to accumulate evidence over multiple inference steps in the graph, demonstrating a large improvement of 29.8\% on FB15k-237 over a decoder-only baseline.},
  isbn = {978-3-319-93417-4},
  langid = {english},
  file = {/Users/luke/Zotero/storage/QTWXU7K9/Schlichtkrull et al. - 2018 - Modeling Relational Data with Graph Convolutional .pdf}
}

@article{scoccolaApproximateDiscreteEuclidean2023,
  title = {Approximate and discrete {{Euclidean}} vector bundles},
  author = {Scoccola, Luis and Perea, Jose A.},
  date = {2023-01},
  journaltitle = {Forum of Mathematics, Sigma},
  volume = {11},
  pages = {e20},
  issn = {2050-5094},
  url = {https://www.cambridge.org/core/journals/forum-of-mathematics-sigma/article/approximate-and-discrete-euclidean-vector-bundles/0FD787673265108D500C4D997F661F5C},
  urldate = {2024-04-01},
  abstract = {We introduce  ε\textbackslash varepsilon  -approximate versions of the notion of a Euclidean vector bundle for  ε≥0\textbackslash varepsilon \textbackslash geq 0 , which recover the classical notion of a Euclidean vector bundle when  ε=0\textbackslash varepsilon = 0 . In particular, we study Čech cochains with coefficients in the orthogonal group that satisfy an approximate cocycle condition. We show that  ε\textbackslash varepsilon  -approximate vector bundles can be used to represent classical vector bundles when  ε{$>$}0\textbackslash varepsilon{$>$} 0  is sufficiently small. We also introduce distances between approximate vector bundles and use them to prove that sufficiently similar approximate vector bundles represent the same classical vector bundle. This gives a way of specifying vector bundles over finite simplicial complexes using a finite amount of data and also allows for some tolerance to noise when working with vector bundles in an applied setting. As an example, we prove a reconstruction theorem for vector bundles from finite samples. We give algorithms for the effective computation of low-dimensional characteristic classes of vector bundles directly from discrete and approximate representations and illustrate the usage of these algorithms with computational examples.},
  langid = {english},
  keywords = {55N31,55R99,55U99,68W05},
  file = {/Users/luke/Zotero/storage/HKFDABMV/Scoccola and Perea - 2023 - Approximate and discrete Euclidean vector bundles.pdf}
}

@misc{seleznovaNeuralTangentKernel2023,
  title = {Neural ({{Tangent Kernel}}) {{Collapse}}},
  author = {Seleznova, Mariia and Weitzner, Dana and Giryes, Raja and Kutyniok, Gitta and Chou, Hung-Hsu},
  date = {2023-10-26},
  eprint = {2305.16427},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2305.16427},
  urldate = {2024-02-24},
  abstract = {This work bridges two important concepts: the Neural Tangent Kernel (NTK), which captures the evolution of deep neural networks (DNNs) during training, and the Neural Collapse (NC) phenomenon, which refers to the emergence of symmetry and structure in the last-layer features of well-trained classification DNNs. We adopt the natural assumption that the empirical NTK develops a block structure aligned with the class labels, i.e., samples within the same class have stronger correlations than samples from different classes. Under this assumption, we derive the dynamics of DNNs trained with mean squared (MSE) loss and break them into interpretable phases. Moreover, we identify an invariant that captures the essence of the dynamics, and use it to prove the emergence of NC in DNNs with block-structured NTK. We provide large-scale numerical experiments on three common DNN architectures and three benchmark datasets to support our theory.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/luke/Zotero/storage/QHNIF7PW/Seleznova et al. - 2023 - Neural (Tangent Kernel) Collapse.pdf;/Users/luke/Zotero/storage/R4A8JL8E/2305.html},
  note = {arXiv:2305.16427}
}

@inproceedings{SelfsupervisedHeterogeneousGraph2023a,
  title = {Self-supervised {{Heterogeneous Graph Learning}}: a {{Homogeneity}} and {{Heterogeneity Perspective}}},
  shorttitle = {Self-supervised {{Heterogeneous Graph Learning}}},
  date = {2023-10-13},
  url = {https://openreview.net/forum?id=3FJOKjooIj},
  urldate = {2024-01-29},
  abstract = {Self-supervised heterogeneous graph learning has achieved promising results in various real applications, but it still suffers from the following issues: (i) meta-paths are employed to capture the homogeneity in the heterogeneous graph, but meta-paths are human-defined, requiring substantial expert knowledge and computational costs; and (ii) the heterogeneity in the heterogeneous graph is generally underutilized, leading to the loss of task-related information. To solve these issues, this paper proposes to capture both homogeneity and heterogeneity in the heterogeneous graph without pre-defined meta-paths. Specifically, we propose to learn a self-expressive matrix to capture the homogeneity from the subspace and nearby neighbors. Meanwhile, we propose to capture the heterogeneity by aggregating the information of nodes from different types. We further design a consistency loss and a specificity loss, respectively, to extract the consistent information between homogeneity and heterogeneity and to preserve their specific task-related information. We theoretically analyze that the learned homogeneous representations exhibit the grouping effect to capture the homogeneity, and considering both homogeneity and heterogeneity introduces more task-related information. Extensive experimental results verify the superiority of the proposed method on different downstream tasks.},
  eventtitle = {The {{Twelfth International Conference}} on {{Learning Representations}}},
  langid = {english},
  file = {/Users/luke/Zotero/storage/KDM8MJ8W/2023 - Self-supervised Heterogeneous Graph Learning a Ho.pdf}
}

@inproceedings{selvarajuGradCAMVisualExplanations2017,
  title = {Grad-{{CAM}}: {{Visual Explanations}} from {{Deep Networks}} via {{Gradient-Based Localization}}},
  shorttitle = {Grad-{{CAM}}},
  booktitle = {2017 {{IEEE International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {Selvaraju, Ramprasaath R. and Cogswell, Michael and Das, Abhishek and Vedantam, Ramakrishna and Parikh, Devi and Batra, Dhruv},
  date = {2017-10},
  pages = {618--626},
  issn = {2380-7504},
  url = {https://ieeexplore.ieee.org/document/8237336},
  urldate = {2024-03-08},
  abstract = {We propose a technique for producing `visual explanations' for decisions from a large class of Convolutional Neural Network (CNN)-based models, making them more transparent. Our approach - Gradient-weighted Class Activation Mapping (Grad-CAM), uses the gradients of any target concept (say logits for `dog' or even a caption), flowing into the final convolutional layer to produce a coarse localization map highlighting the important regions in the image for predicting the concept. Unlike previous approaches, Grad- CAM is applicable to a wide variety of CNN model-families: (1) CNNs with fully-connected layers (e.g. VGG), (2) CNNs used for structured outputs (e.g. captioning), (3) CNNs used in tasks with multi-modal inputs (e.g. visual question answering) or reinforcement learning, without architectural changes or re-training. We combine Grad-CAM with existing fine-grained visualizations to create a high-resolution class-discriminative visualization, Guided Grad-CAM, and apply it to image classification, image captioning, and visual question answering (VQA) models, including ResNet-based architectures. In the context of image classification models, our visualizations (a) lend insights into failure modes of these models (showing that seemingly unreasonable predictions have reasonable explanations), (b) outperform previous methods on the ILSVRC-15 weakly-supervised localization task, (c) are more faithful to the underlying model, and (d) help achieve model generalization by identifying dataset bias. For image captioning and VQA, our visualizations show even non-attention based models can localize inputs. Finally, we design and conduct human studies to measure if Grad-CAM explanations help users establish appropriate trust in predictions from deep networks and show that Grad-CAM helps untrained users successfully discern a `stronger' deep network from a `weaker' one even when both make identical predictions. Our code is available at https: //github.com/ramprs/grad-cam/ along with a demo on CloudCV [2] and video at youtu.be/COjUB9Izk6E.},
  eventtitle = {2017 {{IEEE International Conference}} on {{Computer Vision}} ({{ICCV}})},
  keywords = {Cats,Computer architecture,Dogs,Knowledge discovery,Visualization},
  file = {/Users/luke/Zotero/storage/GWGK2VFM/Selvaraju et al. - 2017 - Grad-CAM Visual Explanations from Deep Networks v.pdf;/Users/luke/Zotero/storage/PXEDN2B5/8237336.html}
}

@inproceedings{seoStructuredSequenceModeling2018,
  title = {Structured {{Sequence Modeling}} with {{Graph Convolutional Recurrent Networks}}},
  booktitle = {Neural {{Information Processing}}},
  author = {Seo, Youngjoo and Defferrard, Michaël and Vandergheynst, Pierre and Bresson, Xavier},
  editor = {Cheng, Long and Leung, Andrew Chi Sing and Ozawa, Seiichi},
  date = {2018},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {362--373},
  publisher = {Springer International Publishing},
  location = {Cham},
  abstract = {This paper introduces Graph Convolutional Recurrent Network (GCRN), a deep learning model able to predict structured sequences of data. Precisely, GCRN is a generalization of classical recurrent neural networks (RNN) to data structured by an arbitrary graph. The structured sequences can represent series of frames in videos, spatio-temporal measurements on a network of sensors, or random walks on a vocabulary graph for natural language modeling. The proposed model combines convolutional neural networks (CNN) on graphs to identify spatial structures and RNN to find dynamic patterns. We study two possible architectures of GCRN, and apply the models to two practical problems: predicting moving MNIST data, and modeling natural language with the Penn Treebank dataset. Experiments show that exploiting simultaneously graph spatial and dynamic information about data can improve both precision and learning speed.},
  isbn = {978-3-030-04167-0},
  langid = {english},
  keywords = {Graph neural networks,Language modeling,Recurrent neural networks},
  file = {/Users/luke/Zotero/storage/DTJBGN8V/Seo et al. - 2018 - Structured Sequence Modeling with Graph Convolutio.pdf}
}

@misc{shanTheoryNeuralTangent2022,
  title = {A {{Theory}} of {{Neural Tangent Kernel Alignment}} and {{Its Influence}} on {{Training}}},
  author = {Shan, Haozhe and Bordelon, Blake},
  date = {2022-02-09},
  eprint = {2105.14301},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/2105.14301},
  urldate = {2024-02-20},
  abstract = {The training dynamics and generalization properties of neural networks (NN) can be precisely characterized in function space via the neural tangent kernel (NTK). Structural changes to the NTK during training reflect feature learning and underlie the superior performance of networks outside of the static kernel regime. In this work, we seek to theoretically understand kernel alignment, a prominent and ubiquitous structural change that aligns the NTK with the target function. We first study a toy model of kernel evolution in which the NTK evolves to accelerate training and show that alignment naturally emerges from this demand. We then study alignment mechanism in deep linear networks and two layer ReLU networks. These theories provide good qualitative descriptions of kernel alignment and specialization in practical networks and identify factors in network architecture and data structure that drive kernel alignment. In nonlinear networks with multiple outputs, we identify the phenomenon of kernel specialization, where the kernel function for each output head preferentially aligns to its own target function. Together, our results provide a mechanistic explanation of how kernel alignment emerges during NN training and a normative explanation of how it benefits training.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/luke/Zotero/storage/DR9FC5NL/Shan and Bordelon - 2022 - A Theory of Neural Tangent Kernel Alignment and It.pdf;/Users/luke/Zotero/storage/M662343E/2105.html},
  note = {arXiv:2105.14301}
}

@misc{sharmaSurveyMachineLearning2022,
  title = {A {{Survey}} on {{Machine Learning Techniques}} for {{Source Code Analysis}}},
  author = {Sharma, Tushar and Kechagia, Maria and Georgiou, Stefanos and Tiwari, Rohit and Vats, Indira and Moazen, Hadi and Sarro, Federica},
  date = {2022-09-13},
  eprint = {2110.09610},
  eprinttype = {arXiv},
  eprintclass = {cs.SE},
  url = {http://arxiv.org/abs/2110.09610},
  urldate = {2023-12-18},
  abstract = {The advancements in machine learning techniques have encouraged researchers to apply these techniques to a myriad of software engineering tasks that use source code analysis, such as testing and vulnerability detection. Such a large number of studies hinders the community from understanding the current research landscape. This paper aims to summarize the current knowledge in applied machine learning for source code analysis. We review studies belonging to twelve categories of software engineering tasks and corresponding machine learning techniques, tools, and datasets that have been applied to solve them. To do so, we conducted an extensive literature search and identified 479 primary studies published between 2011 and 2021. We summarize our observations and findings with the help of the identified studies. Our findings suggest that the use of machine learning techniques for source code analysis tasks is consistently increasing. We synthesize commonly used steps and the overall workflow for each task and summarize machine learning techniques employed. We identify a comprehensive list of available datasets and tools useable in this context. Finally, the paper discusses perceived challenges in this area, including the availability of standard datasets, reproducibility and replicability, and hardware resources.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Computer Science - Software Engineering},
  file = {/Users/luke/Zotero/storage/YRDAHTZP/Sharma et al. - 2022 - A Survey on Machine Learning Techniques for Source.pdf;/Users/luke/Zotero/storage/8TZX2C64/2110.html},
  note = {arXiv:2110.09610}
}

@thesis{shepardCellularDescriptionDerived1985,
  title = {A {{Cellular Description}} of the {{Derived Category}} of a {{Stratified Space}}},
  author = {Shepard, Allen Dudley},
  date = {1985},
  institution = {Brown University},
  langid = {english},
  pagetotal = {312}
}

@misc{simonyanDeepConvolutionalNetworks2014,
  title = {Deep {{Inside Convolutional Networks}}: {{Visualising Image Classification Models}} and {{Saliency Maps}}},
  shorttitle = {Deep {{Inside Convolutional Networks}}},
  author = {Simonyan, Karen and Vedaldi, Andrea and Zisserman, Andrew},
  date = {2014-04-19},
  eprint = {1312.6034},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1312.6034},
  urldate = {2024-03-08},
  abstract = {This paper addresses the visualisation of image classification models, learnt using deep Convolutional Networks (ConvNets). We consider two visualisation techniques, based on computing the gradient of the class score with respect to the input image. The first one generates an image, which maximises the class score [Erhan et al., 2009], thus visualising the notion of the class, captured by a ConvNet. The second technique computes a class saliency map, specific to a given image and class. We show that such maps can be employed for weakly supervised object segmentation using classification ConvNets. Finally, we establish the connection between the gradient-based ConvNet visualisation methods and deconvolutional networks [Zeiler et al., 2013].},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/luke/Zotero/storage/AT6ZV84U/Simonyan et al. - 2014 - Deep Inside Convolutional Networks Visualising Im.pdf;/Users/luke/Zotero/storage/IP6P7KL2/1312.html},
  note = {arXiv:1312.6034}
}

@article{singhHigherOrderInteractions2021,
  title = {Higher order interactions and species coexistence},
  author = {Singh, Pragya and Baruah, Gaurav},
  date = {2021-03-01},
  journaltitle = {Theoretical Ecology},
  shortjournal = {Theor Ecol},
  volume = {14},
  number = {1},
  pages = {71--83},
  issn = {1874-1746},
  url = {https://doi.org/10.1007/s12080-020-00481-8},
  urldate = {2024-05-17},
  abstract = {Higher order interactions (HOIs) have been suggested to stabilize diverse ecological communities. However, their role in maintaining species coexistence from the perspective of modern coexistence theory is not known. Here, using generalized Lotka-Volterra model, we derive a general rule for species coexistence modulated by HOIs. We show that where pairwise species interactions fail to promote species coexistence in regions of extreme fitness differences, negative HOIs that intensify pairwise competition, however, can promote coexistence provided that HOIs strengthen intraspecific competition more than interspecific competition. In contrast, positive HOIs that alleviate pairwise competition can stabilize coexistence across a wide range of fitness differences, irrespective of differences in strength of inter- and intraspecific competition. In addition, we extend our three-species analytical result to multispecies communities and show, using simulations, that multispecies coexistence is possible provided that strength of negative intraspecific HOIs is higher than interspecific HOIs. Our work sheds light on the underlying mechanisms through which HOIs can maintain species diversity.},
  langid = {english},
  keywords = {Competition,Higher order interactions,Lotka-Volterra equations,Modern coexistence theory,Pairwise interactions,Species coexistence,Weyl’s inequality},
  file = {/Users/luke/Zotero/storage/UE2399HT/Singh and Baruah - 2021 - Higher order interactions and species coexistence.pdf}
}

@misc{smilkovSmoothGradRemovingNoise2017,
  title = {{{SmoothGrad}}: removing noise by adding noise},
  shorttitle = {{{SmoothGrad}}},
  author = {Smilkov, Daniel and Thorat, Nikhil and Kim, Been and Viégas, Fernanda and Wattenberg, Martin},
  date = {2017-06-12},
  eprint = {1706.03825},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1706.03825},
  urldate = {2024-03-08},
  abstract = {Explaining the output of a deep network remains a challenge. In the case of an image classifier, one type of explanation is to identify pixels that strongly influence the final decision. A starting point for this strategy is the gradient of the class score function with respect to the input image. This gradient can be interpreted as a sensitivity map, and there are several techniques that elaborate on this basic idea. This paper makes two contributions: it introduces SmoothGrad, a simple method that can help visually sharpen gradient-based sensitivity maps, and it discusses lessons in the visualization of these maps. We publish the code for our experiments and a website with our results.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/luke/Zotero/storage/7DV8VPQB/Smilkov et al. - 2017 - SmoothGrad removing noise by adding noise.pdf;/Users/luke/Zotero/storage/7W4R8KDR/1706.html},
  note = {arXiv:1706.03825}
}

@misc{snoekPracticalBayesianOptimization2012,
  title = {Practical {{Bayesian Optimization}} of {{Machine Learning Algorithms}}},
  author = {Snoek, Jasper and Larochelle, Hugo and Adams, Ryan P.},
  date = {2012-08-29},
  eprint = {1206.2944},
  eprinttype = {arXiv},
  eprintclass = {stat.ML},
  url = {http://arxiv.org/abs/1206.2944},
  urldate = {2024-09-06},
  abstract = {Machine learning algorithms frequently require careful tuning of model hyperparameters, regularization terms, and optimization parameters. Unfortunately, this tuning is often a "black art" that requires expert experience, unwritten rules of thumb, or sometimes brute-force search. Much more appealing is the idea of developing automatic approaches which can optimize the performance of a given learning algorithm to the task at hand. In this work, we consider the automatic tuning problem within the framework of Bayesian optimization, in which a learning algorithm's generalization performance is modeled as a sample from a Gaussian process (GP). The tractable posterior distribution induced by the GP leads to efficient use of the information gathered by previous experiments, enabling optimal choices about what parameters to try next. Here we show how the effects of the Gaussian process prior and the associated inference procedure can have a large impact on the success or failure of Bayesian optimization. We show that thoughtful choices can lead to results that exceed expert-level performance in tuning machine learning algorithms. We also describe new algorithms that take into account the variable cost (duration) of learning experiments and that can leverage the presence of multiple cores for parallel experimentation. We show that these proposed algorithms improve on previous automatic procedures and can reach or surpass human expert-level optimization on a diverse set of contemporary algorithms including latent Dirichlet allocation, structured SVMs and convolutional neural networks.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/luke/Zotero/storage/7FTVK45T/Snoek et al. - 2012 - Practical Bayesian Optimization of Machine Learnin.pdf;/Users/luke/Zotero/storage/3NNLTMJZ/1206.html},
  note = {arXiv:1206.2944}
}

@misc{sohoniLowMemoryNeuralNetwork2022,
  title = {Low-{{Memory Neural Network Training}}: {{A Technical Report}}},
  shorttitle = {Low-{{Memory Neural Network Training}}},
  author = {Sohoni, Nimit S. and Aberger, Christopher R. and Leszczynski, Megan and Zhang, Jian and Ré, Christopher},
  date = {2022-04-08},
  eprint = {1904.10631},
  eprinttype = {arXiv},
  eprintclass = {cs.LG},
  url = {http://arxiv.org/abs/1904.10631},
  urldate = {2023-10-19},
  abstract = {Memory is increasingly often the bottleneck when training neural network models. Despite this, techniques to lower the overall memory requirements of training have been less widely studied compared to the extensive literature on reducing the memory requirements of inference. In this paper we study a fundamental question: How much memory is actually needed to train a neural network? To answer this question, we profile the overall memory usage of training on two representative deep learning benchmarks -- the WideResNet model for image classification and the DynamicConv Transformer model for machine translation -- and comprehensively evaluate four standard techniques for reducing the training memory requirements: (1) imposing sparsity on the model, (2) using low precision, (3) microbatching, and (4) gradient checkpointing. We explore how each of these techniques in isolation affects both the peak memory usage of training and the quality of the end model, and explore the memory, accuracy, and computation tradeoffs incurred when combining these techniques. Using appropriate combinations of these techniques, we show that it is possible to the reduce the memory required to train a WideResNet-28-2 on CIFAR-10 by up to 60.7x with a 0.4\% loss in accuracy, and reduce the memory required to train a DynamicConv model on IWSLT'14 German to English translation by up to 8.7x with a BLEU score drop of 0.15.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/luke/Zotero/storage/GMIXJTYP/Sohoni et al. - 2022 - Low-Memory Neural Network Training A Technical Re.pdf;/Users/luke/Zotero/storage/K8558LY9/1904.html},
  note = {arXiv:1904.10631}
}

@article{sperdutiSupervisedNeuralNetworks1997,
  title = {Supervised neural networks for the classification of structures},
  author = {Sperduti, A. and Starita, A.},
  date = {1997-05},
  journaltitle = {IEEE Transactions on Neural Networks},
  volume = {8},
  number = {3},
  pages = {714--735},
  issn = {1941-0093},
  url = {https://ieeexplore.ieee.org/document/572108},
  urldate = {2024-02-23},
  abstract = {Standard neural networks and statistical methods are usually believed to be inadequate when dealing with complex structures because of their feature-based approach. In fact, feature-based approaches usually fail to give satisfactory solutions because of the sensitivity of the approach to the a priori selection of the features, and the incapacity to represent any specific information on the relationships among the components of the structures. However, we show that neural networks can, in fact, represent and classify structured patterns. The key idea underpinning our approach is the use of the so called "generalized recursive neuron", which is essentially a generalization to structures of a recurrent neuron. By using generalized recursive neurons, all the supervised networks developed for the classification of sequences, such as backpropagation through time networks, real-time recurrent networks, simple recurrent networks, recurrent cascade correlation networks, and neural trees can, on the whole, be generalized to structures. The results obtained by some of the above networks (with generalized recursive neurons) on the classification of logic terms are presented.},
  eventtitle = {{{IEEE Transactions}} on {{Neural Networks}}},
  keywords = {Application software,Backpropagation,Medical diagnostic imaging,Neural networks,Neurons,Robustness,Sequences,Speech analysis,Speech processing,Tree graphs},
  file = {/Users/luke/Zotero/storage/9FJLMUL5/Sperduti and Starita - 1997 - Supervised neural networks for the classification .pdf;/Users/luke/Zotero/storage/3GK7TJAW/572108.html}
}

@incollection{sunIntroduction2012,
  title = {Introduction},
  booktitle = {Mining {{Heterogeneous Information Networks}}: {{Principles}} and {{Methodologies}}},
  author = {Sun, Yizhou and Han, Jiawei},
  editor = {Sun, Yizhou and Han, Jiawei},
  date = {2012},
  pages = {1--10},
  publisher = {Springer International Publishing},
  location = {Cham},
  url = {https://doi.org/10.1007/978-3-031-01902-9_1},
  urldate = {2024-06-28},
  abstract = {We are living in an interconnected world. Most of data or informational objects, individual agents, groups, or components are interconnected or interact with each other, forming numerous, large, interconnected, and sophisticated networks. Without loss of generality, such interconnected networks are called information networks in this book. Examples of information networks include social networks, the World Wide Web, research publication networks [22], biological networks [55], highway networks [32], public health systems, electrical power grids, and so on. Clearly, information networks are ubiquitous and form a critical component of modern information infrastructure. The analysis of information networks, or their special kinds, such as social networks and the Web, has gained extremely wide attentions nowadays from researchers in computer science, social science, physics, economics, biology, and so on, with exciting discoveries and successful applications across all the disciplines.},
  isbn = {978-3-031-01902-9},
  langid = {english},
  file = {/Users/luke/Zotero/storage/REU9P4K4/Sun and Han - 2012 - Introduction.pdf}
}

@book{sunMiningHeterogeneousInformation2012,
  title = {Mining {{Heterogeneous Information Networks}}: {{Principles}} and {{Methodologies}}},
  shorttitle = {Mining {{Heterogeneous Information Networks}}},
  author = {Sun, Yizhou and Han, Jiawei},
  date = {2012},
  series = {Synthesis {{Lectures}} on {{Data Mining}} and {{Knowledge Discovery}}},
  publisher = {Springer International Publishing},
  location = {Cham},
  url = {https://link.springer.com/10.1007/978-3-031-01902-9},
  urldate = {2024-07-08},
  isbn = {978-3-031-01902-9},
  langid = {english},
  file = {/Users/luke/Zotero/storage/ES4IQ5BP/Sun and Han - 2012 - Mining Heterogeneous Information Networks Princip.pdf}
}

@article{sunMiningHeterogeneousInformation2013,
  title = {Mining heterogeneous information networks: a structural analysis approach},
  shorttitle = {Mining heterogeneous information networks},
  author = {Sun, Yizhou and Han, Jiawei},
  date = {2013-04-30},
  journaltitle = {ACM SIGKDD Explorations Newsletter},
  shortjournal = {SIGKDD Explor. Newsl.},
  volume = {14},
  number = {2},
  pages = {20--28},
  issn = {1931-0145},
  url = {https://dl.acm.org/doi/10.1145/2481244.2481248},
  urldate = {2023-11-04},
  abstract = {Most objects and data in the real world are of multiple types, interconnected, forming complex, heterogeneous but often semi-structured information networks. However, most network science researchers are focused on homogeneous networks, without distinguishing different types of objects and links in the networks. We view interconnected, multityped data, including the typical relational database data, as heterogeneous information networks, study how to leverage the rich semantic meaning of structural types of objects and links in the networks, and develop a structural analysis approach on mining semi-structured, multi-typed heterogeneous information networks. In this article, we summarize a set of methodologies that can effectively and efficiently mine useful knowledge from such information networks, and point out some promising research directions.},
  file = {/Users/luke/Zotero/storage/9MBW8MZ7/Sun and Han - 2013 - Mining heterogeneous information networks a struc.pdf}
}

@article{sunPathSimMetaPathbased2011,
  title = {{{PathSim}}: meta path-based top-{{K}} similarity search in heterogeneous information networks},
  shorttitle = {{{PathSim}}},
  author = {Sun, Yizhou and Han, Jiawei and Yan, Xifeng and Yu, Philip S. and Wu, Tianyi},
  date = {2011-08-01},
  journaltitle = {Proc. VLDB Endow.},
  volume = {4},
  number = {11},
  pages = {992--1003},
  issn = {2150-8097},
  url = {https://doi.org/10.14778/3402707.3402736},
  urldate = {2024-07-08},
  abstract = {Similarity search is a primitive operation in database and Web search engines. With the advent of large-scale heterogeneous information networks that consist of multi-typed, interconnected objects, such as the bibliographic networks and social media networks, it is important to study similarity search in such networks. Intuitively, two objects are similar if they are linked by many paths in the network. However, most existing similarity measures are defined for homogeneous networks. Different semantic meanings behind paths are not taken into consideration. Thus they cannot be directly applied to heterogeneous networks.In this paper, we study similarity search that is defined among the same type of objects in heterogeneous networks. Moreover, by considering different linkage paths in a network, one could derive various similarity semantics. Therefore, we introduce the concept of meta path-based similarity, where a meta path is a path consisting of a sequence of relations defined between different object types (i.e., structural paths at the meta level). No matter whether a user would like to explicitly specify a path combination given sufficient domain knowledge, or choose the best path by experimental trials, or simply provide training examples to learn it, meta path forms a common base for a network-based similarity search engine. In particular, under the meta path framework we define a novel similarity measure called PathSim that is able to find peer objects in the network (e.g., find authors in the similar field and with similar reputation), which turns out to be more meaningful in many scenarios compared with random-walk based similarity measures. In order to support fast online query processing for PathSim queries, we develop an efficient solution that partially materializes short meta paths and then concatenates them online to compute top-k results. Experiments on real data sets demonstrate the effectiveness and efficiency of our proposed paradigm.},
  file = {/Users/luke/Zotero/storage/JVECG56Z/Sun et al. - 2011 - PathSim meta path-based top-K similarity search i.pdf}
}

@inproceedings{svyatkovskiyIntelliCodeComposeCode2020,
  title = {{{IntelliCode}} compose: code generation using transformer},
  shorttitle = {{{IntelliCode}} compose},
  booktitle = {Proceedings of the 28th {{ACM Joint Meeting}} on {{European Software Engineering Conference}} and {{Symposium}} on the {{Foundations}} of {{Software Engineering}}},
  author = {Svyatkovskiy, Alexey and Deng, Shao Kun and Fu, Shengyu and Sundaresan, Neel},
  date = {2020-11-08},
  series = {{{ESEC}}/{{FSE}} 2020},
  pages = {1433--1443},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  url = {https://doi.org/10.1145/3368089.3417058},
  urldate = {2023-12-19},
  abstract = {In software development through integrated development environments (IDEs), code completion is one of the most widely used features. Nevertheless, majority of integrated development environments only support completion of methods and APIs, or arguments. In this paper, we introduce IntelliCode Compose – a general-purpose multilingual code completion tool which is capable of predicting sequences of code tokens of arbitrary types, generating up to entire lines of syntactically correct code. It leverages state-of-the-art generative transformer model trained on 1.2 billion lines of source code in Python, C\#, JavaScript and TypeScript programming languages. IntelliCode Compose is deployed as a cloud-based web service. It makes use of client-side tree-based caching, efficient parallel implementation of the beam search decoder, and compute graph optimizations to meet edit-time completion suggestion requirements in the Visual Studio Code IDE and Azure Notebook. Our best model yields an average edit similarity of 86.7\% and a perplexity of 1.82 for Python programming language.},
  isbn = {978-1-4503-7043-1},
  keywords = {Code completion,naturalness of software,neural networks},
  file = {/Users/luke/Zotero/storage/GXBF4SU9/Svyatkovskiy et al. - 2020 - IntelliCode compose code generation using transfo.pdf}
}

@misc{tabaghiUniversalRepresentationPermutationInvariant2023,
  title = {Universal {{Representation}} of {{Permutation-Invariant Functions}} on {{Vectors}} and {{Tensors}}},
  author = {Tabaghi, Puoya and Wang, Yusu},
  date = {2023-10-20},
  eprint = {2310.13829},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2310.13829},
  urldate = {2024-05-30},
  abstract = {A main object of our study is multiset functions -- that is, permutation-invariant functions over inputs of varying sizes. Deep Sets, proposed by \textbackslash cite\{zaheer2017deep\}, provides a \textbackslash emph\{universal representation\} for continuous multiset functions on scalars via a sum-decomposable model. Restricting the domain of the functions to finite multisets of \$D\$-dimensional vectors, Deep Sets also provides a \textbackslash emph\{universal approximation\} that requires a latent space dimension of \$O(N\textasciicircum D)\$ -- where \$N\$ is an upper bound on the size of input multisets. In this paper, we strengthen this result by proving that universal representation is guaranteed for continuous and discontinuous multiset functions though a latent space dimension of \$O(N\textasciicircum D)\$. We then introduce \textbackslash emph\{identifiable\} multisets for which we can uniquely label their elements using an identifier function, namely, finite-precision vectors are identifiable. Using our analysis on identifiable multisets, we prove that a sum-decomposable model for general continuous multiset functions only requires a latent dimension of \$2DN\$. We further show that both encoder and decoder functions of the model are continuous -- our main contribution to the existing work which lack such a guarantee. Also this provides a significant improvement over the aforementioned \$O(N\textasciicircum D)\$ bound which was derived for universal representation of continuous and discontinuous multiset functions. We then extend our results and provide special sum-decomposition structures to universally represent permutation-invariant tensor functions on identifiable tensors. These families of sum-decomposition models enables us to design deep network architectures and deploy them on a variety of learning tasks on sequences, images, and graphs.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/luke/Zotero/storage/GE29JAAX/Tabaghi and Wang - 2023 - Universal Representation of Permutation-Invariant .pdf;/Users/luke/Zotero/storage/XCMMIR6B/2310.html},
  note = {arXiv:2310.13829}
}

@inproceedings{tancikNerfstudioModularFramework2023,
  title = {Nerfstudio: {{A Modular Framework}} for {{Neural Radiance Field Development}}},
  shorttitle = {Nerfstudio},
  author = {Tancik, Matthew and Weber, Ethan and Ng, Evonne and Li, Ruilong and Yi, Brent and Wang, Terrance and Kristoffersen, Alexander and Austin, Jake and Salahi, Kamyar and Ahuja, Abhik and Mcallister, David and Kerr, Justin and Kanazawa, Angjoo},
  date = {2023-07-23},
  url = {https://dl.acm.org/doi/10.1145/3588432.3591516},
  urldate = {2023-10-20},
  abstract = {Neural Radiance Fields (NeRF) are a rapidly growing area of research with wide-ranging applications in computer vision, graphics, robotics, and more. In order to streamline the development and deployment of NeRF research, we propose a modular PyTorch framework, Nerfstudio. Our framework includes plug-and-play components for implementing NeRF-based methods, which make it easy for researchers and practitioners to incorporate NeRF into their projects. Additionally, the modular design enables support for extensive real-time visualization tools, streamlined pipelines for importing captured in-the-wild data, and tools for exporting to video, point cloud and mesh representations. The modularity of Nerfstudio enables the development of Nerfacto, our method that combines components from recent papers to achieve a balance between speed and quality, while also remaining flexible to future modifications. To promote community-driven development, all associated code and data are made publicly available with open-source licensing.},
  eventtitle = {{{ACM SIGGRAPH}}},
  keywords = {3D reconstruction,computer graphics,computer vision,machine learning,multi-view,neural rendering},
  file = {/Users/luke/Zotero/storage/75RXWKK4/Tancik et al. - 2023 - Nerfstudio A Modular Framework for Neural Radianc.pdf}
}

@inproceedings{tangArnetMinerExtractionMining2008,
  title = {{{ArnetMiner}}: extraction and mining of academic social networks},
  shorttitle = {{{ArnetMiner}}},
  booktitle = {Proceedings of the 14th {{ACM SIGKDD}} international conference on {{Knowledge}} discovery and data mining},
  author = {Tang, Jie and Zhang, Jing and Yao, Limin and Li, Juanzi and Zhang, Li and Su, Zhong},
  date = {2008-08-24},
  series = {{{KDD}} '08},
  pages = {990--998},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  url = {https://dl.acm.org/doi/10.1145/1401890.1402008},
  urldate = {2024-02-13},
  abstract = {This paper addresses several key issues in the ArnetMiner system, which aims at extracting and mining academic social networks. Specifically, the system focuses on: 1) Extracting researcher profiles automatically from the Web; 2) Integrating the publication data into the network from existing digital libraries; 3) Modeling the entire academic network; and 4) Providing search services for the academic network. So far, 448,470 researcher profiles have been extracted using a unified tagging approach. We integrate publications from online Web databases and propose a probabilistic framework to deal with the name ambiguity problem. Furthermore, we propose a unified modeling approach to simultaneously model topical aspects of papers, authors, and publication venues. Search services such as expertise search and people association search have been provided based on the modeling results. In this paper, we describe the architecture and main features of the system. We also present the empirical evaluation of the proposed methods.},
  isbn = {978-1-60558-193-4},
  keywords = {association search,expertise search,information extraction,name disambiguation,social network,topic modeling},
  file = {/Users/luke/Zotero/storage/R5ITGASJ/Tang et al. - 2008 - ArnetMiner extraction and mining of academic soci.pdf}
}

@misc{telyatnikovHypergraphNeuralNetworks2023,
  title = {Hypergraph {{Neural Networks}} through the {{Lens}} of {{Message Passing}}: {{A Common Perspective}} to {{Homophily}} and {{Architecture Design}}},
  shorttitle = {Hypergraph {{Neural Networks}} through the {{Lens}} of {{Message Passing}}},
  author = {Telyatnikov, Lev and Bucarelli, Maria Sofia and Bernardez, Guillermo and Zaghen, Olga and Scardapane, Simone and Lio, Pietro},
  date = {2023-10-11},
  eprint = {2310.07684},
  eprinttype = {arXiv},
  eprintclass = {cs.AI},
  url = {http://arxiv.org/abs/2310.07684},
  urldate = {2023-10-31},
  abstract = {Most of the current hypergraph learning methodologies and benchmarking datasets in the hypergraph realm are obtained by lifting procedures from their graph analogs, simultaneously leading to overshadowing hypergraph network foundations. This paper attempts to confront some pending questions in that regard: Can the concept of homophily play a crucial role in Hypergraph Neural Networks (HGNNs), similar to its significance in graph-based research? Is there room for improving current hypergraph architectures and methodologies? (e.g. by carefully addressing the specific characteristics of higher-order networks) Do existing datasets provide a meaningful benchmark for HGNNs? Diving into the details, this paper proposes a novel conceptualization of homophily in higher-order networks based on a message passing scheme; this approach harmonizes the analytical frameworks of datasets and architectures, offering a unified perspective for exploring and interpreting complex, higher-order network structures and dynamics. Further, we propose MultiSet, a novel message passing framework that redefines HGNNs by allowing hyperedge-dependent node representations, as well as introduce a novel architecture MultiSetMixer that leverages a new hyperedge sampling strategy. Finally, we provide an extensive set of experiments that contextualize our proposals and lead to valuable insights in hypergraph representation learning.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Social and Information Networks},
  file = {/Users/luke/Zotero/storage/NTB7XIF3/Telyatnikov et al. - 2023 - Hypergraph Neural Networks through the Lens of Mes.pdf;/Users/luke/Zotero/storage/ICFRE5GQ/2310.html},
  note = {arXiv:2310.07684}
}

@inproceedings{tianContrastiveRepresentationDistillation2019,
  title = {Contrastive {{Representation Distillation}}},
  author = {Tian, Yonglong and Krishnan, Dilip and Isola, Phillip},
  date = {2019-09-25},
  url = {https://openreview.net/forum?id=SkgpBJrtvS},
  urldate = {2023-10-20},
  abstract = {Often we wish to transfer representational knowledge from one neural network to another. Examples include distilling a large network into a smaller one, transferring knowledge from one sensory modality to a second, or ensembling a collection of models into a single estimator. Knowledge distillation, the standard approach to these problems, minimizes the KL divergence between the probabilistic outputs of a teacher and student network. We demonstrate that this objective ignores important structural knowledge of the teacher network. This motivates an alternative objective by which we train a student to capture significantly more information in the teacher's representation of the data. We formulate this objective as contrastive learning. Experiments demonstrate that our resulting new objective outperforms knowledge distillation on a variety of knowledge transfer tasks, including single model compression, ensemble distillation, and cross-modal transfer. When combined with knowledge distillation, our method sets a state of the art in many transfer tasks, sometimes even outperforming the teacher network.},
  eventtitle = {{{ICLR}}},
  langid = {english},
  file = {/Users/luke/Zotero/storage/WUEXYVFZ/Tian et al. - 2019 - Contrastive Representation Distillation.pdf}
}

@inproceedings{todorikiSemiAutomaticReliableExplanations2021,
  title = {Semi-{{Automatic Reliable Explanations}} for {{Prediction}} in {{Graphs}}},
  booktitle = {2021 {{IEEE}} 11th {{Annual Computing}} and {{Communication Workshop}} and {{Conference}} ({{CCWC}})},
  author = {Todoriki, Masaru and Shingu, Masafumi and Yano, Shotaro and Tolmachev, Arseny and Komikado, Tao and Maruhashi, Koji},
  date = {2021-01},
  pages = {0311--0320},
  url = {https://ieeexplore.ieee.org/abstract/document/9375922},
  urldate = {2024-03-08},
  abstract = {We discuss reliability for local explanations for prediction in graphs. Meaningfully explaining predictions of machine learning models is an open and important research question particularly in relation to human judgement. Objectively evaluating explanation is strongly required to make Artificial Intelligence trusted. Model-agnostic local explanation methods such as LIME have recently emerged and are being widely used for commonly used types of data: tables, texts, and images. A locally linear regression model is constructed using perturbed data generated in the vicinity of the instance to be explained with LIME. However, the creation of adequate perturbed data is not necessarily easy depending on the task and dataset leading to less explainability and instability of explanations. It is more problematic to apply such local explanation methods to graph data because there are difficulties unique to graphs such as the complexity of the structure and the variety of definitions of the distance among them. We propose a local explanation method for graphs originated from LIME and for fundamental synthetic datasets experimentally investigate the characteristics of perturbed data concerning explainability, e.g., “what perturbed data can increase explainability?” or “is there a criterion to determine reliable explanations regarding perturbed data?”. The effect of the following various factors in the explanation process are investigated: a generation method for perturbed data, distance function, dataset, prediction model, and data augmentation due to noise in training. Although the effect of these factors is quite complex, the ratio of the perturbed data that has a different class than the instance has is the most important index for higher explanations independently of the complex effect. We also propose a practical method to semi-automatically determine a reliable explanation with minimum human support using this index. We also evaluate a model-agnostic manner in our proposed method for graph classification tasks and prediction models such as graph convolutional networks (GCNs) or support vector machines (SVM) with graph kernels. The results indicate that the locally linear regression model can work well under specific situations. Moreover, the possibility to obtain better explainability than with the state-of-the-art graph explanation method GNNExplainer is shown as a reference.},
  eventtitle = {2021 {{IEEE}} 11th {{Annual Computing}} and {{Communication Workshop}} and {{Conference}} ({{CCWC}})},
  keywords = {Data models,Explainable Artificial Intelligence,Graph,Indexes,Interpretability,Linear regression,Local Explanation,Machine Learning,Model-Agnostic,Predictive models,Reliability,Support vector machines,Task analysis},
  file = {/Users/luke/Zotero/storage/94GXF6ZI/Todoriki et al. - 2021 - Semi-Automatic Reliable Explanations for Predictio.pdf;/Users/luke/Zotero/storage/C2H5H466/9375922.html}
}

@incollection{tongPredictingPatientOutcomes2022,
  title = {Predicting {{Patient Outcomes}} with~{{Graph Representation Learning}}},
  booktitle = {{{AI}} for {{Disease Surveillance}} and {{Pandemic Intelligence}}: {{Intelligent Disease Detection}} in {{Action}}},
  author = {Tong, Catherine and Rocheteau, Emma and Veličković, Petar and Lane, Nicholas and Liò, Pietro},
  editor = {Shaban-Nejad, Arash and Michalowski, Martin and Bianco, Simone},
  date = {2022},
  series = {Studies in {{Computational Intelligence}}},
  pages = {281--293},
  publisher = {Springer International Publishing},
  location = {Cham},
  url = {https://doi.org/10.1007/978-3-030-93080-6_20},
  urldate = {2023-11-03},
  abstract = {Recent work on predicting patient outcomes in the Intensive Care Unit (ICU) has focused heavily on the physiological time series data, largely ignoring sparse data such as diagnoses and medications. When they are included, they are usually concatenated in the late stages of a model, which may struggle to learn from rarer disease patterns. Instead, we propose a strategy to exploit diagnoses as relational information by connecting similar patients in a graph. To this end, we propose LSTM-GNN for patient outcome prediction tasks: a hybrid model combining Long Short-Term Memory networks (LSTMs) for extracting temporal features and Graph Neural Networks (GNNs) for extracting the patient neighbourhood information. We demonstrate that LSTM-GNNs outperform the LSTM-only baseline on length of stay prediction tasks on the eICU database. More generally, our results indicate that exploiting information from neighbouring patient cases using graph neural networks is a promising research direction, yielding tangible returns in supervised learning performance on Electronic Health Records.},
  isbn = {978-3-030-93080-6},
  langid = {english},
  keywords = {Graph neural networks,Intensive care unit,Length of stay,Mortality,Patient outcome prediction},
  file = {/Users/luke/Zotero/storage/TBGBIMVT/Tong et al. - 2022 - Predicting Patient Outcomes with Graph Representat.pdf}
}

@article{tophamUKAirTraffic2023,
  entrysubtype = {newspaper},
  title = {{{UK}} air traffic control failure: what caused it, and who will have to pay?},
  shorttitle = {{{UK}} air traffic control failure},
  author = {Topham, Gwyn and {correspondent}, Gwyn Topham Transport},
  date = {2023-08-30T14:51:40},
  journaltitle = {The Guardian},
  issn = {0261-3077},
  url = {https://www.theguardian.com/world/2023/aug/30/uk-air-traffic-control-failure-what-caused-it-and-who-will-have-to-pay},
  urldate = {2023-12-23},
  abstract = {We look at what lies behind hundreds of thousands of people having their flights cancelled or delayed},
  journalsubtitle = {World news},
  langid = {british},
  keywords = {Air transport,Airline industry,Business,Consumer rights,Transport,UK news},
  file = {/Users/luke/Zotero/storage/2ZRSMGGR/uk-air-traffic-control-failure-what-caused-it-and-who-will-have-to-pay.html}
}

@article{trinhSolvingOlympiadGeometry2024,
  title = {Solving olympiad geometry without human demonstrations},
  author = {Trinh, Trieu H. and Wu, Yuhuai and Le, Quoc V. and He, He and Luong, Thang},
  date = {2024-01},
  journaltitle = {Nature},
  volume = {625},
  number = {7995},
  pages = {476--482},
  publisher = {Nature Publishing Group},
  issn = {1476-4687},
  url = {https://www.nature.com/articles/s41586-023-06747-5},
  urldate = {2024-03-13},
  abstract = {Proving mathematical theorems at the olympiad level represents a notable milestone in human-level automated reasoning1–4, owing to their reputed difficulty among the world’s best talents in pre-university mathematics. Current machine-learning approaches, however, are not applicable to most mathematical domains owing to the high cost of translating human proofs into machine-verifiable format. The problem is even worse for geometry because of its unique translation challenges1,5, resulting in severe scarcity of training data. We propose AlphaGeometry, a theorem prover for Euclidean plane geometry that sidesteps the need for human demonstrations by synthesizing millions of theorems and proofs across different levels of complexity. AlphaGeometry is a neuro-symbolic system that uses a neural language model, trained from scratch on our large-scale synthetic data, to guide a symbolic deduction engine through infinite branching points in challenging problems. On a test set of 30 latest olympiad-level problems, AlphaGeometry solves 25, outperforming the previous best method that only solves ten problems and approaching the performance of an average International Mathematical Olympiad (IMO) gold medallist. Notably, AlphaGeometry produces human-readable proofs, solves all geometry problems in the IMO 2000 and 2015 under human expert evaluation and discovers a generalized version of a~translated IMO theorem in 2004.},
  langid = {english},
  keywords = {Computational science,Computer science},
  file = {/Users/luke/Zotero/storage/QB8WE8UG/Trinh et al. - 2024 - Solving olympiad geometry without human demonstrat.pdf}
}

@misc{tsitsulinUnsupervisedEmbeddingQuality2023,
  title = {Unsupervised {{Embedding Quality Evaluation}}},
  author = {Tsitsulin, Anton and Munkhoeva, Marina and Perozzi, Bryan},
  date = {2023-07-17},
  eprint = {2305.16562},
  eprinttype = {arXiv},
  eprintclass = {cs.LG},
  url = {http://arxiv.org/abs/2305.16562},
  urldate = {2024-02-19},
  abstract = {Unsupervised learning has recently significantly gained in popularity, especially with deep learning-based approaches. Despite numerous successes and approaching supervised-level performance on a variety of academic benchmarks, it is still hard to train and evaluate SSL models in practice due to the unsupervised nature of the problem. Even with networks trained in a supervised fashion, it is often unclear whether they will perform well when transferred to another domain. Past works are generally limited to assessing the amount of information contained in embeddings, which is most relevant for self-supervised learning of deep neural networks. This works chooses to follow a different approach: can we quantify how easy it is to linearly separate the data in a stable way? We survey the literature and uncover three methods that could be potentially used for evaluating quality of representations. We also introduce one novel method based on recent advances in understanding the high-dimensional geometric structure of self-supervised learning. We conduct extensive experiments and study the properties of these metrics and ones introduced in the previous work. Our results suggest that while there is no free lunch, there are metrics that can robustly estimate embedding quality in an unsupervised way.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/luke/Zotero/storage/K2JKUAZQ/Tsitsulin et al. - 2023 - Unsupervised Embedding Quality Evaluation.pdf;/Users/luke/Zotero/storage/WZGEN96M/2305.html},
  note = {arXiv:2305.16562}
}

@book{tuIntroductionManifolds2008,
  title = {An introduction to manifolds},
  author = {Tu, Loring W.},
  date = {2008},
  series = {Universitext},
  publisher = {Springer},
  location = {New York},
  isbn = {978-0-387-48098-5},
  langid = {english},
  pagetotal = {xv+360},
  keywords = {Manifolds (Mathematics),Textbooks}
}

@misc{ujvaryRethinkingSharpnessAwareMinimization2022,
  title = {Rethinking {{Sharpness-Aware Minimization}} as {{Variational Inference}}},
  author = {Ujváry, Szilvia and Telek, Zsigmond and Kerekes, Anna and Mészáros, Anna and Huszár, Ferenc},
  date = {2022-10-19},
  eprint = {2210.10452},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/2210.10452},
  urldate = {2024-03-05},
  abstract = {Sharpness-aware minimization (SAM) aims to improve the generalisation of gradient-based learning by seeking out flat minima. In this work, we establish connections between SAM and Mean-Field Variational Inference (MFVI) of neural network parameters. We show that both these methods have interpretations as optimizing notions of flatness, and when using the reparametrisation trick, they both boil down to calculating the gradient at a perturbed version of the current mean parameter. This thinking motivates our study of algorithms that combine or interpolate between SAM and MFVI. We evaluate the proposed variational algorithms on several benchmark datasets, and compare their performance to variants of SAM. Taking a broader perspective, our work suggests that SAM-like updates can be used as a drop-in replacement for the reparametrisation trick.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/luke/Zotero/storage/YAWC8A8M/Ujváry et al. - 2022 - Rethinking Sharpness-Aware Minimization as Variati.pdf;/Users/luke/Zotero/storage/U6BJHRJR/2210.html},
  note = {arXiv:2210.10452}
}

@inproceedings{vaswaniAttentionAllYou2017,
  title = {Attention is {{All}} you {{Need}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan and Kaiser, Łukasz and Polosukhin, Illia},
  date = {2017},
  volume = {30},
  publisher = {Curran Associates, Inc.},
  url = {https://proceedings.neurips.cc/paper_files/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html},
  urldate = {2024-03-19},
  abstract = {The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.},
  eventtitle = {{{NIPS}} 2017},
  file = {/Users/luke/Zotero/storage/YC4MH8NW/Vaswani et al. - 2017 - Attention is All you Need.pdf}
}

@inproceedings{velickovicDeepGraphInfomax2018,
  title = {Deep {{Graph Infomax}}},
  author = {Veličković, Petar and Fedus, William and Hamilton, William L. and Liò, Pietro and Bengio, Yoshua and Hjelm, R. Devon},
  date = {2018-09-27},
  url = {https://openreview.net/forum?id=rklz9iAcKQ},
  urldate = {2024-02-19},
  abstract = {We present Deep Graph Infomax (DGI), a general approach for learning node representations within graph-structured data in an unsupervised manner. DGI relies on maximizing mutual information between patch representations and corresponding high-level summaries of graphs---both derived using established graph convolutional network architectures. The learnt patch representations summarize subgraphs centered around nodes of interest, and can thus be reused for downstream node-wise learning tasks. In contrast to most prior approaches to unsupervised learning with GCNs, DGI does not rely on random walk objectives, and is readily applicable to both transductive and inductive learning setups. We demonstrate competitive performance on a variety of node classification benchmarks, which at times even exceeds the performance of supervised learning.},
  eventtitle = {International {{Conference}} on {{Learning Representations}}},
  langid = {english},
  file = {/Users/luke/Zotero/storage/CYV26T4Q/Veličković et al. - 2018 - Deep Graph Infomax.pdf}
}

@inproceedings{velickovicGraphAttentionNetworks2018,
  title = {Graph {{Attention Networks}}},
  author = {Veličković, Petar and Cucurull, Guillem and Casanova, Arantxa and Romero, Adriana and Liò, Pietro and Bengio, Yoshua},
  date = {2018-02-15},
  url = {https://openreview.net/forum?id=rJXMpikCZ},
  urldate = {2023-10-11},
  abstract = {We present graph attention networks (GATs), novel neural network architectures that operate on graph-structured data, leveraging masked self-attentional layers to address the shortcomings of prior methods based on graph convolutions or their approximations. By stacking layers in which nodes are able to attend over their neighborhoods' features, we enable (implicitly) specifying different weights to different nodes in a neighborhood, without requiring any kind of computationally intensive matrix operation (such as inversion) or depending on knowing the graph structure upfront. In this way, we address several key challenges of spectral-based graph neural networks simultaneously, and make our model readily applicable to inductive as well as transductive problems. Our GAT models have achieved or matched state-of-the-art results across four established transductive and inductive graph benchmarks: the Cora, Citeseer and Pubmed citation network datasets, as well as a protein-protein interaction dataset (wherein test graphs remain unseen during training).},
  eventtitle = {{{ICLR}} 2018},
  langid = {english},
  file = {/Users/luke/Zotero/storage/RC5W6WKF/Veličković et al. - 2018 - Graph Attention Networks.pdf}
}

@article{vinasHypergraphFactorizationMultitissue2023,
  title = {Hypergraph factorization for multi-tissue gene expression imputation},
  author = {Viñas, Ramon and Joshi, Chaitanya K. and Georgiev, Dobrik and Lin, Phillip and Dumitrascu, Bianca and Gamazon, Eric R. and Liò, Pietro},
  date = {2023-07},
  journaltitle = {Nature Machine Intelligence},
  shortjournal = {Nat Mach Intell},
  volume = {5},
  number = {7},
  pages = {739--753},
  publisher = {Nature Publishing Group},
  issn = {2522-5839},
  url = {https://www.nature.com/articles/s42256-023-00684-8},
  urldate = {2024-03-07},
  abstract = {Integrating gene expression across tissues and cell types is crucial for understanding the coordinated biological mechanisms that drive disease and characterize homoeostasis. However, traditional multi-tissue integration methods either cannot handle uncollected tissues or rely on genotype information, which is often unavailable and subject to privacy concerns. Here we present HYFA (hypergraph factorization), a parameter-efficient graph representation learning approach for joint imputation of multi-tissue and cell-type gene expression. HYFA is genotype agnostic, supports a variable number of collected tissues per individual, and imposes strong inductive biases to leverage the shared regulatory architecture of tissues and genes. In performance comparison on Genotype–Tissue Expression project data, HYFA achieves superior performance over existing methods, especially when multiple reference tissues are available. The HYFA-imputed dataset can be used to identify replicable regulatory genetic variations (expression quantitative trait loci), with substantial gains over the original incomplete dataset. HYFA can accelerate the effective and scalable integration of tissue and cell-type transcriptome biorepositories.},
  langid = {english},
  keywords = {Machine learning,Transcriptomics},
  file = {/Users/luke/Zotero/storage/XXV7YMV7/Viñas et al. - 2023 - Hypergraph factorization for multi-tissue gene exp.pdf}
}

@inproceedings{wagstaffLimitationsRepresentingFunctions2019,
  title = {On the {{Limitations}} of {{Representing Functions}} on {{Sets}}},
  booktitle = {Proceedings of the 36th {{International Conference}} on {{Machine Learning}}},
  author = {Wagstaff, Edward and Fuchs, Fabian and Engelcke, Martin and Posner, Ingmar and Osborne, Michael A.},
  date = {2019-05-24},
  pages = {6487--6494},
  publisher = {PMLR},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v97/wagstaff19a.html},
  urldate = {2024-05-08},
  abstract = {Recent work on the representation of functions on sets has considered the use of summation in a latent space to enforce permutation invariance. In particular, it has been conjectured that the dimension of this latent space may remain fixed as the cardinality of the sets under consideration increases. However, we demonstrate that the analysis leading to this conjecture requires mappings which are highly discontinuous and argue that this is only of limited practical use. Motivated by this observation, we prove that an implementation of this model via continuous mappings (as provided by e.g. neural networks or Gaussian processes) actually imposes a constraint on the dimensionality of the latent space. Practical universal function representation for set inputs can only be achieved with a latent dimension at least the size of the maximum number of input elements.},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english},
  file = {/Users/luke/Zotero/storage/42AW9TQZ/Wagstaff et al. - 2019 - On the Limitations of Representing Functions on Se.pdf}
}

@misc{wangDeepGraphLibrary2020,
  title = {Deep {{Graph Library}}: {{A Graph-Centric}}, {{Highly-Performant Package}} for {{Graph Neural Networks}}},
  shorttitle = {Deep {{Graph Library}}},
  author = {Wang, Minjie and Zheng, Da and Ye, Zihao and Gan, Quan and Li, Mufei and Song, Xiang and Zhou, Jinjing and Ma, Chao and Yu, Lingfan and Gai, Yu and Xiao, Tianjun and He, Tong and Karypis, George and Li, Jinyang and Zhang, Zheng},
  date = {2020-08-25},
  eprint = {1909.01315},
  eprinttype = {arXiv},
  eprintclass = {cs.LG},
  url = {http://arxiv.org/abs/1909.01315},
  urldate = {2023-12-20},
  abstract = {Advancing research in the emerging field of deep graph learning requires new tools to support tensor computation over graphs. In this paper, we present the design principles and implementation of Deep Graph Library (DGL). DGL distills the computational patterns of GNNs into a few generalized sparse tensor operations suitable for extensive parallelization. By advocating graph as the central programming abstraction, DGL can perform optimizations transparently. By cautiously adopting a framework-neutral design, DGL allows users to easily port and leverage the existing components across multiple deep learning frameworks. Our evaluation shows that DGL significantly outperforms other popular GNN-oriented frameworks in both speed and memory consumption over a variety of benchmarks and has little overhead for small scale workloads.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/luke/Zotero/storage/BDV2NLYL/Wang et al. - 2020 - Deep Graph Library A Graph-Centric, Highly-Perfor.pdf;/Users/luke/Zotero/storage/TPHZGYU7/1909.html},
  note = {arXiv:1909.01315}
}

@misc{wangDisenHANDisentangledHeterogeneous2020,
  title = {{{DisenHAN}}: {{Disentangled Heterogeneous Graph Attention Network}} for {{Recommendation}}},
  shorttitle = {{{DisenHAN}}},
  booktitle = {Proceedings of the 29th {{ACM International Conference}} on {{Information}} \& {{Knowledge Management}}},
  author = {Wang, Yifan and Tang, Suyao and Lei, Yuntong and Song, Weiping and Wang, Sheng and Zhang, Ming},
  date = {2020-10-19},
  eprint = {2106.10879},
  eprinttype = {arXiv},
  eprintclass = {cs},
  pages = {1605--1614},
  url = {http://arxiv.org/abs/2106.10879},
  urldate = {2024-07-05},
  abstract = {Heterogeneous information network has been widely used to alleviate sparsity and cold start problems in recommender systems since it can model rich context information in user-item interactions. Graph neural network is able to encode this rich context information through propagation on the graph. However, existing heterogeneous graph neural networks neglect entanglement of the latent factors stemming from different aspects. Moreover, meta paths in existing approaches are simplified as connecting paths or side information between node pairs, overlooking the rich semantic information in the paths. In this paper, we propose a novel disentangled heterogeneous graph attention network DisenHAN for top-\$N\$ recommendation, which learns disentangled user/item representations from different aspects in a heterogeneous information network. In particular, we use meta relations to decompose high-order connectivity between node pairs and propose a disentangled embedding propagation layer which can iteratively identify the major aspect of meta relations. Our model aggregates corresponding aspect features from each meta relation for the target user/item. With different layers of embedding propagation, DisenHAN is able to explicitly capture the collaborative filtering effect semantically. Extensive experiments on three real-world datasets show that DisenHAN consistently outperforms state-of-the-art approaches. We further demonstrate the effectiveness and interpretability of the learned disentangled representations via insightful case studies and visualization.},
  keywords = {Computer Science - Information Retrieval,Computer Science - Machine Learning,Computer Science - Social and Information Networks},
  file = {/Users/luke/Zotero/storage/PMRH8EMT/Wang et al. - 2020 - DisenHAN Disentangled Heterogeneous Graph Attenti.pdf;/Users/luke/Zotero/storage/M66SUVZ3/2106.html},
  note = {arXiv:2106.10879}
}

@inproceedings{wangEfficientSharpnessAwareMinimization2023,
  title = {Efficient {{Sharpness-Aware Minimization}} for {{Molecular Graph Transformer Models}}},
  author = {Wang, Yili and Zhou, Kaixiong and Liu, Ninghao and Wang, Ying and Wang, Xin},
  date = {2023-10-13},
  url = {https://openreview.net/forum?id=Od39h4XQ3Y},
  urldate = {2024-02-19},
  abstract = {Sharpness-aware minimization (SAM) has received increasing attention in computer vision since it can effectively eliminate the sharp local minima from the training trajectory and mitigate generalization degradation. However, SAM requires two sequential gradient computations during the optimization of each step: one to obtain the perturbation gradient and the other to obtain the updating gradient. Compared with the base optimizer (e.g., Adam), SAM doubles the time overhead due to the additional perturbation gradient. By dissecting the theory of SAM and observing the training gradient of the molecular graph transformer, we propose a new algorithm named GraphSAM, which reduces the training cost of SAM and improves the generalization performance of graph transformer models. There are two key factors that contribute to this result: (i) \textbackslash textit\{gradient approximation\}: we use the updating gradient of the previous step to approximate the perturbation gradient at the intermediate steps smoothly (\textbackslash textbf\{increases efficiency\}); (ii) \textbackslash textit\{loss landscape approximation\}: we theoretically prove that the loss landscape of GraphSAM is limited to a small range centered on the expected loss of SAM (\textbackslash textbf\{guarantees generalization performance\}). The extensive experiments on six datasets with different tasks demonstrate the superiority of GraphSAM, especially in optimizing the model update process.},
  eventtitle = {The {{Twelfth International Conference}} on {{Learning Representations}}},
  langid = {english},
  file = {/Users/luke/Zotero/storage/ZKZ7DL3C/2023 - Efficient Sharpness-Aware Minimization for Molecul.pdf}
}

@inproceedings{wangEquivariantHypergraphDiffusion2022,
  title = {Equivariant {{Hypergraph Diffusion Neural Operators}}},
  author = {Wang, Peihao and Yang, Shenghao and Liu, Yunyu and Wang, Zhangyang and Li, Pan},
  date = {2022-09-29},
  url = {https://openreview.net/forum?id=RiTjKoscnNd},
  urldate = {2024-05-08},
  abstract = {Hypergraph neural networks (HNNs) using neural networks to encode hypergraphs provide a promising way to model higher-order relations in data and further solve relevant prediction tasks built upon such higher-order relations. However, higher-order relations in practice contain complex patterns and are often highly irregular. So, it is often challenging to design an HNN that suffices to express those relations while keeping computational efficiency. Inspired by hypergraph diffusion algorithms, this work proposes a new HNN architecture named ED-HNN, which provably approximates any continuous equivariant hypergraph diffusion operators that can model a wide range of higher-order relations. ED-HNN can be implemented efficiently by combining star expansions of hypergraphs with standard message passing neural networks. ED-HNN further shows great superiority in processing heterophilic hypergraphs and constructing deep models. We evaluate ED-HNN for node classification on nine real-world hypergraph datasets. ED-HNN uniformly outperforms the best baselines over these nine datasets and achieves more than 2\%\$\textbackslash uparrow\$ in prediction accuracy over four datasets therein. Our code is available at: https://github.com/Graph-COM/ED-HNN.},
  eventtitle = {The {{Eleventh International Conference}} on {{Learning Representations}}},
  langid = {english},
  file = {/Users/luke/Zotero/storage/N58QRXLN/Wang et al. - 2022 - Equivariant Hypergraph Diffusion Neural Operators.pdf}
}

@misc{wangHeterogeneousGraphAttention2019,
  title = {Heterogeneous {{Graph Attention Network}}},
  author = {Wang, Xiao and Ji, Houye and Shi, Chuan and Wang, Bai and Cui, Peng and Yu, P. and Ye, Yanfang},
  date = {2019-03-18},
  eprint = {1903.07293},
  eprinttype = {arXiv},
  eprintclass = {cs.SI},
  url = {http://arxiv.org/abs/1903.07293},
  urldate = {2023-11-04},
  abstract = {Graph neural network, as a powerful graph representation technique based on deep learning, has shown superior performance and attracted considerable research interest. However, it has not been fully considered in graph neural network for heterogeneous graph which contains different types of nodes and links. The heterogeneity and rich semantic information bring great challenges for designing a graph neural network for heterogeneous graph. Recently, one of the most exciting advancements in deep learning is the attention mechanism, whose great potential has been well demonstrated in various areas. In this paper, we first propose a novel heterogeneous graph neural network based on the hierarchical attention, including node-level and semantic-level attentions. Specifically, the node-level attention aims to learn the importance between a node and its metapath based neighbors, while the semantic-level attention is able to learn the importance of different meta-paths. With the learned importance from both node-level and semantic-level attention, the importance of node and meta-path can be fully considered. Then the proposed model can generate node embedding by aggregating features from meta-path based neighbors in a hierarchical manner. Extensive experimental results on three real-world heterogeneous graphs not only show the superior performance of our proposed model over the state-of-the-arts, but also demonstrate its potentially good interpretability for graph analysis.},
  pubstate = {prepublished},
  keywords = {Computer Science - Social and Information Networks},
  file = {/Users/luke/Zotero/storage/DPUEYSGC/Wang et al. - 2019 - Heterogeneous Graph Attention Network.pdf;/Users/luke/Zotero/storage/VDF8KVVC/1903.html},
  note = {arXiv:1903.07293}
}

@inproceedings{wangHeterogeneousGraphAttention2019a,
  title = {Heterogeneous {{Graph Attention Network}}},
  booktitle = {The {{World Wide Web Conference}}},
  author = {Wang, Xiao and Ji, Houye and Shi, Chuan and Wang, Bai and Ye, Yanfang and Cui, Peng and Yu, Philip S},
  date = {2019-05-13},
  series = {{{WWW}} '19},
  pages = {2022--2032},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  url = {https://doi.org/10.1145/3308558.3313562},
  urldate = {2024-07-05},
  abstract = {Graph neural network, as a powerful graph representation technique based on deep learning, has shown superior performance and attracted considerable research interest. However, it has not been fully considered in graph neural network for heterogeneous graph which contains different types of nodes and links. The heterogeneity and rich semantic information bring great challenges for designing a graph neural network for heterogeneous graph. Recently, one of the most exciting advancements in deep learning is the attention mechanism, whose great potential has been well demonstrated in various areas. In this paper, we first propose a novel heterogeneous graph neural network based on the hierarchical attention, including node-level and semantic-level attentions. Specifically, the node-level attention aims to learn the importance between a node and its meta-path based neighbors, while the semantic-level attention is able to learn the importance of different meta-paths. With the learned importance from both node-level and semantic-level attention, the importance of node and meta-path can be fully considered. Then the proposed model can generate node embedding by aggregating features from meta-path based neighbors in a hierarchical manner. Extensive experimental results on three real-world heterogeneous graphs not only show the superior performance of our proposed model over the state-of-the-arts, but also demonstrate its potentially good interpretability for graph analysis.},
  isbn = {978-1-4503-6674-8},
  file = {/Users/luke/Zotero/storage/953NKF6T/Wang et al. - 2019 - Heterogeneous Graph Attention Network.pdf}
}

@inproceedings{wangR2LDistillingNeural2022,
  title = {{{R2L}}: {{Distilling Neural Radiance Field}} to~{{Neural Light Field}} for~{{Efficient Novel View Synthesis}}},
  shorttitle = {{{R2L}}},
  author = {Wang, Huan and Ren, Jian and Huang, Zeng and Olszewski, Kyle and Chai, Menglei and Fu, Yun and Tulyakov, Sergey},
  editor = {Avidan, Shai and Brostow, Gabriel and Cissé, Moustapha and Farinella, Giovanni Maria and Hassner, Tal},
  date = {2022},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {612--629},
  publisher = {Springer Nature Switzerland},
  location = {Cham},
  abstract = {Recent research explosion on Neural Radiance Field (NeRF) shows the encouraging potential to represent complex scenes with neural networks. One major drawback of NeRF is its prohibitive inference time: Rendering a single pixel requires querying the NeRF network hundreds of times. To resolve it, existing efforts mainly attempt to reduce the number of required sampled points. However, the problem of iterative sampling still exists. On the other hand, Neural Light Field (NeLF) presents a more straightforward representation over NeRF in novel view synthesis – the rendering of a pixel amounts to one single forward pass without ray-marching. In this work, we present a deep residual MLP network (88 layers) to effectively learn the light field. We show the key to successfully learning such a deep NeLF network is to have sufficient data, for which we transfer the knowledge from a pre-trained NeRF model via data distillation. Extensive experiments on both synthetic and real-world scenes show the merits of our method over other counterpart algorithms. On the synthetic scenes, we achieve \$\$26\textbackslash sim 35\textbackslash times \$\$26∼35×FLOPs reduction (per camera ray) and \$\$28\textbackslash sim 31\textbackslash times \$\$28∼31×runtime speedup, meanwhile delivering significantly better (\$\$1.4\textbackslash sim 2.8\$\$1.4∼2.8dB average PSNR improvement) rendering quality than NeRF without any customized parallelism requirement.},
  eventtitle = {{{ECCV}}},
  isbn = {978-3-031-19821-2},
  langid = {english},
  file = {/Users/luke/Zotero/storage/S9G3L9SB/Wang et al. - 2022 - R2L Distilling Neural Radiance Field to Neural Li.pdf}
}

@inproceedings{wangVideosSpaceTimeRegion2018,
  title = {Videos as {{Space-Time Region Graphs}}},
  booktitle = {Computer {{Vision}} – {{ECCV}} 2018},
  author = {Wang, Xiaolong and Gupta, Abhinav},
  editor = {Ferrari, Vittorio and Hebert, Martial and Sminchisescu, Cristian and Weiss, Yair},
  date = {2018},
  pages = {413--431},
  publisher = {Springer International Publishing},
  location = {Cham},
  abstract = {How do humans recognize the action “opening a book”? We argue that there are two important cues: modeling temporal shape dynamics and modeling functional relationships between humans and objects. In this paper, we propose to represent videos as space-time region graphs which capture these two important cues. Our graph nodes are defined by the object region proposals from different frames in a long range video. These nodes are connected by two types of relations: (i) similarity relations capturing the long range dependencies between correlated objects and (ii) spatial-temporal relations capturing the interactions between nearby objects. We perform reasoning on this graph representation via Graph Convolutional Networks. We achieve state-of-the-art results on the Charades and Something-Something datasets. Especially for Charades with complex environments, we obtain a huge \$\$4.4\textbackslash\%\$\$4.4\%gain when our model is applied in complex environments.},
  isbn = {978-3-030-01228-1},
  langid = {english},
  file = {/Users/luke/Zotero/storage/NAWKAL83/Wang and Gupta - 2018 - Videos as Space-Time Region Graphs.pdf}
}

@inproceedings{weiLambdaNetProbabilisticType2019,
  title = {{{LambdaNet}}: {{Probabilistic Type Inference}} using {{Graph Neural Networks}}},
  shorttitle = {{{LambdaNet}}},
  author = {Wei, Jiayi and Goyal, Maruth and Durrett, Greg and Dillig, Isil},
  date = {2019-09-25},
  url = {https://openreview.net/forum?id=Hkx6hANtwH},
  urldate = {2023-12-19},
  abstract = {As gradual typing becomes increasingly popular in languages like Python and TypeScript, there is a growing need to infer type annotations automatically. While type annotations help with tasks like code completion and static error catching, these annotations cannot be fully inferred by compilers and are tedious to annotate by hand. This paper proposes a probabilistic type inference scheme for TypeScript based on a graph neural network. Our approach first uses lightweight source code analysis to generate a program abstraction called a type dependency graph, which links type variables with logical constraints as well as name and usage information. Given this program abstraction, we then use a graph neural network to propagate information between related type variables and eventually make type predictions. Our neural architecture can predict both standard types, like number or string, as well as user-defined types that have not been encountered during training. Our experimental results show that our approach outperforms prior work in this space by 14\% (absolute) on library types, while having the ability to make type predictions that are out of scope for existing techniques.},
  eventtitle = {International {{Conference}} on {{Learning Representations}}},
  langid = {english},
  file = {/Users/luke/Zotero/storage/XR2TMPVE/Wei et al. - 2019 - LambdaNet Probabilistic Type Inference using Grap.pdf}
}

@article{weinerAlzheimerDiseaseNeuroimaging2017,
  title = {The {{Alzheimer}}'s {{Disease Neuroimaging Initiative}} 3: {{Continued}} innovation for clinical trial improvement},
  shorttitle = {The {{Alzheimer}}'s {{Disease Neuroimaging Initiative}} 3},
  author = {Weiner, Michael W. and Veitch, Dallas P. and Aisen, Paul S. and Beckett, Laurel A. and Cairns, Nigel J. and Green, Robert C. and Harvey, Danielle and Jack, Clifford R. and Jagust, William and Morris, John C. and Petersen, Ronald C. and Salazar, Jennifer and Saykin, Andrew J. and Shaw, Leslie M. and Toga, Arthur W. and Trojanowski, John Q. and {Alzheimer's Disease Neuroimaging Initiative}},
  date = {2017-05},
  journaltitle = {Alzheimer's \& Dementia: The Journal of the Alzheimer's Association},
  shortjournal = {Alzheimers Dement},
  volume = {13},
  number = {5},
  eprint = {27931796},
  eprinttype = {pmid},
  pages = {561--571},
  issn = {1552-5279},
  abstract = {INTRODUCTION: The overall goal of the Alzheimer's Disease Neuroimaging Initiative (ADNI) is to validate biomarkers for Alzheimer's disease (AD) clinical trials. ADNI-3, which began on August 1, 2016, is a 5-year renewal of the current ADNI-2 study. METHODS: ADNI-3 will follow current and additional subjects with normal cognition, mild cognitive impairment, and AD using innovative technologies such as tau imaging, magnetic resonance imaging sequences for connectivity analyses, and a highly automated immunoassay platform and mass spectroscopy approach for cerebrospinal fluid biomarker analysis. A Systems Biology/pathway approach will be used to identify genetic factors for subject selection/enrichment. Amyloid positron emission tomography scanning will be standardized using the Centiloid method. The Brain Health Registry will help recruit subjects and monitor subject cognition. RESULTS: Multimodal analyses will provide insight into AD pathophysiology and disease progression. DISCUSSION: ADNI-3 will aim to inform AD treatment trials and facilitate development of AD disease-modifying treatments.},
  langid = {english},
  pmcid = {PMC5536850},
  keywords = {Alzheimer Disease,Alzheimer's disease,Amyloid phenotyping,Biomarkers,Brain,Brain Health Registry,Centiloid method,Clinical trial biomarkers,Clinical Trials as Topic,Disease Progression,Functional connectivity,Humans,Magnetic Resonance Imaging,Neuroimaging,Positron-Emission Tomography,Radionuclide Imaging,Tau imaging},
  file = {/Users/luke/Zotero/storage/CTHV2RTV/Weiner et al. - 2017 - The Alzheimer's Disease Neuroimaging Initiative 3.pdf}
}

@article{wishartDrugBankMajorUpdate2018,
  title = {{{DrugBank}} 5.0: a major update to the {{DrugBank}} database for 2018},
  shorttitle = {{{DrugBank}} 5.0},
  author = {Wishart, David S and Feunang, Yannick D and Guo, An C and Lo, Elvis J and Marcu, Ana and Grant, Jason R and Sajed, Tanvir and Johnson, Daniel and Li, Carin and Sayeeda, Zinat and Assempour, Nazanin and Iynkkaran, Ithayavani and Liu, Yifeng and Maciejewski, Adam and Gale, Nicola and Wilson, Alex and Chin, Lucy and Cummings, Ryan and Le, Diana and Pon, Allison and Knox, Craig and Wilson, Michael},
  date = {2018-01-04},
  journaltitle = {Nucleic Acids Research},
  volume = {46},
  number = {D1},
  pages = {D1074-D1082},
  issn = {0305-1048, 1362-4962},
  url = {http://academic.oup.com/nar/article/46/D1/D1074/4602867},
  urldate = {2024-05-14},
  langid = {english},
  file = {/Users/luke/Zotero/storage/EBCCUB5Q/Wishart et al. - 2018 - DrugBank 5.0 a major update to the DrugBank datab.pdf}
}

@online{wittenbrinkSpotifyTrackNeural2023,
  title = {Spotify {{Track Neural Recommender System}}},
  author = {Wittenbrink, Benjamin},
  date = {2023-05-16T18:27:53},
  url = {https://medium.com/stanford-cs224w/spotify-track-neural-recommender-system-51d266e31e16},
  urldate = {2024-04-03},
  abstract = {By Eva Batelaan, Thomas Brink, and Benjamin Wittenbrink},
  langid = {english},
  organization = {Stanford CS224W GraphML Tutorials},
  file = {/Users/luke/Zotero/storage/VLHBV6W2/spotify-track-neural-recommender-system-51d266e31e16.html}
}

@misc{wolfHuggingFaceTransformersStateoftheart2020,
  title = {{{HuggingFace}}'s {{Transformers}}: {{State-of-the-art Natural Language Processing}}},
  shorttitle = {{{HuggingFace}}'s {{Transformers}}},
  author = {Wolf, Thomas and Debut, Lysandre and Sanh, Victor and Chaumond, Julien and Delangue, Clement and Moi, Anthony and Cistac, Pierric and Rault, Tim and Louf, Rémi and Funtowicz, Morgan and Davison, Joe and Shleifer, Sam and family=Platen, given=Patrick, prefix=von, useprefix=true and Ma, Clara and Jernite, Yacine and Plu, Julien and Xu, Canwen and Scao, Teven Le and Gugger, Sylvain and Drame, Mariama and Lhoest, Quentin and Rush, Alexander M.},
  date = {2020-07-13},
  eprint = {1910.03771},
  eprinttype = {arXiv},
  eprintclass = {cs.CL},
  url = {http://arxiv.org/abs/1910.03771},
  urldate = {2023-12-20},
  abstract = {Recent progress in natural language processing has been driven by advances in both model architecture and model pretraining. Transformer architectures have facilitated building higher-capacity models and pretraining has made it possible to effectively utilize this capacity for a wide variety of tasks. \textbackslash textit\{Transformers\} is an open-source library with the goal of opening up these advances to the wider machine learning community. The library consists of carefully engineered state-of-the art Transformer architectures under a unified API. Backing this library is a curated collection of pretrained models made by and available for the community. \textbackslash textit\{Transformers\} is designed to be extensible by researchers, simple for practitioners, and fast and robust in industrial deployments. The library is available at \textbackslash url\{https://github.com/huggingface/transformers\}.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/luke/Zotero/storage/V4YUGZ46/Wolf et al. - 2020 - HuggingFace's Transformers State-of-the-art Natur.pdf;/Users/luke/Zotero/storage/PC5EHK7E/1910.html},
  note = {arXiv:1910.03771}
}

@inproceedings{wuConnectingDotsMultivariate2020,
  title = {Connecting the {{Dots}}: {{Multivariate Time Series Forecasting}} with {{Graph Neural Networks}}},
  shorttitle = {Connecting the {{Dots}}},
  booktitle = {Proceedings of the 26th {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} \& {{Data Mining}}},
  author = {Wu, Zonghan and Pan, Shirui and Long, Guodong and Jiang, Jing and Chang, Xiaojun and Zhang, Chengqi},
  date = {2020-08-20},
  series = {{{KDD}} '20},
  pages = {753--763},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  url = {https://dl.acm.org/doi/10.1145/3394486.3403118},
  urldate = {2024-02-26},
  abstract = {Modeling multivariate time series has long been a subject that has attracted researchers from a diverse range of fields including economics, finance, and traffic. A basic assumption behind multivariate time series forecasting is that its variables depend on one another but, upon looking closely, it is fair to say that existing methods fail to fully exploit latent spatial dependencies between pairs of variables. In recent years, meanwhile, graph neural networks (GNNs) have shown high capability in handling relational dependencies. GNNs require well-defined graph structures for information propagation which means they cannot be applied directly for multivariate time series where the dependencies are not known in advance. In this paper, we propose a general graph neural network framework designed specifically for multivariate time series data. Our approach automatically extracts the uni-directed relations among variables through a graph learning module, into which external knowledge like variable attributes can be easily integrated. A novel mix-hop propagation layer and a dilated inception layer are further proposed to capture the spatial and temporal dependencies within the time series. The graph learning, graph convolution, and temporal convolution modules are jointly learned in an end-to-end framework. Experimental results show that our proposed model outperforms the state-of-the-art baseline methods on 3 of 4 benchmark datasets and achieves on-par performance with other approaches on two traffic datasets which provide extra structural information.},
  isbn = {978-1-4503-7998-4},
  keywords = {graph neural networks,graph structure learning,multivariate time series forecasting,spatial-temporal graphs},
  file = {/Users/luke/Zotero/storage/JTI3JIHV/Wu et al. - 2020 - Connecting the Dots Multivariate Time Series Fore.pdf}
}

@article{wuGraphNeuralNetworks2022,
  title = {Graph {{Neural Networks}} in {{Recommender Systems}}: {{A Survey}}},
  shorttitle = {Graph {{Neural Networks}} in {{Recommender Systems}}},
  author = {Wu, Shiwen and Sun, Fei and Zhang, Wentao and Xie, Xu and Cui, Bin},
  date = {2022-12-03},
  journaltitle = {ACM Computing Surveys},
  shortjournal = {ACM Comput. Surv.},
  volume = {55},
  number = {5},
  pages = {97:1--97:37},
  issn = {0360-0300},
  url = {https://dl.acm.org/doi/10.1145/3535101},
  urldate = {2023-11-03},
  abstract = {With the explosive growth of online information, recommender systems play a key role to alleviate such information overload. Due to the important application value of recommender systems, there have always been emerging works in this field. In recommender systems, the main challenge is to learn the effective user/item representations from their interactions and side information (if any). Recently, graph neural network (GNN) techniques have been widely utilized in recommender systems since most of the information in recommender systems essentially has graph structure and GNN has superiority in graph representation learning. This article aims to provide a comprehensive review of recent research efforts on GNN-based recommender systems. Specifically, we provide a taxonomy of GNN-based recommendation models according to the types of information used and recommendation tasks. Moreover, we systematically analyze the challenges of applying GNN on different types of data and discuss how existing works in this field address these challenges. Furthermore, we state new perspectives pertaining to the development of this field. We collect the representative papers along with their open-source implementations in https://github.com/wusw14/GNN-in-RS.},
  keywords = {graph neural network,Recommender system,survey},
  file = {/Users/luke/Zotero/storage/8CDVBLUD/Wu et al. - 2022 - Graph Neural Networks in Recommender Systems A Su.pdf}
}

@inproceedings{xieS3IMStochasticStructural2023,
  title = {{{S3IM}}: {{Stochastic Structural SIMilarity}} and {{Its Unreasonable Effectiveness}} for {{Neural Fields}}},
  shorttitle = {{{S3IM}}},
  author = {Xie, Zeke and Yang, Xindi and Yang, Yujie and Sun, Qi and Jiang, Yixiang and Wang, Haoran and Cai, Yunfeng and Sun, Mingming},
  date = {2023},
  url = {https://openaccess.thecvf.com/content/ICCV2023/html/Xie_S3IM_Stochastic_Structural_SIMilarity_and_Its_Unreasonable_Effectiveness_for_Neural_ICCV_2023_paper.html},
  urldate = {2023-10-22},
  eventtitle = {{{ICCV}}},
  langid = {english},
  file = {/Users/luke/Zotero/storage/HLH8YSS6/Xie et al. - 2023 - S3IM Stochastic Structural SIMilarity and Its Unr.pdf}
}

@inproceedings{xiongRelevantWalkSearch2023,
  title = {Relevant {{Walk Search}} for {{Explaining Graph Neural Networks}}},
  booktitle = {Proceedings of the 40th {{International Conference}} on {{Machine Learning}}},
  author = {Xiong, Ping and Schnake, Thomas and Gastegger, Michael and Montavon, Grégoire and Muller, Klaus Robert and Nakajima, Shinichi},
  date = {2023-07-03},
  pages = {38301--38324},
  publisher = {PMLR},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v202/xiong23b.html},
  urldate = {2024-03-08},
  abstract = {Graph Neural Networks (GNNs) have become important machine learning tools for graph analysis, and its explainability is crucial for safety, fairness, and robustness. Layer-wise relevance propagation for GNNs (GNN-LRP) evaluates the relevance of walks to reveal important information flows in the network, and provides higher-order explanations, which have been shown to be superior to the lower-order, i.e., node-/edge-level, explanations. However, identifying relevant walks by GNN-LRP requires exponential computational complexity with respect to the network depth, which we will remedy in this paper. Specifically, we propose polynomial-time algorithms for finding top-\$K\$ relevant walks, which drastically reduces the computation and thus increases the applicability of GNN-LRP to large-scale problems. Our proposed algorithms are based on the max-product algorithm—a common tool for finding the maximum likelihood configurations in probabilistic graphical models—and can find the most relevant walks exactly at the neuron level and approximately at the node level. Our experiments demonstrate the performance of our algorithms at scale and their utility across application domains, i.e., on epidemiology, molecular, and natural language benchmarks. We provide our codes under github.com/xiong-ping/rel\_walk\_gnnlrp.},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english},
  file = {/Users/luke/Zotero/storage/UDABD2W3/Xiong et al. - 2023 - Relevant Walk Search for Explaining Graph Neural N.pdf}
}

@inproceedings{xuHowPowerfulAre2018,
  title = {How {{Powerful}} are {{Graph Neural Networks}}?},
  author = {Xu, Keyulu and Hu, Weihua and Leskovec, Jure and Jegelka, Stefanie},
  date = {2018-09-27},
  url = {https://openreview.net/forum?id=ryGs6iA5Km},
  urldate = {2024-03-08},
  abstract = {Graph Neural Networks (GNNs) are an effective framework for representation learning of graphs. GNNs follow a neighborhood aggregation scheme, where the representation vector of a node is computed by recursively aggregating and transforming representation vectors of its neighboring nodes. Many GNN variants have been proposed and have achieved state-of-the-art results on both node and graph classification tasks. However, despite GNNs revolutionizing graph representation learning, there is limited understanding of their representational properties and limitations. Here, we present a theoretical framework for analyzing the expressive power of GNNs to capture different graph structures. Our results characterize the discriminative power of popular GNN variants, such as Graph Convolutional Networks and GraphSAGE, and show that they cannot learn to distinguish certain simple graph structures. We then develop a simple architecture that is provably the most expressive among the class of GNNs and is as powerful as the Weisfeiler-Lehman graph isomorphism test. We empirically validate our theoretical findings on a number of graph classification benchmarks, and demonstrate that our model achieves state-of-the-art performance.},
  eventtitle = {International {{Conference}} on {{Learning Representations}}},
  langid = {english},
  file = {/Users/luke/Zotero/storage/EKKR2B4E/Xu et al. - 2018 - How Powerful are Graph Neural Networks.pdf}
}

@inproceedings{xuRepresentationLearningGraphs2018,
  title = {Representation {{Learning}} on {{Graphs}} with {{Jumping Knowledge Networks}}},
  booktitle = {Proceedings of the 35th {{International Conference}} on {{Machine Learning}}},
  author = {Xu, Keyulu and Li, Chengtao and Tian, Yonglong and Sonobe, Tomohiro and Kawarabayashi, Ken-ichi and Jegelka, Stefanie},
  date = {2018-07-03},
  pages = {5453--5462},
  publisher = {PMLR},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v80/xu18c.html},
  urldate = {2024-08-12},
  abstract = {Recent deep learning approaches for representation learning on graphs follow a neighborhood aggregation procedure. We analyze some important properties of these models, and propose a strategy to overcome those. In particular, the range of "neighboring" nodes that a node’s representation draws from strongly depends on the graph structure, analogous to the spread of a random walk. To adapt to local neighborhood properties and tasks, we explore an architecture – jumping knowledge (JK) networks – that flexibly leverages, for each node, different neighborhood ranges to enable better structure-aware representation. In a number of experiments on social, bioinformatics and citation networks, we demonstrate that our model achieves state-of-the-art performance. Furthermore, combining the JK framework with models like Graph Convolutional Networks, GraphSAGE and Graph Attention Networks consistently improves those models’ performance.},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english},
  file = {/Users/luke/Zotero/storage/7KBA754S/Xu et al. - 2018 - Representation Learning on Graphs with Jumping Kno.pdf;/Users/luke/Zotero/storage/XP98BJ8B/Xu et al. - 2018 - Representation Learning on Graphs with Jumping Kno.pdf}
}

@inproceedings{xuRethinkingHigherorderRepresentation2023,
  title = {Rethinking {{Higher-order Representation Learning}} with {{Graph Neural Networks}}},
  author = {Xu, Tuo and Zou, Lei},
  date = {2023-11-25},
  url = {https://openreview.net/forum?id=2OyoYw4InI},
  urldate = {2023-12-04},
  abstract = {In the field of graph machine learning, graph neural networks (GNNs) are promising models for learning graph representations and node representations. However, many GNNs perform poorly on learning higher-order representations such as links due to their limited expressive power. Zhang et al. summarize recent advances in link prediction and propose labeling trick as a common framework for learning node set representations with GNNs. However, their theory is limited to employing an ideally expressive GNN as the backend, and can only justify a limited series of link prediction models. In this paper, we take a further step to study the expressive power of various higher-order representation learning methods. Our analysis begins with showing the inherent symmetry between node labeling and higher-order GNNs, which directly justifies previous labeling trick methods (SEAL, GraIL) and other node labeling methods (ID-GNN, NBFNet), also higher-order GNNs through a unfied framework. Then, we study the utilization of MPNNs for computing representations in these methods, and show the expressive power upper bounds under these situations. After that, we provide a comprehensive analysis about how these previous methods surpass plain GNNs by showing their ability to capture path information. Finally, using the intuitions provided by the analysis, we propose an extremely simple method for link prediction tasks, which we believe could bring insights for designing more complicated and powerful models in the future.},
  eventtitle = {The {{Second Learning}} on {{Graphs Conference}}},
  langid = {english},
  file = {/Users/luke/Zotero/storage/3HREYYUL/Xu and Zou - 2023 - Rethinking Higher-order Representation Learning wi.pdf}
}

@inproceedings{yadatiHyperGCNNewMethod2019,
  title = {{{HyperGCN}}: {{A New Method For Training Graph Convolutional Networks}} on {{Hypergraphs}}},
  shorttitle = {{{HyperGCN}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Yadati, Naganand and Nimishakavi, Madhav and Yadav, Prateek and Nitin, Vikram and Louis, Anand and Talukdar, Partha},
  date = {2019},
  volume = {32},
  publisher = {Curran Associates, Inc.},
  url = {https://proceedings.neurips.cc/paper_files/paper/2019/hash/1efa39bcaec6f3900149160693694536-Abstract.html},
  urldate = {2024-02-26},
  abstract = {In many real-world network datasets such as co-authorship, co-citation, email communication, etc., relationships are complex and go beyond pairwise. Hypergraphs provide a flexible and natural modeling tool to model such complex relationships. The obvious existence of such complex relationships in many real-world networks naturaly motivates the problem of learning with hypergraphs. A popular learning paradigm is hypergraph-based semi-supervised learning (SSL) where the goal is to assign labels to initially unlabeled vertices in a hypergraph. Motivated by the fact that a graph convolutional network (GCN) has been effective for graph-based SSL, we propose HyperGCN, a novel GCN for SSL on attributed hypergraphs. Additionally, we show how HyperGCN can be used as a learning-based approach for combinatorial optimisation on NP-hard hypergraph problems. We demonstrate HyperGCN's effectiveness through detailed experimentation on real-world hypergraphs. We have made HyperGCN's source code available to foster reproducible research.},
  eventtitle = {{{NeurIPS}} 2019},
  file = {/Users/luke/Zotero/storage/T5FLID65/Yadati et al. - 2019 - HyperGCN A New Method For Training Graph Convolut.pdf}
}

@misc{yangEmbeddingEntitiesRelations2015,
  title = {Embedding {{Entities}} and {{Relations}} for {{Learning}} and {{Inference}} in {{Knowledge Bases}}},
  author = {Yang, Bishan and Yih, Wen-tau and He, Xiaodong and Gao, Jianfeng and Deng, Li},
  date = {2015-08-29},
  eprint = {1412.6575},
  eprinttype = {arXiv},
  eprintclass = {cs.CL},
  url = {http://arxiv.org/abs/1412.6575},
  urldate = {2024-08-13},
  abstract = {We consider learning representations of entities and relations in KBs using the neural-embedding approach. We show that most existing models, including NTN (Socher et al., 2013) and TransE (Bordes et al., 2013b), can be generalized under a unified learning framework, where entities are low-dimensional vectors learned from a neural network and relations are bilinear and/or linear mapping functions. Under this framework, we compare a variety of embedding models on the link prediction task. We show that a simple bilinear formulation achieves new state-of-the-art results for the task (achieving a top-10 accuracy of 73.2\% vs. 54.7\% by TransE on Freebase). Furthermore, we introduce a novel approach that utilizes the learned relation embeddings to mine logical rules such as "BornInCity(a,b) and CityInCountry(b,c) ={$>$} Nationality(a,c)". We find that embeddings learned from the bilinear objective are particularly good at capturing relational semantics and that the composition of relations is characterized by matrix multiplication. More interestingly, we demonstrate that our embedding-based rule extraction approach successfully outperforms a state-of-the-art confidence-based rule mining approach in mining Horn rules that involve compositional reasoning.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/luke/Zotero/storage/MPX6F84H/Yang et al. - 2015 - Embedding Entities and Relations for Learning and .pdf;/Users/luke/Zotero/storage/FYSBX922/1412.html},
  note = {arXiv:1412.6575}
}

@article{yangHeterogeneousNetworkRepresentation2022,
  title = {Heterogeneous {{Network Representation Learning}}: {{A Unified Framework With Survey}} and {{Benchmark}}},
  shorttitle = {Heterogeneous {{Network Representation Learning}}},
  author = {Yang, Carl and Xiao, Yuxin and Zhang, Yu and Sun, Yizhou and Han, Jiawei},
  date = {2022-10-01},
  journaltitle = {IEEE Trans. on Knowl. and Data Eng.},
  volume = {34},
  number = {10},
  pages = {4854--4873},
  issn = {1041-4347},
  url = {https://doi.org/10.1109/TKDE.2020.3045924},
  urldate = {2024-07-03},
  abstract = {Since real-world objects and their interactions are often multi-modal and multi-typed, heterogeneous networks have been widely used as a more powerful, realistic, and generic superclass of traditional homogeneous networks (graphs). Meanwhile, representation learning (\&lt;italic\&gt;a.k.a.\&lt;/italic\&gt; embedding) has recently been intensively studied and shown effective for various network mining and analytical tasks. In this work, we aim to provide a unified framework to deeply summarize and evaluate existing research on heterogeneous network embedding (HNE), which includes but goes beyond a normal survey. Since there has already been a broad body of HNE algorithms, as the first contribution of this article, we provide a generic paradigm for the systematic categorization and analysis over the merits of various existing HNE algorithms. Moreover, existing HNE algorithms, though mostly claimed generic, are often evaluated on different datasets. Understandable due to the application favor of HNE, such indirect comparisons largely hinder the proper attribution of improved task performance towards effective data preprocessing and novel technical design, especially considering the various ways possible to construct a heterogeneous network from real-world application data. Therefore, as the second contribution, we create four benchmark datasets with various properties regarding scale, structure, attribute/label availability, and \&lt;italic\&gt;etc\&lt;/italic\&gt;. from different sources, towards handy and fair evaluations of HNE algorithms. As the third contribution, we carefully refactor and amend the implementations and create friendly interfaces for 13 popular HNE algorithms, and provide all-around comparisons among them over multiple tasks and experimental settings. By putting all existing HNE algorithms under a unified framework, we aim to provide a universal reference and guideline for the understanding and development of HNE algorithms. Meanwhile, by open-sourcing all data and code, we envision to serve the community with an ready-to-use benchmark platform to test and compare the performance of existing and future HNE algorithms (\&lt;uri\&gt;https://github.com/yangji9181/HNE\&lt;/uri\&gt;).},
  file = {/Users/luke/Zotero/storage/XHC38MN4/Yang et al. - 2022 - Heterogeneous Network Representation Learning A U.pdf}
}

@inproceedings{yangMultiSageEmpoweringGCN2020,
  title = {{{MultiSage}}: {{Empowering GCN}} with {{Contextualized Multi-Embeddings}} on {{Web-Scale Multipartite Networks}}},
  shorttitle = {{{MultiSage}}},
  booktitle = {Proceedings of the 26th {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} \& {{Data Mining}}},
  author = {Yang, Carl and Pal, Aditya and Zhai, Andrew and Pancha, Nikil and Han, Jiawei and Rosenberg, Charles and Leskovec, Jure},
  date = {2020-08-20},
  series = {{{KDD}} '20},
  pages = {2434--2443},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  url = {https://dl.acm.org/doi/10.1145/3394486.3403293},
  urldate = {2024-08-26},
  abstract = {Graph convolutional networks (GCNs) are a powerful class of graph neural networks. Trained in a semi-supervised end-to-end fashion, GCNs can learn to integrate node features and graph structures to generate high-quality embeddings that can be used for various downstream tasks like search and recommendation. However, existing GCNs mostly work on homogeneous graphs and consider a single embedding for each node, which do not sufficiently model the multi-facet nature and complex interaction of nodes in real-world networks. Here, we present a contextualized GCN engine by modeling the multipartite networks of target nodes and their intermediatecontext nodes that specify the contexts of their interactions. Towards the neighborhood aggregation process, we devise a contextual masking operation at the feature level and a contextual attention mechanism at the node level to achieve interaction contextualization by treating neighboring target nodes based on intermediate context nodes. Consequently, we compute multiple embeddings for target nodes that capture their diverse facets and different interactions during graph convolution, which is useful for fine-grained downstream applications. To enable efficient web-scale training, we build a parallel random walk engine to pre-sample contextualized neighbors, and a Hadoop2-based data provider pipeline to pre-join training data, dynamically reduce multi-GPU training time, and avoid high memory cost. Extensive experiments on the bipartite Pinterest graph and tripartite OAG graph corroborate the advantage of the proposed system.},
  isbn = {978-1-4503-7998-4},
  file = {/Users/luke/Zotero/storage/INN2P65S/Yang et al. - 2020 - MultiSage Empowering GCN with Contextualized Mult.pdf}
}

@inproceedings{yangSessionBasedRecommendationGraph2021,
  title = {Session-{{Based Recommendation}} with {{Graph Neural Networks}} for {{Repeat Consumption}}},
  booktitle = {Proceedings of the 2020 9th {{International Conference}} on {{Computing}} and {{Pattern Recognition}}},
  author = {Yang, Gang and Zhang, Xiaofeng and Li, Yueping},
  date = {2021-01-11},
  series = {{{ICCPR}} '20},
  pages = {519--524},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  url = {https://dl.acm.org/doi/10.1145/3436369.3436454},
  urldate = {2024-08-26},
  abstract = {Session-based recommendation aims to predict user actions only based on anonymous session sequence. The repeat consumption is a common phenomenon in the recommendation scenarios which the items will be clicked repeatedly many times. Existing recommended methods for session-based repeat consumption model a session as sequence and use Markov Chain (MC) or Recurrent Neural Network (RNNs) to generate the representations of items. Although achieved promising results, these models still have the deficiencies in obtaining the correct user vectors and complex transitions of items. On the other hand, the number of unclicked items is very large relative to the items that have been clicked in a session, that it is more difficult to mine information for unclicked items. In this paper, we proposed an improved model named GNN-RepeatNet based on RepeatNet to explicitly model the repeat consumptions in session-based recommendation, by utilizing the graph neural network and multi-layer self-attention. In GNN-RepeatNet, we get accurate item embedding and complex transitions of items via graph neural network (GNN). Then through a repeat-explore mechanism, we compute the probabilities of predicted items in repeat mode and deep-explore mode separately. In addition, the multi-layer self-attention networks is introduced to deeply explore the unclicked items in deep-explore model. Extensive experiments conducted on two real datasets show that GNN-RepeatNet can improve the performance compared to the state-of-the-art methods.},
  isbn = {978-1-4503-8783-5},
  file = {/Users/luke/Zotero/storage/CXXV5NH3/Yang et al. - 2021 - Session-Based Recommendation with Graph Neural Net.pdf}
}

@article{yildirimDrugtargetNetwork2007,
  title = {Drug-target network},
  author = {Yildirim, Muhammed A. and Goh, Kwang-Il and Cusick, Michael E. and Barabási, Albert-László and Vidal, Marc},
  date = {2007-10},
  journaltitle = {Nature Biotechnology},
  shortjournal = {Nat Biotechnol},
  volume = {25},
  number = {10},
  eprint = {17921997},
  eprinttype = {pmid},
  pages = {1119--1126},
  issn = {1087-0156},
  abstract = {The global set of relationships between protein targets of all drugs and all disease-gene products in the human protein-protein interaction or 'interactome' network remains uncharacterized. We built a bipartite graph composed of US Food and Drug Administration-approved drugs and proteins linked by drug-target binary associations. The resulting network connects most drugs into a highly interlinked giant component, with strong local clustering of drugs of similar types according to Anatomical Therapeutic Chemical classification. Topological analyses of this network quantitatively showed an overabundance of 'follow-on' drugs, that is, drugs that target already targeted proteins. By including drugs currently under investigation, we identified a trend toward more functionally diverse targets improving polypharmacology. To analyze the relationships between drug targets and disease-gene products, we measured the shortest distance between both sets of proteins in current models of the human interactome network. Significant differences in distance were found between etiological and palliative drugs. A recent trend toward more rational drug design was observed.},
  langid = {english},
  keywords = {Computer Simulation,Databases Factual,Gene Expression Regulation,Genomics,Humans,Metabolic Networks and Pathways,Models Biological,Signal Transduction,Systems Biology,United States,United States Food and Drug Administration}
}

@inproceedings{yingGNNExplainerGeneratingExplanations2019,
  title = {{{GNNExplainer}}: {{Generating Explanations}} for {{Graph Neural Networks}}},
  shorttitle = {{{GNNExplainer}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Ying, Zhitao and Bourgeois, Dylan and You, Jiaxuan and Zitnik, Marinka and Leskovec, Jure},
  date = {2019},
  volume = {32},
  publisher = {Curran Associates, Inc.},
  url = {https://proceedings.neurips.cc/paper_files/paper/2019/hash/d80b7040b773199015de6d3b4293c8ff-Abstract.html},
  urldate = {2024-03-08},
  abstract = {Graph Neural Networks (GNNs) are a powerful tool for machine learning on graphs.GNNs combine node feature information with the graph structure by  recursively passing neural messages along edges of the input graph. However, incorporating both graph structure and feature information leads to complex models, and explaining predictions made by GNNs remains unsolved. Here we propose GNNExplainer, the first general, model-agnostic approach for providing interpretable explanations for predictions of any GNN-based model on any graph-based machine learning task. Given an instance, GNNExplainer identifies a compact subgraph structure and a small subset of node features that have a crucial role in GNN's prediction.  Further, GNNExplainer  can generate consistent and concise explanations for an entire class of instances. We formulate GNNExplainer as an optimization task that maximizes the mutual information between a GNN's prediction and distribution of possible subgraph structures. Experiments on synthetic and real-world graphs show that our approach can identify important graph structures as well as node features, and outperforms baselines by 17.1\% on average. GNNExplainer  provides a variety of benefits, from the ability to visualize semantically relevant structures to interpretability, to giving insights into errors of faulty GNNs.},
  file = {/Users/luke/Zotero/storage/Y5RYBRQR/Ying et al. - 2019 - GNNExplainer Generating Explanations for Graph Ne.pdf}
}

@inproceedings{yuanExplainabilityGraphNeural2021,
  title = {On {{Explainability}} of {{Graph Neural Networks}} via {{Subgraph Explorations}}},
  booktitle = {Proceedings of the 38th {{International Conference}} on {{Machine Learning}}},
  author = {Yuan, Hao and Yu, Haiyang and Wang, Jie and Li, Kang and Ji, Shuiwang},
  date = {2021-07-01},
  pages = {12241--12252},
  publisher = {PMLR},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v139/yuan21c.html},
  urldate = {2024-03-08},
  abstract = {We consider the problem of explaining the predictions of graph neural networks (GNNs), which otherwise are considered as black boxes. Existing methods invariably focus on explaining the importance of graph nodes or edges but ignore the substructures of graphs, which are more intuitive and human-intelligible. In this work, we propose a novel method, known as SubgraphX, to explain GNNs by identifying important subgraphs. Given a trained GNN model and an input graph, our SubgraphX explains its predictions by efficiently exploring different subgraphs with Monte Carlo tree search. To make the tree search more effective, we propose to use Shapley values as a measure of subgraph importance, which can also capture the interactions among different subgraphs. To expedite computations, we propose efficient approximation schemes to compute Shapley values for graph data. Our work represents the first attempt to explain GNNs via identifying subgraphs explicitly and directly. Experimental results show that our SubgraphX achieves significantly improved explanations, while keeping computations at a reasonable level.},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english},
  file = {/Users/luke/Zotero/storage/DNULNL2F/Yuan et al. - 2021 - On Explainability of Graph Neural Networks via Sub.pdf}
}

@misc{yuanExplainabilityGraphNeural2022,
  title = {Explainability in {{Graph Neural Networks}}: {{A Taxonomic Survey}}},
  shorttitle = {Explainability in {{Graph Neural Networks}}},
  author = {Yuan, Hao and Yu, Haiyang and Gui, Shurui and Ji, Shuiwang},
  date = {2022-07-01},
  eprint = {2012.15445},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2012.15445},
  urldate = {2024-02-25},
  abstract = {Deep learning methods are achieving ever-increasing performance on many artificial intelligence tasks. A major limitation of deep models is that they are not amenable to interpretability. This limitation can be circumvented by developing post hoc techniques to explain the predictions, giving rise to the area of explainability. Recently, explainability of deep models on images and texts has achieved significant progress. In the area of graph data, graph neural networks (GNNs) and their explainability are experiencing rapid developments. However, there is neither a unified treatment of GNN explainability methods, nor a standard benchmark and testbed for evaluations. In this survey, we provide a unified and taxonomic view of current GNN explainability methods. Our unified and taxonomic treatments of this subject shed lights on the commonalities and differences of existing methods and set the stage for further methodological developments. To facilitate evaluations, we generate a set of benchmark graph datasets specifically for GNN explainability. We summarize current datasets and metrics for evaluating GNN explainability. Altogether, this work provides a unified methodological treatment of GNN explainability and a standardized testbed for evaluations.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/luke/Zotero/storage/JXQY575V/Yuan et al. - 2022 - Explainability in Graph Neural Networks A Taxonom.pdf;/Users/luke/Zotero/storage/4VEK5ZL3/2012.html},
  note = {arXiv:2012.15445}
}

@article{yuanInterpretingImageClassifiers2022,
  title = {Interpreting {{Image Classifiers}} by {{Generating Discrete Masks}}},
  author = {Yuan, Hao and Cai, Lei and Hu, Xia and Wang, Jie and Ji, Shuiwang},
  date = {2022-04-01},
  journaltitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {44},
  number = {04},
  pages = {2019--2030},
  publisher = {IEEE Computer Society},
  issn = {0162-8828},
  url = {https://www.computer.org/csdl/journal/tp/2022/04/09214476/1nHNEVsfYTm},
  urldate = {2024-03-08},
  abstract = {Deep models are commonly treated as black-boxes and lack interpretability. Here, we propose a novel approach to interpret deep image classifiers by generating discrete masks. Our method follows the generative adversarial network formalism. The deep model to be interpreted is the discriminator while we train a generator to explain it. The generator is trained to capture discriminative image regions that should convey the same or similar meaning as the original image from the model’s perspective. It produces a probability map from which a discrete mask can be sampled. Then the discriminator is used to measure the quality of the sampled mask and provide feedbacks for updating. Due to the sampling operations, the generator cannot be trained directly by back-propagation. We propose to update it using policy gradient. Furthermore, we propose to incorporate gradients as auxiliary information to reduce the search space and facilitate training. We conduct both quantitative and qualitative experiments on the ILSVRC dataset. Experimental results indicate that our method can provide reasonable explanations for predictions and outperform existing approaches. In addition, our method can pass the model randomization test, indicating that it is reasoning the attribution of network predictions.},
  langid = {english}
}

@inproceedings{yuanXGNNModelLevelExplanations2020,
  title = {{{XGNN}}: {{Towards Model-Level Explanations}} of {{Graph Neural Networks}}},
  shorttitle = {{{XGNN}}},
  booktitle = {Proceedings of the 26th {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} \& {{Data Mining}}},
  author = {Yuan, Hao and Tang, Jiliang and Hu, Xia and Ji, Shuiwang},
  date = {2020-08-20},
  series = {{{KDD}} '20},
  pages = {430--438},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  url = {https://dl.acm.org/doi/10.1145/3394486.3403085},
  urldate = {2024-03-08},
  abstract = {Graphs neural networks (GNNs) learn node features by aggregating and combining neighbor information, which have achieved promising performance on many graph tasks. However, GNNs are mostly treated as black-boxes and lack human intelligible explanations. Thus, they cannot be fully trusted and used in certain application domains if GNN models cannot be explained. In this work, we propose a novel approach, known as XGNN, to interpret GNNs at the model-level. Our approach can provide high-level insights and generic understanding of how GNNs work. In particular, we propose to explain GNNs by training a graph generator so that the generated graph patterns maximize a certain prediction of the model. We formulate the graph generation as a reinforcement learning task, where for each step, the graph generator predicts how to add an edge into the current graph. The graph generator is trained via a policy gradient method based on information from the trained GNNs. In addition, we incorporate several graph rules to encourage the generated graphs to be valid. Experimental results on both synthetic and real-world datasets show that our proposed methods help understand and verify the trained GNNs. Furthermore, our experimental results indicate that the generated graphs can provide guidance on how to improve the trained GNNs.},
  isbn = {978-1-4503-7998-4},
  keywords = {deep learning,graph neural networks,interpretability},
  file = {/Users/luke/Zotero/storage/FUJP7NX3/Yuan et al. - 2020 - XGNN Towards Model-Level Explanations of Graph Ne.pdf}
}

@article{yuFormulatingRepresentingMultiagent2024,
  title = {Formulating and {{Representing Multiagent Systems With Hypergraphs}}},
  author = {Yu, Shuo and Huang, Huafei and Shen, Yanming and Wang, Pengfei and Zhang, Qiang and Sun, Ke and Chen, Honglong},
  date = {2024},
  journaltitle = {IEEE Transactions on Neural Networks and Learning Systems},
  pages = {1--15},
  issn = {2162-2388},
  url = {https://ieeexplore.ieee.org/document/10449457},
  urldate = {2024-03-07},
  abstract = {Graph-learning methods, especially graph neural networks (GNNs), have shown remarkable effectiveness in handling non-Euclidean data and have achieved great success in various scenarios. Existing GNNs are primarily based on message-passing schemes, that is, aggregating information from neighboring nodes. However, the diversity and complexity of complex systems from real-world circumstances are not sufficiently taken into account. In these cases, the individual should be treated as an agent, with the ability to perceive their surroundings and interact with other individuals, rather than just be viewed as nodes in existing graph approaches. Additionally, the pairwise interactions used in existing methods also lack the expressiveness for the higher-order complex relations among multiple agents, thus limiting the performance in various tasks. In this work, we propose a Multiagent Hypergraph Force-learning method dubbed MHGForce. First, we formalize the multiagent system (MAS) and illustrate its connection to graph learning. Then, we propose a generalized multiagent hypergraph-learning framework. In this framework, we integrate message-passing and force-based interactions to devise a pluggable method. The method empowers graph approaches to excel in downstream tasks while effectively maintaining structural information in the representations. Experimental results on the Cora, Citeseer, Cora-CA, Zoo, and NTU2012 datasets in node classification demonstrate the effectiveness and generality of our proposed method. We also discuss the characteristics of the MHGForce and explore its role through parametric analysis and visualization. Finally, we give a discussion, conclude our work, and propose future directions.},
  eventtitle = {{{IEEE Transactions}} on {{Neural Networks}} and {{Learning Systems}}},
  keywords = {Birds,Complex systems,Decision making,Deep learning,graph learning,hypergraph learning,Learning systems,Multi-agent systems,multiagent system (MAS),Optimization,Task analysis},
  file = {/Users/luke/Zotero/storage/YHDB7GF4/Yu et al. - 2024 - Formulating and Representing Multiagent Systems Wi.pdf;/Users/luke/Zotero/storage/52F8MTAI/10449457.html}
}

@inproceedings{yunGraphTransformerNetworks2019,
  title = {Graph {{Transformer Networks}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Yun, Seongjun and Jeong, Minbyul and Kim, Raehyun and Kang, Jaewoo and Kim, Hyunwoo J},
  date = {2019},
  volume = {32},
  publisher = {Curran Associates, Inc.},
  url = {https://proceedings.neurips.cc/paper_files/paper/2019/hash/9d63484abb477c97640154d40595a3bb-Abstract.html},
  urldate = {2024-07-05},
  abstract = {Graph neural networks (GNNs) have been widely used in representation learning on graphs and achieved state-of-the-art performance in tasks such as node classification and link prediction. However, most existing GNNs are designed to learn node representations on the fixed and homogeneous graphs. The limitations especially become problematic when learning representations on a misspecified graph or a heterogeneous graph that consists of various types of nodes and edges. In this paper, we propose Graph Transformer Networks (GTNs) that are capable of generating new graph structures, which involve identifying useful connections between unconnected nodes on the original graph, while learning effective node representation on the new graphs in an end-to-end fashion. Graph Transformer layer, a core layer of GTNs, learns a soft selection of edge types and composite relations for generating useful multi-hop connections so-call meta-paths. Our experiments show that GTNs learn new graph structures, based on data and tasks without domain knowledge, and yield powerful node representation via convolution on the new graphs. Without domain-specific graph preprocessing, GTNs achieved the best performance in all three benchmark node classification tasks against the state-of-the-art methods that require pre-defined meta-paths from domain knowledge.},
  file = {/Users/luke/Zotero/storage/3C7GHKPK/Yun et al. - 2019 - Graph Transformer Networks.pdf}
}

@inproceedings{yuPixelNeRFNeuralRadiance2021,
  title = {{{pixelNeRF}}: {{Neural Radiance Fields}} from {{One}} or {{Few Images}}},
  shorttitle = {{{pixelNeRF}}},
  author = {Yu, Alex and Ye, Vickie and Tancik, Matthew and Kanazawa, Angjoo},
  date = {2021-06},
  issn = {2575-7075},
  url = {https://ieeexplore.ieee.org/document/9577688},
  urldate = {2023-10-20},
  abstract = {We propose pixelNeRF, a learning framework that predicts a continuous neural scene representation conditioned on one or few input images. The existing approach for constructing neural radiance fields [27] involves optimizing the representation to every scene independently, requiring many calibrated views and significant compute time. We take a step towards resolving these shortcomings by introducing an architecture that conditions a NeRF on image inputs in a fully convolutional manner. This allows the network to be trained across multiple scenes to learn a scene prior, enabling it to perform novel view synthesis in a feed-forward manner from a sparse set of views (as few as one). Leveraging the volume rendering approach of NeRF, our model can be trained directly from images with no explicit 3D supervision. We conduct extensive experiments on ShapeNet benchmarks for single image novel view synthesis tasks with held-out objects as well as entire unseen categories. We further demonstrate the flexibility of pixelNeRF by demonstrating it on multi-object ShapeNet scenes and real scenes from the DTU dataset. In all cases, pixelNeRF outperforms current state-of-the-art baselines for novel view synthesis and single image 3D reconstruction. For the video and code, please visit the project website:https://alexyu.net/pixelnerf.},
  eventtitle = {{{CVPR}}},
  file = {/Users/luke/Zotero/storage/BMW7NQD7/Yu et al. - 2021 - pixelNeRF Neural Radiance Fields from One or Few .pdf;/Users/luke/Zotero/storage/UGLBRMVV/9577688.html}
}

@inproceedings{zaheerDeepSets2017,
  title = {Deep {{Sets}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Zaheer, Manzil and Kottur, Satwik and Ravanbakhsh, Siamak and Poczos, Barnabas and Salakhutdinov, Russ R and Smola, Alexander J},
  date = {2017},
  volume = {30},
  publisher = {Curran Associates, Inc.},
  url = {https://papers.nips.cc/paper_files/paper/2017/hash/f22e4747da1aa27e363d86d40ff442fe-Abstract.html},
  urldate = {2024-05-08},
  abstract = {We study the problem of designing models for machine learning tasks defined on sets. In contrast to the traditional approach of operating on fixed dimensional vectors, we consider objective functions defined on sets and are invariant to permutations. Such problems are widespread, ranging from the estimation of population statistics, to anomaly detection in piezometer data of embankment dams, to cosmology. Our main theorem characterizes the permutation invariant objective functions and provides a family of functions to which any permutation invariant objective function must belong. This family of functions has a special structure which enables us to design a deep network architecture that can operate on sets and which can be deployed on a variety of scenarios including both unsupervised and supervised learning tasks. We demonstrate the applicability of our method on population statistic estimation, point cloud classification, set expansion, and outlier detection.},
  file = {/Users/luke/Zotero/storage/AJM7ETCG/Zaheer et al. - 2017 - Deep Sets.pdf}
}

@inproceedings{zeinLocalSystemsConstructible2010,
  title = {Local {{Systems}} and {{Constructible Sheaves}}},
  booktitle = {Arrangements, {{Local Systems}} and {{Singularities}}},
  author = {Zein, Fouad El and Snoussi, Jawad},
  editor = {El Zein, Fouad and Suciu, Alexandru I. and Tosun, Meral and Uludağ, A. Muhammed and Yuzvinsky, Sergey},
  date = {2010},
  pages = {111--153},
  publisher = {Birkhäuser},
  location = {Basel},
  abstract = {The article describes local systems, integrable connections, the equivalence of both categories and their relations to linear differential equations. We report in details on regular singularities of connections and on singularities of local systems which leads to the theory of intermediate extensions and the decomposition theorem.},
  isbn = {978-3-0346-0209-9},
  langid = {english},
  keywords = {32S40,Algebraic geometry,analytic geometry,connections,constructible sheaves,Hard Lefschetz theorem,linear differential equations,local systems,perverse sheaves,Primary 32S60,Secondary 14F40}
}

@article{zengTargetIdentificationKnown2020,
  title = {Target identification among known drugs by deep learning from heterogeneous networks},
  author = {Zeng, Xiangxiang and Zhu, Siyi and Lu, Weiqiang and Liu, Zehui and Huang, Jin and Zhou, Yadi and Fang, Jiansong and Huang, Yin and Guo, Huimin and Li, Lang and Trapp, Bruce D. and Nussinov, Ruth and Eng, Charis and Loscalzo, Joseph and Cheng, Feixiong},
  date = {2020-02-19},
  journaltitle = {Chemical Science},
  shortjournal = {Chemical Science},
  volume = {11},
  number = {7},
  pages = {1775--1797},
  issn = {2041-6520},
  url = {https://www.sciencedirect.com/science/article/pii/S2041652023019715},
  abstract = {ABSTRACT Without foreknowledge of the complete drug target information, development of promising and affordable approaches for effective treatment of human diseases is challenging. Here, we develop deepDTnet, a deep learning methodology for new target identification and drug repurposing in a heterogeneous drug–gene–disease network embedding 15 types of chemical, genomic, phenotypic, and cellular network profiles. Trained on 732 U.S. Food and Drug Administration-approved small molecule drugs, deepDTnet shows high accuracy (the area under the receiver operating characteristic curve = 0.963) in identifying novel molecular targets for known drugs, outperforming previously published state-of-the-art methodologies. We then experimentally validate that deepDTnet-predicted topotecan (an approved topoisomerase inhibitor) is a new, direct inhibitor (IC50 = 0.43 μM) of human retinoic-acid-receptor-related orphan receptor-gamma t (ROR-γt). Furthermore, by specifically targeting ROR-γt, topotecan reveals a potential therapeutic effect in a mouse model of multiple sclerosis. In summary, deepDTnet offers a powerful network-based deep learning methodology for target identification to accelerate drug repurposing and minimize the translational gap in drug development.}
}

@inproceedings{zhangColorfulImageColorization2016,
  title = {Colorful {{Image Colorization}}},
  booktitle = {Computer {{Vision}} – {{ECCV}} 2016},
  author = {Zhang, Richard and Isola, Phillip and Efros, Alexei A.},
  editor = {Leibe, Bastian and Matas, Jiri and Sebe, Nicu and Welling, Max},
  date = {2016},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {649--666},
  publisher = {Springer International Publishing},
  location = {Cham},
  abstract = {Given a grayscale photograph as input, this paper attacks the problem of hallucinating a plausible color version of the photograph. This problem is clearly underconstrained, so previous approaches have either relied on significant user interaction or resulted in desaturated colorizations. We propose a fully automatic approach that produces vibrant and realistic colorizations. We embrace the underlying uncertainty of the problem by posing it as a classification task and use class-rebalancing at training time to increase the diversity of colors in the result. The system is implemented as a feed-forward pass in a CNN at test time and is trained on over a million color images. We evaluate our algorithm using a “colorization Turing test,” asking human participants to choose between a generated and ground truth color image. Our method successfully fools humans on 32~\% of the trials, significantly higher than previous methods. Moreover, we show that colorization can be a powerful pretext task for self-supervised feature learning, acting as a cross-channel encoder. This approach results in state-of-the-art performance on several feature learning benchmarks.},
  isbn = {978-3-319-46487-9},
  langid = {english},
  keywords = {CNNs,Colorization,Self-supervised learning,Vision for graphics},
  file = {/Users/luke/Zotero/storage/X56YDIWR/Zhang et al. - 2016 - Colorful Image Colorization.pdf}
}

@inproceedings{zhangDeepMutualLearning2018,
  title = {Deep {{Mutual Learning}}},
  author = {Zhang, Ying and Xiang, Tao and Hospedales, Timothy M. and Lu, Huchuan},
  date = {2018},
  url = {https://openaccess.thecvf.com/content_cvpr_2018/html/Zhang_Deep_Mutual_Learning_CVPR_2018_paper.html},
  urldate = {2023-11-22},
  eventtitle = {{{CVPR}}},
  file = {/Users/luke/Zotero/storage/XNWRLB27/Zhang et al. - 2018 - Deep Mutual Learning.pdf}
}

@inproceedings{zhangHeterogeneousGraphNeural2019,
  title = {Heterogeneous {{Graph Neural Network}}},
  booktitle = {Proceedings of the 25th {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} \& {{Data Mining}}},
  author = {Zhang, Chuxu and Song, Dongjin and Huang, Chao and Swami, Ananthram and Chawla, Nitesh V.},
  date = {2019-07-25},
  series = {{{KDD}} '19},
  pages = {793--803},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  url = {https://dl.acm.org/doi/10.1145/3292500.3330961},
  urldate = {2024-03-28},
  abstract = {Representation learning in heterogeneous graphs aims to pursue a meaningful vector representation for each node so as to facilitate downstream applications such as link prediction, personalized recommendation, node classification, etc. This task, however, is challenging not only because of the demand to incorporate heterogeneous structural (graph) information consisting of multiple types of nodes and edges, but also due to the need for considering heterogeneous attributes or contents (e.g., text or image) associated with each node. Despite a substantial amount of effort has been made to homogeneous (or heterogeneous) graph embedding, attributed graph embedding as well as graph neural networks, few of them can jointly consider heterogeneous structural (graph) information as well as heterogeneous contents information of each node effectively. In this paper, we propose HetGNN, a heterogeneous graph neural network model, to resolve this issue. Specifically, we first introduce a random walk with restart strategy to sample a fixed size of strongly correlated heterogeneous neighbors for each node and group them based upon node types. Next, we design a neural network architecture with two modules to aggregate feature information of those sampled neighboring nodes. The first module encodes "deep" feature interactions of heterogeneous contents and generates content embedding for each node. The second module aggregates content (attribute) embeddings of different neighboring groups (types) and further combines them by considering the impacts of different groups to obtain the ultimate node embedding. Finally, we leverage a graph context loss and a mini-batch gradient descent procedure to train the model in an end-to-end manner. Extensive experiments on several datasets demonstrate that HetGNN can outperform state-of-the-art baselines in various graph mining tasks, i.e., link prediction, recommendation, node classification \& clustering and inductive node classification \& clustering.},
  isbn = {978-1-4503-6201-6},
  keywords = {graph embedding,graph neural networks,heterogeneous graphs},
  file = {/Users/luke/Zotero/storage/V2UPYDHW/Zhang et al. - 2019 - Heterogeneous Graph Neural Network.pdf}
}

@misc{zhangLearnableHypergraphLaplacian2021,
  title = {Learnable {{Hypergraph Laplacian}} for {{Hypergraph Learning}}},
  author = {Zhang, Jiying and Chen, Yuzhao and Xiao, Xi and Lu, Runiu and Xia, Shu-Tao},
  date = {2021-06-10},
  eprint = {2106.05701},
  eprinttype = {arXiv},
  eprintclass = {cs.LG},
  url = {http://arxiv.org/abs/2106.05701},
  urldate = {2024-02-26},
  abstract = {HyperGraph Convolutional Neural Networks (HGCNNs) have demonstrated their potential in modeling high-order relations preserved in graph structured data. However, most existing convolution filters are localized and determined by the pre-defined initial hypergraph topology, neglecting to explore implicit and long-ange relations in real-world data. In this paper, we propose the first learning-based method tailored for constructing adaptive hypergraph structure, termed HypERgrAph Laplacian aDaptor (HERALD), which serves as a generic plug-in-play module for improving the representational power of HGCNNs. Specifically, HERALD adaptively optimizes the adjacency relationship between hypernodes and hyperedges in an end-to-end manner and thus the task-aware hypergraph is learned. Furthermore, HERALD employs the self-attention mechanism to capture the non-local paired-nodes relation. Extensive experiments on various popular hypergraph datasets for node classification and graph classification tasks demonstrate that our approach obtains consistent and considerable performance enhancement, proving its effectiveness and generalization ability.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/luke/Zotero/storage/VIASVDM7/Zhang et al. - 2021 - Learnable Hypergraph Laplacian for Hypergraph Lear.pdf;/Users/luke/Zotero/storage/4EKVYN5A/2106.html},
  note = {arXiv:2106.05701}
}

@misc{zhangProtGNNSelfExplainingGraph2021,
  title = {{{ProtGNN}}: {{Towards Self-Explaining Graph Neural Networks}}},
  shorttitle = {{{ProtGNN}}},
  author = {Zhang, Zaixi and Liu, Qi and Wang, Hao and Lu, Chengqiang and Lee, Cheekong},
  date = {2021-12-01},
  eprint = {2112.00911},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2112.00911},
  urldate = {2024-02-25},
  abstract = {Despite the recent progress in Graph Neural Networks (GNNs), it remains challenging to explain the predictions made by GNNs. Existing explanation methods mainly focus on post-hoc explanations where another explanatory model is employed to provide explanations for a trained GNN. The fact that post-hoc methods fail to reveal the original reasoning process of GNNs raises the need of building GNNs with built-in interpretability. In this work, we propose Prototype Graph Neural Network (ProtGNN), which combines prototype learning with GNNs and provides a new perspective on the explanations of GNNs. In ProtGNN, the explanations are naturally derived from the case-based reasoning process and are actually used during classification. The prediction of ProtGNN is obtained by comparing the inputs to a few learned prototypes in the latent space. Furthermore, for better interpretability and higher efficiency, a novel conditional subgraph sampling module is incorporated to indicate which part of the input graph is most similar to each prototype in ProtGNN+. Finally, we evaluate our method on a wide range of datasets and perform concrete case studies. Extensive results show that ProtGNN and ProtGNN+ can provide inherent interpretability while achieving accuracy on par with the non-interpretable counterparts.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/luke/Zotero/storage/CIATL6RT/Zhang et al. - 2021 - ProtGNN Towards Self-Explaining Graph Neural Netw.pdf;/Users/luke/Zotero/storage/H8QUQP4J/2112.html},
  note = {arXiv:2112.00911}
}

@inproceedings{zhangRelExModelAgnosticRelational2021,
  title = {{{RelEx}}: {{A Model-Agnostic Relational Model Explainer}}},
  shorttitle = {{{RelEx}}},
  booktitle = {Proceedings of the 2021 {{AAAI}}/{{ACM Conference}} on {{AI}}, {{Ethics}}, and {{Society}}},
  author = {Zhang, Yue and Defazio, David and Ramesh, Arti},
  date = {2021-07-30},
  series = {{{AIES}} '21},
  pages = {1042--1049},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  url = {https://dl.acm.org/doi/10.1145/3461702.3462562},
  urldate = {2024-03-08},
  abstract = {In recent years, considerable progress has been made on improving the interpretability of machine learning models. This is essential, as complex deep learning models with millions of parameters produce state of the art performance, but it can be nearly impossible to explain their predictions. While various explainability techniques have achieved impressive results, nearly all of them assume each data instance to be independent and identically distributed (iid). This excludes relational models, such as Statistical Relational Learning (SRL), and the recently popular Graph Neural Networks (GNNs), resulting in few options to explain them. While there does exist work on explaining GNNs, GNN-Explainer, they assume access to the gradients of the model to learn explanations, which is restrictive in terms of its applicability across non-differentiable relational models and practicality. In this work, we develop RelEx, amodel-agnostic relational explainer to explain black-box relational models with only access to the outputs of the black-box. RelEx is able to explain any relational model, including SRL models and GNNs. We compare RelEx to the state-of-the-art relational explainer, GNN-Explainer, and relational extensions of iid explanation models and show that RelEx achieves comparable or better performance, while remaining model-agnostic.},
  isbn = {978-1-4503-8473-5},
  keywords = {model-agnostic,relational explainer},
  file = {/Users/luke/Zotero/storage/ZYMM5AAV/Zhang et al. - 2021 - RelEx A Model-Agnostic Relational Model Explainer.pdf}
}

@misc{zhangUnreasonableEffectivenessDeep2018,
  title = {The {{Unreasonable Effectiveness}} of {{Deep Features}} as a {{Perceptual Metric}}},
  author = {Zhang, Richard and Isola, Phillip and Efros, Alexei A. and Shechtman, Eli and Wang, Oliver},
  date = {2018-04-10},
  eprint = {1801.03924},
  eprinttype = {arXiv},
  eprintclass = {cs.CV},
  url = {http://arxiv.org/abs/1801.03924},
  urldate = {2023-10-19},
  abstract = {While it is nearly effortless for humans to quickly assess the perceptual similarity between two images, the underlying processes are thought to be quite complex. Despite this, the most widely used perceptual metrics today, such as PSNR and SSIM, are simple, shallow functions, and fail to account for many nuances of human perception. Recently, the deep learning community has found that features of the VGG network trained on ImageNet classification has been remarkably useful as a training loss for image synthesis. But how perceptual are these so-called "perceptual losses"? What elements are critical for their success? To answer these questions, we introduce a new dataset of human perceptual similarity judgments. We systematically evaluate deep features across different architectures and tasks and compare them with classic metrics. We find that deep features outperform all previous metrics by large margins on our dataset. More surprisingly, this result is not restricted to ImageNet-trained VGG features, but holds across different deep architectures and levels of supervision (supervised, self-supervised, or even unsupervised). Our results suggest that perceptual similarity is an emergent property shared across deep visual representations.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics},
  file = {/Users/luke/Zotero/storage/N9Q6C9IA/Zhang et al. - 2018 - The Unreasonable Effectiveness of Deep Features as.pdf;/Users/luke/Zotero/storage/2KMC3C26/1801.html},
  note = {arXiv:1801.03924}
}

@inproceedings{zhaoSpace4HGNNNovelModularized2022,
  title = {{{Space4HGNN}}: {{A Novel}}, {{Modularized}} and {{Reproducible Platform}} to {{Evaluate Heterogeneous Graph Neural Network}}},
  shorttitle = {{{Space4HGNN}}},
  booktitle = {Proceedings of the 45th {{International ACM SIGIR Conference}} on {{Research}} and {{Development}} in {{Information Retrieval}}},
  author = {Zhao, Tianyu and Yang, Cheng and Li, Yibo and Gan, Quan and Wang, Zhenyi and Liang, Fengqi and Zhao, Huan and Shao, Yingxia and Wang, Xiao and Shi, Chuan},
  date = {2022-07-07},
  series = {{{SIGIR}} '22},
  pages = {2776--2789},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  url = {https://doi.org/10.1145/3477495.3531720},
  urldate = {2024-07-08},
  abstract = {Heterogeneous Graph Neural Network (HGNN) has been successfully employed in various tasks, but we cannot accurately know the importance of different design dimensions of HGNNs due to diverse architectures and applied scenarios. Besides, in the research community of HGNNs, implementing and evaluating various tasks still need much human effort. To mitigate these issues, we first propose a unified framework covering most HGNNs, consisting of three components: heterogeneous linear transformation, heterogeneous graph transformation, and heterogeneous message passing layer. Then we build a platform Space4HGNN by defining a design space for HGNNs based on the unified framework, which offers modularized components, reproducible implementations, and standardized evaluation for HGNNs. Finally, we conduct experiments to analyze the effect of different designs. With the insights found, we distill a condensed design space and verify its effectiveness.},
  isbn = {978-1-4503-8732-3},
  file = {/Users/luke/Zotero/storage/BWKT667P/Zhao et al. - 2022 - Space4HGNN A Novel, Modularized and Reproducible .pdf}
}

@article{zhengGMANGraphMultiAttention2020,
  title = {{{GMAN}}: {{A Graph Multi-Attention Network}} for {{Traffic Prediction}}},
  shorttitle = {{{GMAN}}},
  author = {Zheng, Chuanpan and Fan, Xiaoliang and Wang, Cheng and Qi, Jianzhong},
  date = {2020-04-03},
  journaltitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
  shortjournal = {AAAI},
  volume = {34},
  number = {01},
  pages = {1234--1241},
  issn = {2374-3468, 2159-5399},
  url = {https://ojs.aaai.org/index.php/AAAI/article/view/5477},
  urldate = {2024-02-26},
  abstract = {Long-term traffic prediction is highly challenging due to the complexity of traffic systems and the constantly changing nature of many impacting factors. In this paper, we focus on the spatio-temporal factors, and propose a graph multi-attention network (GMAN) to predict traffic conditions for time steps ahead at different locations on a road network graph. GMAN adapts an encoder-decoder architecture, where both the encoder and the decoder consist of multiple spatio-temporal attention blocks to model the impact of the spatio-temporal factors on traffic conditions. The encoder encodes the input traffic features and the decoder predicts the output sequence. Between the encoder and the decoder, a transform attention layer is applied to convert the encoded traffic features to generate the sequence representations of future time steps as the input of the decoder. The transform attention mechanism models the direct relationships between historical and future time steps that helps to alleviate the error propagation problem among prediction time steps. Experimental results on two real-world traffic prediction tasks (i.e., traffic volume prediction and traffic speed prediction) demonstrate the superiority of GMAN. In particular, in the 1 hour ahead prediction, GMAN outperforms state-of-the-art methods by up to 4\% improvement in MAE measure. The source code is available at https://github.com/zhengchuanpan/GMAN.},
  file = {/Users/luke/Zotero/storage/J8XD5FK3/Zheng et al. - 2020 - GMAN A Graph Multi-Attention Network for Traffic .pdf}
}

@inproceedings{zhouDevignEffectiveVulnerability2019,
  title = {Devign: {{Effective Vulnerability Identification}} by {{Learning Comprehensive Program Semantics}} via {{Graph Neural Networks}}},
  shorttitle = {Devign},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Zhou, Yaqin and Liu, Shangqing and Siow, Jingkai and Du, Xiaoning and Liu, Yang},
  date = {2019},
  volume = {32},
  publisher = {Curran Associates, Inc.},
  url = {https://proceedings.neurips.cc/paper_files/paper/2019/hash/49265d2447bc3bbfe9e76306ce40a31f-Abstract.html},
  urldate = {2023-12-20},
  abstract = {Vulnerability identification is crucial to protect the software systems from attacks for cyber security. It is especially important to localize the vulnerable functions among the source code to facilitate the fix. However, it is a challenging and tedious process, and also requires specialized security expertise. Inspired by the work on manually-defined patterns of vulnerabilities from various code representation graphs and the recent advance on graph neural networks, we propose Devign, a general graph neural network based model for graph-level classification through learning on a rich set of code semantic representations. It includes a novel Conv module to efficiently extract useful features in the learned rich node representations for graph-level classification. The model is trained over manually labeled datasets built on 4 diversified large-scale open-source C projects that incorporate high complexity and variety of real source code instead of synthesis code used in previous works. The results of the extensive evaluation on the datasets demonstrate that Devign outperforms the state of the arts significantly with an average of 10.51\% higher accuracy and 8.68\% F1 score, increases averagely 4.66\% accuracy and 6.37\% F1 by the Conv module.},
  eventtitle = {{{NeurIPS}} 2019},
  file = {/Users/luke/Zotero/storage/Q3GFA8VN/Zhou et al. - 2019 - Devign Effective Vulnerability Identification by .pdf}
}

@inproceedings{zhouLearningDeepFeatures2016,
  title = {Learning {{Deep Features}} for {{Discriminative Localization}}},
  author = {Zhou, Bolei and Khosla, Aditya and Lapedriza, Agata and Oliva, Aude and Torralba, Antonio},
  date = {2016-06-01},
  pages = {2921--2929},
  publisher = {IEEE Computer Society},
  issn = {1063-6919},
  url = {https://www.computer.org/csdl/proceedings-article/cvpr/2016/8851c921/12OmNqIhFR6},
  urldate = {2024-03-08},
  abstract = {In this work, we revisit the global average pooling layer proposed in [13], and shed light on how it explicitly enables the convolutional neural network (CNN) to have remarkable localization ability despite being trained on imagelevel labels. While this technique was previously proposed as a means for regularizing training, we find that it actually builds a generic localizable deep representation that exposes the implicit attention of CNNs on an image. Despite the apparent simplicity of global average pooling, we are able to achieve 37.1\% top-5 error for object localization on ILSVRC 2014 without training on any bounding box annotation. We demonstrate in a variety of experiments that our network is able to localize the discriminative image regions despite just being trained for solving classification task1.},
  eventtitle = {2016 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  isbn = {978-1-4673-8851-1},
  langid = {english},
  file = {/Users/luke/Zotero/storage/3DHTV85F/Zhou et al. - 2016 - Learning Deep Features for Discriminative Localiza.pdf}
}

@inproceedings{zhouSlotGATSlotbasedMessage2023,
  title = {{{SlotGAT}}: {{Slot-based Message Passing}} for {{Heterogeneous Graphs}}},
  shorttitle = {{{SlotGAT}}},
  booktitle = {Proceedings of the 40th {{International Conference}} on {{Machine Learning}}},
  author = {Zhou, Ziang and Shi, Jieming and Yang, Renchi and Zou, Yuanhang and Li, Qing},
  date = {2023-07-03},
  pages = {42644--42657},
  publisher = {PMLR},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v202/zhou23j.html},
  urldate = {2024-06-24},
  abstract = {Heterogeneous graphs are ubiquitous to model complex data. There are urgent needs on powerful heterogeneous graph neural networks to effectively support important applications. We identify a potential semantic mixing issue in existing message passing processes, where the representations of the neighbors of a node v are forced to be transformed to the feature space of v for aggregation, though the neighbors are in different types. That is, the semantics in different node types are entangled together into node v’s representation. To address the issue, we propose SlotGAT with separate message passing processes in slots, one for each node type, to maintain the representations in their own node-type feature spaces. Moreover, in a slot-based message passing layer, we design an attention mechanism for effective slot-wise message aggregation. Further, we develop a slot attention technique after the last layer of SlotGAT, to learn the importance of different slots in downstream tasks. Our analysis indicates that the slots in SlotGAT can preserve different semantics in various feature spaces. The superiority of SlotGAT is evaluated against 13 baselines on 6 datasets for node classification and link prediction. Our code is at https://github.com/scottjiao/SlotGAT\_ICML23/.},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english},
  file = {/Users/luke/Zotero/storage/D7QK553B/Zhou et al. - 2023 - SlotGAT Slot-based Message Passing for Heterogene.pdf}
}

@inproceedings{zhuHomophilyGraphNeural2020,
  title = {Beyond {{Homophily}} in {{Graph Neural Networks}}: {{Current Limitations}} and {{Effective Designs}}},
  shorttitle = {Beyond {{Homophily}} in {{Graph Neural Networks}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Zhu, Jiong and Yan, Yujun and Zhao, Lingxiao and Heimann, Mark and Akoglu, Leman and Koutra, Danai},
  date = {2020},
  volume = {33},
  pages = {7793--7804},
  publisher = {Curran Associates, Inc.},
  url = {https://proceedings.neurips.cc/paper/2020/hash/58ae23d878a47004366189884c2f8440-Abstract.html},
  urldate = {2024-05-26},
  abstract = {We investigate the representation power of graph neural networks in the semi-supervised node classification task under heterophily or low homophily, i.e., in networks where connected nodes may have different class labels and dissimilar features. Many popular GNNs fail to generalize to this setting, and are even outperformed by models that ignore the graph structure (e.g., multilayer perceptrons). Motivated by this limitation, we identify a set of key designs—ego- and neighbor-embedding separation, higher-order neighborhoods, and combination of intermediate representations—that boost learning from the graph structure under heterophily. We combine them into a graph neural network, H2GCN, which we use as the base method to empirically evaluate the effectiveness of the identified designs. Going beyond the traditional benchmarks with strong homophily, our empirical analysis shows that the identified designs increase the accuracy of GNNs by up to 40\% and 27\% over models without them on synthetic and real networks with heterophily, respectively, and yield competitive performance under homophily.},
  file = {/Users/luke/Zotero/storage/RULYSP6J/Zhu et al. - 2020 - Beyond Homophily in Graph Neural Networks Current.pdf}
}

@inproceedings{zhuRelationStructureAwareHeterogeneous2019,
  title = {Relation {{Structure-Aware Heterogeneous Graph Neural Network}}},
  booktitle = {2019 {{IEEE International Conference}} on {{Data Mining}} ({{ICDM}})},
  author = {Zhu, Shichao and Zhou, Chuan and Pan, Shirui and Zhu, Xingquan and Wang, Bin},
  date = {2019-11},
  pages = {1534--1539},
  issn = {2374-8486},
  url = {https://ieeexplore.ieee.org/document/8970828},
  urldate = {2024-07-05},
  abstract = {Heterogeneous graphs with different types of nodes and edges are ubiquitous and have immense value in many applications. Existing works on modeling heterogeneous graphs usually follow the idea of splitting a heterogeneous graph into multiple homogeneous subgraphs. This is ineffective in exploiting hidden rich semantic associations between different types of edges for large-scale multi-relational graphs. In this paper, we propose Relation Structure-Aware Heterogeneous Graph Neural Network (RSHN), a unified model that integrates graph and its coarsened line graph to embed both nodes and edges in heterogeneous graphs without requiring any prior knowledge such as metapath. To tackle the heterogeneity of edge connections, RSHN first creates a Coarsened Line Graph Neural Network (CL-GNN) to excavate edge-centric relation structural features that respect the latent associations of different types of edges based on coarsened line graph. After that, a Heterogeneous Graph Neural Network (H-GNN) is used to leverage implicit messages from neighbor nodes and edges propagating among nodes in heterogeneous graphs. As a result, different types of nodes and edges can enhance their embedding through mutual integration and promotion. Experiments and comparisons, based on semi-supervised classification tasks on large scale heterogeneous networks with over a hundred types of edges, show that RSHN significantly outperforms state-of-the-arts.},
  eventtitle = {2019 {{IEEE International Conference}} on {{Data Mining}} ({{ICDM}})},
  keywords = {heterogenous graph coarsened line graph graph neural network},
  file = {/Users/luke/Zotero/storage/CZTADZRV/Zhu et al. - 2019 - Relation Structure-Aware Heterogeneous Graph Neura.pdf;/Users/luke/Zotero/storage/6A5JR5XS/8970828.html}
}
